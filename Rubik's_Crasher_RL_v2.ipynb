{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import mpl_toolkits.mplot3d.art3d as art3d\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "import rl.policy\n",
    "from rl.memory import SequentialMemory\n",
    "import rl.callbacks\n",
    "import json\n",
    "\n",
    "ACTIONS = [\"U\", \"U'\", \"D\", \"D'\", \"L\", \"L'\", \"R\", \"R'\", \"F\", \"F'\", \"B\", \"B'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cube class definition\n",
    "class Cube:\n",
    "    \"\"\"\n",
    "    Class represents a 2x2x2 Rubik's Cube.\n",
    "    Written by Roei Aviad, 2022.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, new_cube: dict=None):\n",
    "        if new_cube == None:\n",
    "            self._cube = {}      # init solved cube\n",
    "            self._cube['U'] = np.array([[0, 0], [0, 0]])\n",
    "            self._cube['D'] = np.array([[1, 1], [1, 1]])\n",
    "            self._cube['F'] = np.array([[2, 2], [2, 2]])\n",
    "            self._cube['B'] = np.array([[3, 3], [3, 3]])\n",
    "            self._cube['L'] = np.array([[4, 4], [4, 4]])\n",
    "            self._cube['R'] = np.array([[5, 5], [5, 5]])\n",
    "        else:\n",
    "            self._cube = new_cube\n",
    "        self._last_move = 'N'       # defualt (None)\n",
    "\n",
    "    def move(self, action: str):\n",
    "        self._last_move = action      # set last action\n",
    "        # rotate clockwise:\n",
    "        if action == \"U\":\n",
    "            self._cube['F'][0], self._cube['R'][0], self._cube['B'][0], self._cube['L'][0] = self._cube['R'][0].copy(), self._cube['B'][0].copy(), self._cube['L'][0].copy(), self._cube['F'][0].copy()\n",
    "        elif action == \"D\":\n",
    "            self._cube['F'][1], self._cube['R'][1], self._cube['B'][1], self._cube['L'][1] = self._cube['L'][1].copy(), self._cube['F'][1].copy(), self._cube['R'][1].copy(), self._cube['B'][1].copy()\n",
    "        elif action == \"F\":\n",
    "            self._cube['U'][1], self._cube['R'][:, 0], self._cube['D'][0], self._cube['L'][:, 1] = self._cube['L'][:, 1][::-1].copy(), self._cube['U'][1].copy(), self._cube['R'][:, 0][::-1].copy(), self._cube['D'][0].copy()\n",
    "        elif action == \"L\":\n",
    "            self._cube['F'][:, 0], self._cube['D'][:, 0], self._cube['B'][:, 1], self._cube['U'][:, 0] = self._cube['U'][:, 0].copy(), self._cube['F'][:, 0].copy(), self._cube['D'][:, 0][::-1].copy(), self._cube['B'][:, 1][::-1].copy()\n",
    "        elif action == \"B\":\n",
    "            self._cube['U'][0], self._cube['L'][:, 0], self._cube['D'][1], self._cube['R'][:, 1] = self._cube['R'][:, 1].copy(), self._cube['U'][0][::-1].copy(), self._cube['L'][:, 0].copy(), self._cube['D'][1][::-1].copy()\n",
    "        elif action == \"R\":\n",
    "            self._cube['F'][:, 1], self._cube['U'][:, 1], self._cube['B'][:, 0], self._cube['D'][:, 1] = self._cube['D'][:, 1].copy(), self._cube['F'][:, 1].copy(), self._cube['U'][:, 1][::-1].copy(), self._cube['B'][:, 0][::-1].copy()\n",
    "\n",
    "        # rotate counter clockwise:\n",
    "        elif action == \"U'\":\n",
    "            self._cube['R'][0], self._cube['B'][0], self._cube['L'][0], self._cube['F'][0] = self._cube['F'][0].copy(), self._cube['R'][0].copy(), self._cube['B'][0].copy(), self._cube['L'][0].copy()\n",
    "        elif action == \"D'\":\n",
    "            self._cube['R'][1], self._cube['B'][1], self._cube['L'][1], self._cube['F'][1] = self._cube['B'][1].copy(), self._cube['L'][1].copy(), self._cube['F'][1].copy(), self._cube['R'][1].copy()\n",
    "        elif action == \"F'\":\n",
    "            self._cube['R'][:, 0], self._cube['D'][0], self._cube['L'][:, 1], self._cube['U'][1] = self._cube['D'][0][::-1].copy(), self._cube['L'][:, 1].copy(), self._cube['U'][1][::-1].copy(), self._cube['R'][:, 0].copy()\n",
    "        elif action == \"L'\":\n",
    "            self._cube['D'][:, 0], self._cube['B'][:, 1], self._cube['U'][:, 0], self._cube['F'][:, 0] = self._cube['B'][:, 1][::-1].copy(), self._cube['U'][:, 0][::-1].copy(), self._cube['F'][:, 0].copy(), self._cube['D'][:, 0].copy()\n",
    "        elif action == \"B'\":\n",
    "            self._cube['L'][:, 0], self._cube['D'][1], self._cube['R'][:, 1], self._cube['U'][0] = self._cube['D'][1].copy(), self._cube['R'][:, 1][::-1].copy(), self._cube['U'][0].copy(), self._cube['L'][:, 0][::-1].copy()\n",
    "        elif action == \"R'\":\n",
    "            self._cube['U'][:, 1], self._cube['B'][:, 0], self._cube['D'][:, 1], self._cube['F'][:, 1] = self._cube['B'][:, 0][::-1].copy(), self._cube['D'][:, 1][::-1].copy(), self._cube['F'][:, 1].copy(), self._cube['U'][:, 1].copy()\n",
    "\n",
    "\n",
    "        if \"'\" in action:\n",
    "            self._cube[action[0]] = np.rot90(self._cube[action[0]])       # rotate backwords\n",
    "        else:\n",
    "            for _ in range(3):\n",
    "                self._cube[action[0]] = np.rot90(self._cube[action[0]])   # rotate forwards\n",
    "\n",
    "    def shuffle(self, length: int=14, verbose: bool=True) -> list:\n",
    "        vprint = print if verbose else lambda *a, **k: None\n",
    "        actions_map = ['U', 'D', 'L', 'R', 'F', 'B']        # define actions\n",
    "        path = [self.copy()]\n",
    "\n",
    "        vprint(\"Shuffle sequence: \", end=' ')\n",
    "\n",
    "        i = 0\n",
    "        while i < length:\n",
    "            valid_actions = [actions_map[i] for i in range(len(actions_map)) if actions_map[i] != self._last_move[0]]     # remove last move\n",
    "            action = valid_actions[random.randint(0, len(valid_actions) - 1)] + \"'\" * random.randint(0, 1)         # get random action\n",
    "\n",
    "            times = 2 if random.randint(0, 10) == 0 else 1       # 1/11 chance to make move twice\n",
    "            for _ in range(times):\n",
    "                if i < length:\n",
    "                    self.move(action)       # make a random action\n",
    "                    path.insert(0, self.copy())\n",
    "                    vprint(action, end=' ')\n",
    "                    i += 1\n",
    "        vprint(\"\\n\")\n",
    "\n",
    "        return path        # return path\n",
    "\n",
    "    def show(self):\n",
    "        color_map = {2: 'red', 4: 'green', 5: 'blue', 1: 'yellow', 3: 'orange', 0: 'white'}\n",
    "        \n",
    "        temp_cube = {}      # copy cube and turn for visual effect\n",
    "        temp_cube['U'] = np.rot90(self._cube['U'], k=3)\n",
    "        temp_cube['D'] = np.flip(np.rot90(self._cube['D'], k=3), 1)\n",
    "        temp_cube['F'] = np.rot90(self._cube['F'], k=3)\n",
    "        temp_cube['B'] = np.flip(np.rot90(self._cube['B'], k=1), 1)\n",
    "        temp_cube['L'] = np.flip(np.rot90(self._cube['L'], k=1), 1)\n",
    "        temp_cube['R'] = np.rot90(self._cube['R'], k=3)\n",
    "\n",
    "        axs = []\n",
    "        fig = plt.figure()          # create 3d figure\n",
    "        fig.set_size_inches(4, 2)\n",
    "        for i in range(2):\n",
    "            axs.append(fig.add_subplot(1, 2, i + 1, projection='3d'))\n",
    "            axs[i].axis(False)\n",
    "            axs[i].grid(False)\n",
    "\n",
    "        special_order = ['L', 'R', 'F', 'B', 'D', 'U']      # for deciding on color and depth\n",
    "\n",
    "        for side in range(len(special_order)):\n",
    "            for i in range(2):\n",
    "                for j in range(2):\n",
    "                    r = Rectangle((i / 2, j / 2), 1 / 2, 1 / 2, ec='black', fc=color_map[temp_cube[special_order[side]][i][j]], alpha=(0.4 if side % 3 == 0 else 1))   # set correct color and location\n",
    "                    axs[1 ^ (side & 1) ^ ((side & 2) >> 1)].add_patch(r)       # 034=1, 125=0\n",
    "                    art3d.pathpatch_2d_to_3d(r, z=(side % 2), zdir=chr(ord('x') + (side // 2)))      # set correct plain based on side\n",
    "        plt.show()\n",
    "\n",
    "    def copy(self):\n",
    "        temp_cube = {}\n",
    "        for key in self._cube.keys():       # copy all inner arrays\n",
    "            temp_cube[key] = self._cube[key].copy()\n",
    "        return Cube(temp_cube)        # return duplicate\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        for key in self._cube.keys():      # for every side\n",
    "            if not (self._cube[key] == other._cube[key]).all():     # check equal\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def toarray(self) -> np.ndarray:\n",
    "        return np.reshape(list(self._cube.values()), (24,))\n",
    "    \n",
    "    @property\n",
    "    def cube(self):\n",
    "        return self._cube\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL environment definition\n",
    "class CubeLearnEnv:\n",
    "    \"\"\"\n",
    "    Rubik's cube 2x2x2 Deep Q-Learning environment class.\n",
    "    Written by Roei Aviad, 2022.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, depth, verbose=False, default=None):\n",
    "        self._depth = depth\n",
    "        self._verbose = verbose\n",
    "        self._default = default\n",
    "        self.reset()        # init object\n",
    "\n",
    "    def step(self, action: int) -> tuple:\n",
    "        ACTIONS = [\"U\", \"U'\", \"D\", \"D'\", \"L\", \"L'\", \"R\", \"R'\", \"F\", \"F'\", \"B\", \"B'\"]\n",
    "        self._cube.move(ACTIONS[action])         # rotate cube\n",
    "        self._counter += 1\n",
    "        if self._cube == Cube():\n",
    "            self._reward = 100\n",
    "            self._done = True\n",
    "        elif self._counter % self._depth == 0:\n",
    "            self._reward = -10\n",
    "            self._cube = self._start_pos.copy()\n",
    "        else:\n",
    "            self._reward = -1\n",
    "        return np.reshape(list(self._cube.copy().cube.values()), (24,)), self._reward, self._done, {}\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._cube = Cube(new_cube=self._default)        # create cube\n",
    "        self._reward = 0           # init reward\n",
    "        if self._default == None:\n",
    "            self._path = self._cube.shuffle(self._depth, verbose=self._verbose)     # init correct path\n",
    "        self._start_pos = self._cube.copy()\n",
    "        self._counter = 0\n",
    "        self._done = False         # set is done?\n",
    "        return np.reshape(list(self._cube.copy().cube.values()), (24,))\n",
    "\n",
    "    @property\n",
    "    def cube(self):\n",
    "        return self._cube.copy()\n",
    "\n",
    "    @property\n",
    "    def depth(self):\n",
    "        return self._depth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten1 (Flatten)          (None, 24)                0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (None, 72)                1800      \n",
      "                                                                 \n",
      " dense2 (Dense)              (None, 60)                4380      \n",
      "                                                                 \n",
      " dense3 (Dense)              (None, 48)                2928      \n",
      "                                                                 \n",
      " dense4 (Dense)              (None, 36)                1764      \n",
      "                                                                 \n",
      " dense5 (Dense)              (None, 24)                888       \n",
      "                                                                 \n",
      " dense6 (Dense)              (None, 12)                300       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,060\n",
      "Trainable params: 12,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create Keras model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1, 24), name=\"flatten1\"))\n",
    "model.add(Dense(72, activation='relu', name=\"dense1\"))\n",
    "model.add(Dense(60, activation='relu', name=\"dense2\"))\n",
    "model.add(Dense(48, activation='relu', name=\"dense3\"))\n",
    "model.add(Dense(36, activation='relu', name=\"dense4\"))\n",
    "model.add(Dense(24, activation='relu', name=\"dense5\"))\n",
    "model.add(Dense(12, activation='linear', name=\"dense6\"))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DQN agent\n",
    "memory = SequentialMemory(limit=5000000, window_length=1)\n",
    "policy = EpsGreedyQPolicy(eps=0.1)\n",
    "nb_actions = 12\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000, \\\n",
    "    target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "\n",
    "if os.path.exists(\"weights_v2.h5\"):\n",
    "    dqn.load_weights(\"weights_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "    1/10000: episode: 1, duration: 0.127s, episode steps:   1, steps per second:   8, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "    2/10000: episode: 2, duration: 0.002s, episode steps:   1, steps per second: 526, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "    3/10000: episode: 3, duration: 0.002s, episode steps:   1, steps per second: 535, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "    4/10000: episode: 4, duration: 0.003s, episode steps:   1, steps per second: 352, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "    5/10000: episode: 5, duration: 0.003s, episode steps:   1, steps per second: 367, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "    6/10000: episode: 6, duration: 0.003s, episode steps:   1, steps per second: 364, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "    7/10000: episode: 7, duration: 0.002s, episode steps:   1, steps per second: 468, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "    8/10000: episode: 8, duration: 0.002s, episode steps:   1, steps per second: 537, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "   10/10000: episode: 9, duration: 0.003s, episode steps:   2, steps per second: 685, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   11/10000: episode: 10, duration: 0.002s, episode steps:   1, steps per second: 547, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "   12/10000: episode: 11, duration: 0.002s, episode steps:   1, steps per second: 451, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   13/10000: episode: 12, duration: 0.002s, episode steps:   1, steps per second: 503, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "   14/10000: episode: 13, duration: 0.002s, episode steps:   1, steps per second: 532, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   15/10000: episode: 14, duration: 0.002s, episode steps:   1, steps per second: 496, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "   16/10000: episode: 15, duration: 0.002s, episode steps:   1, steps per second: 514, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "   17/10000: episode: 16, duration: 0.002s, episode steps:   1, steps per second: 533, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   18/10000: episode: 17, duration: 0.002s, episode steps:   1, steps per second: 524, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   19/10000: episode: 18, duration: 0.002s, episode steps:   1, steps per second: 454, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "   20/10000: episode: 19, duration: 0.002s, episode steps:   1, steps per second: 536, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   22/10000: episode: 20, duration: 0.003s, episode steps:   2, steps per second: 715, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.500 [10.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "   23/10000: episode: 21, duration: 0.002s, episode steps:   1, steps per second: 535, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   24/10000: episode: 22, duration: 0.002s, episode steps:   1, steps per second: 525, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   25/10000: episode: 23, duration: 0.002s, episode steps:   1, steps per second: 546, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "   26/10000: episode: 24, duration: 0.002s, episode steps:   1, steps per second: 461, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   27/10000: episode: 25, duration: 0.002s, episode steps:   1, steps per second: 468, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   29/10000: episode: 26, duration: 0.003s, episode steps:   2, steps per second: 740, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [2.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   30/10000: episode: 27, duration: 0.002s, episode steps:   1, steps per second: 553, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   31/10000: episode: 28, duration: 0.002s, episode steps:   1, steps per second: 524, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\משתמש\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   32/10000: episode: 29, duration: 0.003s, episode steps:   1, steps per second: 384, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "   33/10000: episode: 30, duration: 0.002s, episode steps:   1, steps per second: 449, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   34/10000: episode: 31, duration: 0.002s, episode steps:   1, steps per second: 508, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "   35/10000: episode: 32, duration: 0.002s, episode steps:   1, steps per second: 454, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "   37/10000: episode: 33, duration: 0.003s, episode steps:   2, steps per second: 704, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [7.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "   38/10000: episode: 34, duration: 0.002s, episode steps:   1, steps per second: 561, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   39/10000: episode: 35, duration: 0.002s, episode steps:   1, steps per second: 475, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   40/10000: episode: 36, duration: 0.002s, episode steps:   1, steps per second: 532, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "   41/10000: episode: 37, duration: 0.002s, episode steps:   1, steps per second: 464, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   42/10000: episode: 38, duration: 0.002s, episode steps:   1, steps per second: 404, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "   44/10000: episode: 39, duration: 0.003s, episode steps:   2, steps per second: 588, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [7.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "   45/10000: episode: 40, duration: 0.002s, episode steps:   1, steps per second: 440, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   47/10000: episode: 41, duration: 0.002s, episode steps:   2, steps per second: 842, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [1.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   48/10000: episode: 42, duration: 0.002s, episode steps:   1, steps per second: 556, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   49/10000: episode: 43, duration: 0.002s, episode steps:   1, steps per second: 413, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   50/10000: episode: 44, duration: 0.002s, episode steps:   1, steps per second: 512, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "   51/10000: episode: 45, duration: 0.002s, episode steps:   1, steps per second: 434, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "   52/10000: episode: 46, duration: 0.002s, episode steps:   1, steps per second: 485, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "   53/10000: episode: 47, duration: 0.002s, episode steps:   1, steps per second: 489, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   55/10000: episode: 48, duration: 0.003s, episode steps:   2, steps per second: 760, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [4.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   56/10000: episode: 49, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   57/10000: episode: 50, duration: 0.002s, episode steps:   1, steps per second: 460, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   58/10000: episode: 51, duration: 0.002s, episode steps:   1, steps per second: 485, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "   59/10000: episode: 52, duration: 0.003s, episode steps:   1, steps per second: 397, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "   60/10000: episode: 53, duration: 0.002s, episode steps:   1, steps per second: 452, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "   61/10000: episode: 54, duration: 0.002s, episode steps:   1, steps per second: 427, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   62/10000: episode: 55, duration: 0.002s, episode steps:   1, steps per second: 456, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   63/10000: episode: 56, duration: 0.003s, episode steps:   1, steps per second: 398, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   64/10000: episode: 57, duration: 0.003s, episode steps:   1, steps per second: 369, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   65/10000: episode: 58, duration: 0.002s, episode steps:   1, steps per second: 459, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "   66/10000: episode: 59, duration: 0.002s, episode steps:   1, steps per second: 471, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "   67/10000: episode: 60, duration: 0.003s, episode steps:   1, steps per second: 313, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   68/10000: episode: 61, duration: 0.002s, episode steps:   1, steps per second: 445, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "   69/10000: episode: 62, duration: 0.003s, episode steps:   1, steps per second: 380, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   70/10000: episode: 63, duration: 0.002s, episode steps:   1, steps per second: 529, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "   71/10000: episode: 64, duration: 0.002s, episode steps:   1, steps per second: 477, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "   72/10000: episode: 65, duration: 0.004s, episode steps:   1, steps per second: 259, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "   75/10000: episode: 66, duration: 0.005s, episode steps:   3, steps per second: 600, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "   76/10000: episode: 67, duration: 0.002s, episode steps:   1, steps per second: 515, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "   77/10000: episode: 68, duration: 0.002s, episode steps:   1, steps per second: 537, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "   78/10000: episode: 69, duration: 0.002s, episode steps:   1, steps per second: 558, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   79/10000: episode: 70, duration: 0.002s, episode steps:   1, steps per second: 446, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "   80/10000: episode: 71, duration: 0.002s, episode steps:   1, steps per second: 409, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   81/10000: episode: 72, duration: 0.003s, episode steps:   1, steps per second: 385, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "   82/10000: episode: 73, duration: 0.002s, episode steps:   1, steps per second: 462, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "   83/10000: episode: 74, duration: 0.002s, episode steps:   1, steps per second: 614, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "   84/10000: episode: 75, duration: 0.002s, episode steps:   1, steps per second: 575, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   85/10000: episode: 76, duration: 0.002s, episode steps:   1, steps per second: 558, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "   86/10000: episode: 77, duration: 0.003s, episode steps:   1, steps per second: 376, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "   87/10000: episode: 78, duration: 0.003s, episode steps:   1, steps per second: 393, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "   89/10000: episode: 79, duration: 0.003s, episode steps:   2, steps per second: 743, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [1.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   90/10000: episode: 80, duration: 0.002s, episode steps:   1, steps per second: 588, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "   92/10000: episode: 81, duration: 0.003s, episode steps:   2, steps per second: 745, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [6.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "   93/10000: episode: 82, duration: 0.002s, episode steps:   1, steps per second: 568, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "   94/10000: episode: 83, duration: 0.002s, episode steps:   1, steps per second: 424, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "   96/10000: episode: 84, duration: 0.004s, episode steps:   2, steps per second: 497, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [4.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "   97/10000: episode: 85, duration: 0.002s, episode steps:   1, steps per second: 576, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "   98/10000: episode: 86, duration: 0.002s, episode steps:   1, steps per second: 596, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  100/10000: episode: 87, duration: 0.002s, episode steps:   2, steps per second: 888, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [3.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  101/10000: episode: 88, duration: 0.002s, episode steps:   1, steps per second: 636, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  102/10000: episode: 89, duration: 0.002s, episode steps:   1, steps per second: 430, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  103/10000: episode: 90, duration: 0.002s, episode steps:   1, steps per second: 493, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  104/10000: episode: 91, duration: 0.002s, episode steps:   1, steps per second: 515, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  105/10000: episode: 92, duration: 0.002s, episode steps:   1, steps per second: 421, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  106/10000: episode: 93, duration: 0.002s, episode steps:   1, steps per second: 631, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  107/10000: episode: 94, duration: 0.002s, episode steps:   1, steps per second: 665, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  108/10000: episode: 95, duration: 0.002s, episode steps:   1, steps per second: 663, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  109/10000: episode: 96, duration: 0.002s, episode steps:   1, steps per second: 620, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  110/10000: episode: 97, duration: 0.003s, episode steps:   1, steps per second: 372, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  111/10000: episode: 98, duration: 0.002s, episode steps:   1, steps per second: 520, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  112/10000: episode: 99, duration: 0.002s, episode steps:   1, steps per second: 625, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  113/10000: episode: 100, duration: 0.002s, episode steps:   1, steps per second: 657, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  115/10000: episode: 101, duration: 0.002s, episode steps:   2, steps per second: 869, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  116/10000: episode: 102, duration: 0.002s, episode steps:   1, steps per second: 651, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  117/10000: episode: 103, duration: 0.002s, episode steps:   1, steps per second: 620, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  118/10000: episode: 104, duration: 0.002s, episode steps:   1, steps per second: 403, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  119/10000: episode: 105, duration: 0.002s, episode steps:   1, steps per second: 421, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  120/10000: episode: 106, duration: 0.002s, episode steps:   1, steps per second: 646, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  121/10000: episode: 107, duration: 0.002s, episode steps:   1, steps per second: 663, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  122/10000: episode: 108, duration: 0.002s, episode steps:   1, steps per second: 554, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  123/10000: episode: 109, duration: 0.002s, episode steps:   1, steps per second: 608, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  124/10000: episode: 110, duration: 0.001s, episode steps:   1, steps per second: 674, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  125/10000: episode: 111, duration: 0.002s, episode steps:   1, steps per second: 557, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  126/10000: episode: 112, duration: 0.002s, episode steps:   1, steps per second: 530, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  127/10000: episode: 113, duration: 0.002s, episode steps:   1, steps per second: 600, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  128/10000: episode: 114, duration: 0.002s, episode steps:   1, steps per second: 650, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  129/10000: episode: 115, duration: 0.002s, episode steps:   1, steps per second: 604, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  130/10000: episode: 116, duration: 0.002s, episode steps:   1, steps per second: 516, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  131/10000: episode: 117, duration: 0.002s, episode steps:   1, steps per second: 621, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  132/10000: episode: 118, duration: 0.002s, episode steps:   1, steps per second: 628, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  133/10000: episode: 119, duration: 0.002s, episode steps:   1, steps per second: 449, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  134/10000: episode: 120, duration: 0.002s, episode steps:   1, steps per second: 445, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  135/10000: episode: 121, duration: 0.002s, episode steps:   1, steps per second: 613, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  136/10000: episode: 122, duration: 0.002s, episode steps:   1, steps per second: 647, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  137/10000: episode: 123, duration: 0.002s, episode steps:   1, steps per second: 426, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  138/10000: episode: 124, duration: 0.002s, episode steps:   1, steps per second: 562, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  139/10000: episode: 125, duration: 0.002s, episode steps:   1, steps per second: 623, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  140/10000: episode: 126, duration: 0.002s, episode steps:   1, steps per second: 457, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  141/10000: episode: 127, duration: 0.002s, episode steps:   1, steps per second: 521, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  142/10000: episode: 128, duration: 0.002s, episode steps:   1, steps per second: 510, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  143/10000: episode: 129, duration: 0.002s, episode steps:   1, steps per second: 633, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  144/10000: episode: 130, duration: 0.002s, episode steps:   1, steps per second: 602, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  145/10000: episode: 131, duration: 0.002s, episode steps:   1, steps per second: 641, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  146/10000: episode: 132, duration: 0.002s, episode steps:   1, steps per second: 637, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  147/10000: episode: 133, duration: 0.002s, episode steps:   1, steps per second: 607, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  148/10000: episode: 134, duration: 0.002s, episode steps:   1, steps per second: 416, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  149/10000: episode: 135, duration: 0.002s, episode steps:   1, steps per second: 625, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  150/10000: episode: 136, duration: 0.002s, episode steps:   1, steps per second: 659, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  151/10000: episode: 137, duration: 0.002s, episode steps:   1, steps per second: 618, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  152/10000: episode: 138, duration: 0.002s, episode steps:   1, steps per second: 630, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  153/10000: episode: 139, duration: 0.002s, episode steps:   1, steps per second: 656, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  154/10000: episode: 140, duration: 0.002s, episode steps:   1, steps per second: 630, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  155/10000: episode: 141, duration: 0.002s, episode steps:   1, steps per second: 643, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  156/10000: episode: 142, duration: 0.002s, episode steps:   1, steps per second: 473, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  157/10000: episode: 143, duration: 0.002s, episode steps:   1, steps per second: 560, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  158/10000: episode: 144, duration: 0.002s, episode steps:   1, steps per second: 618, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  159/10000: episode: 145, duration: 0.002s, episode steps:   1, steps per second: 627, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  160/10000: episode: 146, duration: 0.002s, episode steps:   1, steps per second: 611, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  162/10000: episode: 147, duration: 0.002s, episode steps:   2, steps per second: 847, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [5.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  163/10000: episode: 148, duration: 0.002s, episode steps:   1, steps per second: 627, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  164/10000: episode: 149, duration: 0.002s, episode steps:   1, steps per second: 662, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  165/10000: episode: 150, duration: 0.002s, episode steps:   1, steps per second: 538, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  166/10000: episode: 151, duration: 0.002s, episode steps:   1, steps per second: 594, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  168/10000: episode: 152, duration: 0.002s, episode steps:   2, steps per second: 840, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  169/10000: episode: 153, duration: 0.002s, episode steps:   1, steps per second: 648, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  170/10000: episode: 154, duration: 0.002s, episode steps:   1, steps per second: 658, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  172/10000: episode: 155, duration: 0.002s, episode steps:   2, steps per second: 874, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [7.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  174/10000: episode: 156, duration: 0.004s, episode steps:   2, steps per second: 534, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [2.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  175/10000: episode: 157, duration: 0.002s, episode steps:   1, steps per second: 479, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  176/10000: episode: 158, duration: 0.002s, episode steps:   1, steps per second: 505, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  177/10000: episode: 159, duration: 0.002s, episode steps:   1, steps per second: 590, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  178/10000: episode: 160, duration: 0.002s, episode steps:   1, steps per second: 465, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  179/10000: episode: 161, duration: 0.002s, episode steps:   1, steps per second: 558, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  180/10000: episode: 162, duration: 0.002s, episode steps:   1, steps per second: 635, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  181/10000: episode: 163, duration: 0.002s, episode steps:   1, steps per second: 567, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  182/10000: episode: 164, duration: 0.002s, episode steps:   1, steps per second: 484, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  183/10000: episode: 165, duration: 0.002s, episode steps:   1, steps per second: 512, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  184/10000: episode: 166, duration: 0.002s, episode steps:   1, steps per second: 515, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  185/10000: episode: 167, duration: 0.002s, episode steps:   1, steps per second: 418, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  186/10000: episode: 168, duration: 0.002s, episode steps:   1, steps per second: 406, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  187/10000: episode: 169, duration: 0.002s, episode steps:   1, steps per second: 438, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  188/10000: episode: 170, duration: 0.003s, episode steps:   1, steps per second: 367, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  189/10000: episode: 171, duration: 0.002s, episode steps:   1, steps per second: 445, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  190/10000: episode: 172, duration: 0.002s, episode steps:   1, steps per second: 533, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  191/10000: episode: 173, duration: 0.002s, episode steps:   1, steps per second: 489, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  192/10000: episode: 174, duration: 0.002s, episode steps:   1, steps per second: 432, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  193/10000: episode: 175, duration: 0.003s, episode steps:   1, steps per second: 287, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  195/10000: episode: 176, duration: 0.004s, episode steps:   2, steps per second: 485, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.000 [8.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  196/10000: episode: 177, duration: 0.005s, episode steps:   1, steps per second: 218, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  197/10000: episode: 178, duration: 0.003s, episode steps:   1, steps per second: 345, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  198/10000: episode: 179, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  199/10000: episode: 180, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  201/10000: episode: 181, duration: 0.004s, episode steps:   2, steps per second: 546, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [6.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  202/10000: episode: 182, duration: 0.003s, episode steps:   1, steps per second: 358, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  203/10000: episode: 183, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  204/10000: episode: 184, duration: 0.003s, episode steps:   1, steps per second: 353, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  205/10000: episode: 185, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  206/10000: episode: 186, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  207/10000: episode: 187, duration: 0.003s, episode steps:   1, steps per second: 392, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  208/10000: episode: 188, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  209/10000: episode: 189, duration: 0.002s, episode steps:   1, steps per second: 414, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  210/10000: episode: 190, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  211/10000: episode: 191, duration: 0.003s, episode steps:   1, steps per second: 390, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  212/10000: episode: 192, duration: 0.002s, episode steps:   1, steps per second: 449, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  213/10000: episode: 193, duration: 0.003s, episode steps:   1, steps per second: 355, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  214/10000: episode: 194, duration: 0.003s, episode steps:   1, steps per second: 338, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  215/10000: episode: 195, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  216/10000: episode: 196, duration: 0.002s, episode steps:   1, steps per second: 407, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  217/10000: episode: 197, duration: 0.003s, episode steps:   1, steps per second: 287, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  218/10000: episode: 198, duration: 0.004s, episode steps:   1, steps per second: 285, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  219/10000: episode: 199, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  220/10000: episode: 200, duration: 0.003s, episode steps:   1, steps per second: 367, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  222/10000: episode: 201, duration: 0.004s, episode steps:   2, steps per second: 446, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.500 [10.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  223/10000: episode: 202, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  224/10000: episode: 203, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  225/10000: episode: 204, duration: 0.003s, episode steps:   1, steps per second: 396, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  226/10000: episode: 205, duration: 0.003s, episode steps:   1, steps per second: 396, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  227/10000: episode: 206, duration: 0.003s, episode steps:   1, steps per second: 363, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  228/10000: episode: 207, duration: 0.003s, episode steps:   1, steps per second: 369, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  229/10000: episode: 208, duration: 0.002s, episode steps:   1, steps per second: 474, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  230/10000: episode: 209, duration: 0.003s, episode steps:   1, steps per second: 327, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  231/10000: episode: 210, duration: 0.003s, episode steps:   1, steps per second: 337, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  232/10000: episode: 211, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  233/10000: episode: 212, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  234/10000: episode: 213, duration: 0.003s, episode steps:   1, steps per second: 379, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  235/10000: episode: 214, duration: 0.003s, episode steps:   1, steps per second: 353, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  236/10000: episode: 215, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  237/10000: episode: 216, duration: 0.003s, episode steps:   1, steps per second: 351, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  238/10000: episode: 217, duration: 0.003s, episode steps:   1, steps per second: 381, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  239/10000: episode: 218, duration: 0.004s, episode steps:   1, steps per second: 255, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  240/10000: episode: 219, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  241/10000: episode: 220, duration: 0.003s, episode steps:   1, steps per second: 346, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  242/10000: episode: 221, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  243/10000: episode: 222, duration: 0.004s, episode steps:   1, steps per second: 237, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  245/10000: episode: 223, duration: 0.003s, episode steps:   2, steps per second: 607, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [4.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  246/10000: episode: 224, duration: 0.002s, episode steps:   1, steps per second: 457, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  247/10000: episode: 225, duration: 0.002s, episode steps:   1, steps per second: 466, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  248/10000: episode: 226, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  249/10000: episode: 227, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  250/10000: episode: 228, duration: 0.009s, episode steps:   1, steps per second: 108, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  251/10000: episode: 229, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  252/10000: episode: 230, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  253/10000: episode: 231, duration: 0.004s, episode steps:   1, steps per second: 277, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  254/10000: episode: 232, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  255/10000: episode: 233, duration: 0.006s, episode steps:   1, steps per second: 177, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  257/10000: episode: 234, duration: 0.003s, episode steps:   2, steps per second: 595, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [7.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  258/10000: episode: 235, duration: 0.003s, episode steps:   1, steps per second: 334, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  259/10000: episode: 236, duration: 0.002s, episode steps:   1, steps per second: 448, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  260/10000: episode: 237, duration: 0.005s, episode steps:   1, steps per second: 182, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  261/10000: episode: 238, duration: 0.004s, episode steps:   1, steps per second: 283, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  262/10000: episode: 239, duration: 0.003s, episode steps:   1, steps per second: 383, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  263/10000: episode: 240, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  265/10000: episode: 241, duration: 0.006s, episode steps:   2, steps per second: 315, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [2.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  266/10000: episode: 242, duration: 0.003s, episode steps:   1, steps per second: 373, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  267/10000: episode: 243, duration: 0.003s, episode steps:   1, steps per second: 382, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  268/10000: episode: 244, duration: 0.002s, episode steps:   1, steps per second: 407, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  269/10000: episode: 245, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  270/10000: episode: 246, duration: 0.003s, episode steps:   1, steps per second: 347, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  271/10000: episode: 247, duration: 0.003s, episode steps:   1, steps per second: 360, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  272/10000: episode: 248, duration: 0.003s, episode steps:   1, steps per second: 388, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  273/10000: episode: 249, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  274/10000: episode: 250, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  275/10000: episode: 251, duration: 0.002s, episode steps:   1, steps per second: 474, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  276/10000: episode: 252, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  277/10000: episode: 253, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  278/10000: episode: 254, duration: 0.003s, episode steps:   1, steps per second: 325, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  279/10000: episode: 255, duration: 0.003s, episode steps:   1, steps per second: 349, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  280/10000: episode: 256, duration: 0.003s, episode steps:   1, steps per second: 339, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  281/10000: episode: 257, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  283/10000: episode: 258, duration: 0.005s, episode steps:   2, steps per second: 436, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [1.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  284/10000: episode: 259, duration: 0.003s, episode steps:   1, steps per second: 375, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  285/10000: episode: 260, duration: 0.003s, episode steps:   1, steps per second: 306, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  286/10000: episode: 261, duration: 0.004s, episode steps:   1, steps per second: 237, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  287/10000: episode: 262, duration: 0.003s, episode steps:   1, steps per second: 361, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  288/10000: episode: 263, duration: 0.002s, episode steps:   1, steps per second: 432, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  289/10000: episode: 264, duration: 0.003s, episode steps:   1, steps per second: 337, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  290/10000: episode: 265, duration: 0.006s, episode steps:   1, steps per second: 157, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  291/10000: episode: 266, duration: 0.002s, episode steps:   1, steps per second: 414, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  292/10000: episode: 267, duration: 0.003s, episode steps:   1, steps per second: 357, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  293/10000: episode: 268, duration: 0.002s, episode steps:   1, steps per second: 414, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  294/10000: episode: 269, duration: 0.006s, episode steps:   1, steps per second: 173, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  295/10000: episode: 270, duration: 0.002s, episode steps:   1, steps per second: 411, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  296/10000: episode: 271, duration: 0.002s, episode steps:   1, steps per second: 438, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  297/10000: episode: 272, duration: 0.002s, episode steps:   1, steps per second: 423, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  298/10000: episode: 273, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  299/10000: episode: 274, duration: 0.003s, episode steps:   1, steps per second: 314, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  300/10000: episode: 275, duration: 0.003s, episode steps:   1, steps per second: 349, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  301/10000: episode: 276, duration: 0.002s, episode steps:   1, steps per second: 421, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  302/10000: episode: 277, duration: 0.003s, episode steps:   1, steps per second: 351, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  303/10000: episode: 278, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  304/10000: episode: 279, duration: 0.003s, episode steps:   1, steps per second: 355, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  305/10000: episode: 280, duration: 0.003s, episode steps:   1, steps per second: 380, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  306/10000: episode: 281, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  307/10000: episode: 282, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  308/10000: episode: 283, duration: 0.003s, episode steps:   1, steps per second: 386, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  309/10000: episode: 284, duration: 0.002s, episode steps:   1, steps per second: 407, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  310/10000: episode: 285, duration: 0.003s, episode steps:   1, steps per second: 359, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  311/10000: episode: 286, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  313/10000: episode: 287, duration: 0.004s, episode steps:   2, steps per second: 568, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [7.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  314/10000: episode: 288, duration: 0.002s, episode steps:   1, steps per second: 448, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  315/10000: episode: 289, duration: 0.002s, episode steps:   1, steps per second: 460, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  316/10000: episode: 290, duration: 0.002s, episode steps:   1, steps per second: 640, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  320/10000: episode: 291, duration: 0.006s, episode steps:   4, steps per second: 621, episode reward: 70.000, mean reward: 17.500 [-10.000, 100.000], mean action: 4.500 [0.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  321/10000: episode: 292, duration: 0.002s, episode steps:   1, steps per second: 562, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  322/10000: episode: 293, duration: 0.002s, episode steps:   1, steps per second: 532, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  323/10000: episode: 294, duration: 0.002s, episode steps:   1, steps per second: 523, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  324/10000: episode: 295, duration: 0.002s, episode steps:   1, steps per second: 525, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  325/10000: episode: 296, duration: 0.002s, episode steps:   1, steps per second: 486, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  326/10000: episode: 297, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  327/10000: episode: 298, duration: 0.002s, episode steps:   1, steps per second: 424, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  328/10000: episode: 299, duration: 0.002s, episode steps:   1, steps per second: 471, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  329/10000: episode: 300, duration: 0.002s, episode steps:   1, steps per second: 559, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  330/10000: episode: 301, duration: 0.002s, episode steps:   1, steps per second: 472, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  331/10000: episode: 302, duration: 0.003s, episode steps:   1, steps per second: 308, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  332/10000: episode: 303, duration: 0.002s, episode steps:   1, steps per second: 474, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  333/10000: episode: 304, duration: 0.002s, episode steps:   1, steps per second: 574, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  335/10000: episode: 305, duration: 0.003s, episode steps:   2, steps per second: 648, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [2.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  336/10000: episode: 306, duration: 0.002s, episode steps:   1, steps per second: 604, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  337/10000: episode: 307, duration: 0.002s, episode steps:   1, steps per second: 574, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  339/10000: episode: 308, duration: 0.005s, episode steps:   2, steps per second: 439, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [5.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  340/10000: episode: 309, duration: 0.002s, episode steps:   1, steps per second: 603, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  341/10000: episode: 310, duration: 0.002s, episode steps:   1, steps per second: 609, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  342/10000: episode: 311, duration: 0.002s, episode steps:   1, steps per second: 527, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  343/10000: episode: 312, duration: 0.001s, episode steps:   1, steps per second: 679, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  344/10000: episode: 313, duration: 0.001s, episode steps:   1, steps per second: 716, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  345/10000: episode: 314, duration: 0.001s, episode steps:   1, steps per second: 708, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  346/10000: episode: 315, duration: 0.003s, episode steps:   1, steps per second: 360, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  347/10000: episode: 316, duration: 0.002s, episode steps:   1, steps per second: 596, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  349/10000: episode: 317, duration: 0.002s, episode steps:   2, steps per second: 913, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [2.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  350/10000: episode: 318, duration: 0.001s, episode steps:   1, steps per second: 754, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  351/10000: episode: 319, duration: 0.001s, episode steps:   1, steps per second: 736, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  352/10000: episode: 320, duration: 0.002s, episode steps:   1, steps per second: 647, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  353/10000: episode: 321, duration: 0.001s, episode steps:   1, steps per second: 700, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  354/10000: episode: 322, duration: 0.002s, episode steps:   1, steps per second: 613, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  355/10000: episode: 323, duration: 0.003s, episode steps:   1, steps per second: 342, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  356/10000: episode: 324, duration: 0.002s, episode steps:   1, steps per second: 544, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  357/10000: episode: 325, duration: 0.001s, episode steps:   1, steps per second: 732, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  358/10000: episode: 326, duration: 0.002s, episode steps:   1, steps per second: 642, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  359/10000: episode: 327, duration: 0.001s, episode steps:   1, steps per second: 696, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  360/10000: episode: 328, duration: 0.002s, episode steps:   1, steps per second: 636, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  361/10000: episode: 329, duration: 0.001s, episode steps:   1, steps per second: 701, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  362/10000: episode: 330, duration: 0.002s, episode steps:   1, steps per second: 534, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  363/10000: episode: 331, duration: 0.003s, episode steps:   1, steps per second: 359, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  364/10000: episode: 332, duration: 0.002s, episode steps:   1, steps per second: 666, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  366/10000: episode: 333, duration: 0.002s, episode steps:   2, steps per second: 877, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [4.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  367/10000: episode: 334, duration: 0.002s, episode steps:   1, steps per second: 661, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  368/10000: episode: 335, duration: 0.001s, episode steps:   1, steps per second: 682, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  369/10000: episode: 336, duration: 0.002s, episode steps:   1, steps per second: 659, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  370/10000: episode: 337, duration: 0.002s, episode steps:   1, steps per second: 655, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  371/10000: episode: 338, duration: 0.003s, episode steps:   1, steps per second: 399, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  372/10000: episode: 339, duration: 0.003s, episode steps:   1, steps per second: 386, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  374/10000: episode: 340, duration: 0.002s, episode steps:   2, steps per second: 833, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  375/10000: episode: 341, duration: 0.002s, episode steps:   1, steps per second: 660, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  376/10000: episode: 342, duration: 0.002s, episode steps:   1, steps per second: 597, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  377/10000: episode: 343, duration: 0.002s, episode steps:   1, steps per second: 508, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  378/10000: episode: 344, duration: 0.002s, episode steps:   1, steps per second: 603, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  379/10000: episode: 345, duration: 0.003s, episode steps:   1, steps per second: 300, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  380/10000: episode: 346, duration: 0.002s, episode steps:   1, steps per second: 473, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  381/10000: episode: 347, duration: 0.002s, episode steps:   1, steps per second: 614, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  382/10000: episode: 348, duration: 0.002s, episode steps:   1, steps per second: 620, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  383/10000: episode: 349, duration: 0.002s, episode steps:   1, steps per second: 597, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  384/10000: episode: 350, duration: 0.002s, episode steps:   1, steps per second: 599, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  385/10000: episode: 351, duration: 0.002s, episode steps:   1, steps per second: 608, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  386/10000: episode: 352, duration: 0.003s, episode steps:   1, steps per second: 302, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  387/10000: episode: 353, duration: 0.002s, episode steps:   1, steps per second: 565, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  388/10000: episode: 354, duration: 0.001s, episode steps:   1, steps per second: 686, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  389/10000: episode: 355, duration: 0.002s, episode steps:   1, steps per second: 548, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  390/10000: episode: 356, duration: 0.002s, episode steps:   1, steps per second: 651, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  391/10000: episode: 357, duration: 0.002s, episode steps:   1, steps per second: 665, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  392/10000: episode: 358, duration: 0.001s, episode steps:   1, steps per second: 695, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  393/10000: episode: 359, duration: 0.002s, episode steps:   1, steps per second: 611, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  394/10000: episode: 360, duration: 0.004s, episode steps:   1, steps per second: 282, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  395/10000: episode: 361, duration: 0.002s, episode steps:   1, steps per second: 432, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  396/10000: episode: 362, duration: 0.002s, episode steps:   1, steps per second: 461, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  398/10000: episode: 363, duration: 0.005s, episode steps:   2, steps per second: 370, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [1.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  399/10000: episode: 364, duration: 0.005s, episode steps:   1, steps per second: 190, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  400/10000: episode: 365, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  401/10000: episode: 366, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  402/10000: episode: 367, duration: 0.007s, episode steps:   1, steps per second: 152, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  403/10000: episode: 368, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  404/10000: episode: 369, duration: 0.006s, episode steps:   1, steps per second: 175, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  405/10000: episode: 370, duration: 0.004s, episode steps:   1, steps per second: 231, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  406/10000: episode: 371, duration: 0.004s, episode steps:   1, steps per second: 285, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  407/10000: episode: 372, duration: 0.004s, episode steps:   1, steps per second: 269, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  408/10000: episode: 373, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  409/10000: episode: 374, duration: 0.003s, episode steps:   1, steps per second: 292, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  410/10000: episode: 375, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  411/10000: episode: 376, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  412/10000: episode: 377, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  413/10000: episode: 378, duration: 0.004s, episode steps:   1, steps per second: 267, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  414/10000: episode: 379, duration: 0.006s, episode steps:   1, steps per second: 167, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  415/10000: episode: 380, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  416/10000: episode: 381, duration: 0.004s, episode steps:   1, steps per second: 285, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  417/10000: episode: 382, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  418/10000: episode: 383, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  419/10000: episode: 384, duration: 0.003s, episode steps:   1, steps per second: 304, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  420/10000: episode: 385, duration: 0.003s, episode steps:   1, steps per second: 287, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  421/10000: episode: 386, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  422/10000: episode: 387, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  423/10000: episode: 388, duration: 0.003s, episode steps:   1, steps per second: 295, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  424/10000: episode: 389, duration: 0.004s, episode steps:   1, steps per second: 253, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  425/10000: episode: 390, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  426/10000: episode: 391, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  427/10000: episode: 392, duration: 0.004s, episode steps:   1, steps per second: 270, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  428/10000: episode: 393, duration: 0.006s, episode steps:   1, steps per second: 178, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  429/10000: episode: 394, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  430/10000: episode: 395, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  431/10000: episode: 396, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  432/10000: episode: 397, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  433/10000: episode: 398, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  434/10000: episode: 399, duration: 0.004s, episode steps:   1, steps per second: 228, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  435/10000: episode: 400, duration: 0.005s, episode steps:   1, steps per second: 215, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  436/10000: episode: 401, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  437/10000: episode: 402, duration: 0.006s, episode steps:   1, steps per second: 158, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  439/10000: episode: 403, duration: 0.006s, episode steps:   2, steps per second: 345, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [1.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  440/10000: episode: 404, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  441/10000: episode: 405, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  442/10000: episode: 406, duration: 0.004s, episode steps:   1, steps per second: 253, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  443/10000: episode: 407, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  444/10000: episode: 408, duration: 0.006s, episode steps:   1, steps per second: 165, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  445/10000: episode: 409, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  447/10000: episode: 410, duration: 0.007s, episode steps:   2, steps per second: 269, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [2.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  448/10000: episode: 411, duration: 0.005s, episode steps:   1, steps per second: 218, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  449/10000: episode: 412, duration: 0.004s, episode steps:   1, steps per second: 255, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  450/10000: episode: 413, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  451/10000: episode: 414, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  452/10000: episode: 415, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  453/10000: episode: 416, duration: 0.004s, episode steps:   1, steps per second: 246, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  454/10000: episode: 417, duration: 0.004s, episode steps:   1, steps per second: 229, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  455/10000: episode: 418, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  456/10000: episode: 419, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  457/10000: episode: 420, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  458/10000: episode: 421, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  459/10000: episode: 422, duration: 0.004s, episode steps:   1, steps per second: 277, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  460/10000: episode: 423, duration: 0.004s, episode steps:   1, steps per second: 225, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  461/10000: episode: 424, duration: 0.003s, episode steps:   1, steps per second: 289, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  462/10000: episode: 425, duration: 0.003s, episode steps:   1, steps per second: 294, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  463/10000: episode: 426, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  464/10000: episode: 427, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  465/10000: episode: 428, duration: 0.003s, episode steps:   1, steps per second: 291, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  466/10000: episode: 429, duration: 0.004s, episode steps:   1, steps per second: 282, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  467/10000: episode: 430, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  468/10000: episode: 431, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  469/10000: episode: 432, duration: 0.004s, episode steps:   1, steps per second: 282, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  470/10000: episode: 433, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  471/10000: episode: 434, duration: 0.004s, episode steps:   1, steps per second: 271, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  472/10000: episode: 435, duration: 0.003s, episode steps:   1, steps per second: 303, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  474/10000: episode: 436, duration: 0.007s, episode steps:   2, steps per second: 296, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [0.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  475/10000: episode: 437, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  477/10000: episode: 438, duration: 0.006s, episode steps:   2, steps per second: 329, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.500 [10.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  478/10000: episode: 439, duration: 0.008s, episode steps:   1, steps per second: 122, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  480/10000: episode: 440, duration: 0.009s, episode steps:   2, steps per second: 217, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [6.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  481/10000: episode: 441, duration: 0.003s, episode steps:   1, steps per second: 301, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  482/10000: episode: 442, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  483/10000: episode: 443, duration: 0.003s, episode steps:   1, steps per second: 312, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  484/10000: episode: 444, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  485/10000: episode: 445, duration: 0.003s, episode steps:   1, steps per second: 330, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  486/10000: episode: 446, duration: 0.003s, episode steps:   1, steps per second: 331, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  487/10000: episode: 447, duration: 0.003s, episode steps:   1, steps per second: 351, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  488/10000: episode: 448, duration: 0.004s, episode steps:   1, steps per second: 230, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  489/10000: episode: 449, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  490/10000: episode: 450, duration: 0.004s, episode steps:   1, steps per second: 243, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  491/10000: episode: 451, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  492/10000: episode: 452, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  493/10000: episode: 453, duration: 0.003s, episode steps:   1, steps per second: 328, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  494/10000: episode: 454, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  495/10000: episode: 455, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  496/10000: episode: 456, duration: 0.003s, episode steps:   1, steps per second: 318, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  497/10000: episode: 457, duration: 0.003s, episode steps:   1, steps per second: 356, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  498/10000: episode: 458, duration: 0.003s, episode steps:   1, steps per second: 370, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  499/10000: episode: 459, duration: 0.003s, episode steps:   1, steps per second: 337, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  500/10000: episode: 460, duration: 0.004s, episode steps:   1, steps per second: 274, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  501/10000: episode: 461, duration: 0.003s, episode steps:   1, steps per second: 325, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  502/10000: episode: 462, duration: 0.003s, episode steps:   1, steps per second: 320, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  503/10000: episode: 463, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  504/10000: episode: 464, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  505/10000: episode: 465, duration: 0.003s, episode steps:   1, steps per second: 295, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  506/10000: episode: 466, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  507/10000: episode: 467, duration: 0.003s, episode steps:   1, steps per second: 314, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  508/10000: episode: 468, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  509/10000: episode: 469, duration: 0.003s, episode steps:   1, steps per second: 337, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  510/10000: episode: 470, duration: 0.003s, episode steps:   1, steps per second: 354, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  511/10000: episode: 471, duration: 0.003s, episode steps:   1, steps per second: 346, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  512/10000: episode: 472, duration: 0.003s, episode steps:   1, steps per second: 361, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  513/10000: episode: 473, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  514/10000: episode: 474, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  515/10000: episode: 475, duration: 0.003s, episode steps:   1, steps per second: 292, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  516/10000: episode: 476, duration: 0.004s, episode steps:   1, steps per second: 282, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  518/10000: episode: 477, duration: 0.005s, episode steps:   2, steps per second: 418, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [2.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  519/10000: episode: 478, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  520/10000: episode: 479, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  521/10000: episode: 480, duration: 0.004s, episode steps:   1, steps per second: 271, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  522/10000: episode: 481, duration: 0.003s, episode steps:   1, steps per second: 317, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  523/10000: episode: 482, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  524/10000: episode: 483, duration: 0.006s, episode steps:   1, steps per second: 161, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  525/10000: episode: 484, duration: 0.004s, episode steps:   1, steps per second: 267, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  526/10000: episode: 485, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  527/10000: episode: 486, duration: 0.007s, episode steps:   1, steps per second: 141, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  528/10000: episode: 487, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  529/10000: episode: 488, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  530/10000: episode: 489, duration: 0.006s, episode steps:   1, steps per second: 179, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  532/10000: episode: 490, duration: 0.005s, episode steps:   2, steps per second: 444, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [6.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  533/10000: episode: 491, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  534/10000: episode: 492, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  535/10000: episode: 493, duration: 0.004s, episode steps:   1, steps per second: 235, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  536/10000: episode: 494, duration: 0.005s, episode steps:   1, steps per second: 189, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  537/10000: episode: 495, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  538/10000: episode: 496, duration: 0.003s, episode steps:   1, steps per second: 290, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  539/10000: episode: 497, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  540/10000: episode: 498, duration: 0.003s, episode steps:   1, steps per second: 306, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  541/10000: episode: 499, duration: 0.003s, episode steps:   1, steps per second: 325, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  542/10000: episode: 500, duration: 0.003s, episode steps:   1, steps per second: 346, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  543/10000: episode: 501, duration: 0.003s, episode steps:   1, steps per second: 318, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  545/10000: episode: 502, duration: 0.005s, episode steps:   2, steps per second: 371, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [5.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  546/10000: episode: 503, duration: 0.003s, episode steps:   1, steps per second: 329, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  547/10000: episode: 504, duration: 0.003s, episode steps:   1, steps per second: 351, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  548/10000: episode: 505, duration: 0.003s, episode steps:   1, steps per second: 295, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  550/10000: episode: 506, duration: 0.006s, episode steps:   2, steps per second: 309, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [1.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  552/10000: episode: 507, duration: 0.005s, episode steps:   2, steps per second: 408, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [6.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  553/10000: episode: 508, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  554/10000: episode: 509, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  555/10000: episode: 510, duration: 0.003s, episode steps:   1, steps per second: 312, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  556/10000: episode: 511, duration: 0.003s, episode steps:   1, steps per second: 348, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  557/10000: episode: 512, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  558/10000: episode: 513, duration: 0.003s, episode steps:   1, steps per second: 337, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  559/10000: episode: 514, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  560/10000: episode: 515, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  561/10000: episode: 516, duration: 0.003s, episode steps:   1, steps per second: 343, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  563/10000: episode: 517, duration: 0.006s, episode steps:   2, steps per second: 315, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [1.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  564/10000: episode: 518, duration: 0.004s, episode steps:   1, steps per second: 253, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  565/10000: episode: 519, duration: 0.003s, episode steps:   1, steps per second: 292, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  566/10000: episode: 520, duration: 0.003s, episode steps:   1, steps per second: 359, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  567/10000: episode: 521, duration: 0.004s, episode steps:   1, steps per second: 227, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  568/10000: episode: 522, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  569/10000: episode: 523, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  570/10000: episode: 524, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  571/10000: episode: 525, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  572/10000: episode: 526, duration: 0.003s, episode steps:   1, steps per second: 307, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  574/10000: episode: 527, duration: 0.008s, episode steps:   2, steps per second: 257, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [5.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  575/10000: episode: 528, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  576/10000: episode: 529, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  577/10000: episode: 530, duration: 0.003s, episode steps:   1, steps per second: 287, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  578/10000: episode: 531, duration: 0.003s, episode steps:   1, steps per second: 302, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  579/10000: episode: 532, duration: 0.003s, episode steps:   1, steps per second: 378, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  580/10000: episode: 533, duration: 0.007s, episode steps:   1, steps per second: 144, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  581/10000: episode: 534, duration: 0.003s, episode steps:   1, steps per second: 355, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  582/10000: episode: 535, duration: 0.003s, episode steps:   1, steps per second: 350, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  583/10000: episode: 536, duration: 0.003s, episode steps:   1, steps per second: 356, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  584/10000: episode: 537, duration: 0.004s, episode steps:   1, steps per second: 236, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  585/10000: episode: 538, duration: 0.003s, episode steps:   1, steps per second: 306, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  587/10000: episode: 539, duration: 0.004s, episode steps:   2, steps per second: 558, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  588/10000: episode: 540, duration: 0.002s, episode steps:   1, steps per second: 417, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  589/10000: episode: 541, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  590/10000: episode: 542, duration: 0.002s, episode steps:   1, steps per second: 603, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  591/10000: episode: 543, duration: 0.002s, episode steps:   1, steps per second: 551, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  592/10000: episode: 544, duration: 0.002s, episode steps:   1, steps per second: 579, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  593/10000: episode: 545, duration: 0.001s, episode steps:   1, steps per second: 669, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  594/10000: episode: 546, duration: 0.002s, episode steps:   1, steps per second: 587, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  595/10000: episode: 547, duration: 0.002s, episode steps:   1, steps per second: 594, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  597/10000: episode: 548, duration: 0.004s, episode steps:   2, steps per second: 533, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [3.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  598/10000: episode: 549, duration: 0.002s, episode steps:   1, steps per second: 544, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  599/10000: episode: 550, duration: 0.002s, episode steps:   1, steps per second: 612, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  600/10000: episode: 551, duration: 0.002s, episode steps:   1, steps per second: 613, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  601/10000: episode: 552, duration: 0.002s, episode steps:   1, steps per second: 544, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  602/10000: episode: 553, duration: 0.002s, episode steps:   1, steps per second: 539, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  604/10000: episode: 554, duration: 0.002s, episode steps:   2, steps per second: 866, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  605/10000: episode: 555, duration: 0.002s, episode steps:   1, steps per second: 431, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  606/10000: episode: 556, duration: 0.002s, episode steps:   1, steps per second: 601, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  607/10000: episode: 557, duration: 0.001s, episode steps:   1, steps per second: 673, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  609/10000: episode: 558, duration: 0.002s, episode steps:   2, steps per second: 912, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [1.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  610/10000: episode: 559, duration: 0.002s, episode steps:   1, steps per second: 566, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  611/10000: episode: 560, duration: 0.001s, episode steps:   1, steps per second: 697, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  612/10000: episode: 561, duration: 0.001s, episode steps:   1, steps per second: 686, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  613/10000: episode: 562, duration: 0.003s, episode steps:   1, steps per second: 400, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  614/10000: episode: 563, duration: 0.002s, episode steps:   1, steps per second: 489, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  615/10000: episode: 564, duration: 0.002s, episode steps:   1, steps per second: 641, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  616/10000: episode: 565, duration: 0.002s, episode steps:   1, steps per second: 639, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  618/10000: episode: 566, duration: 0.002s, episode steps:   2, steps per second: 811, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  619/10000: episode: 567, duration: 0.001s, episode steps:   1, steps per second: 672, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  620/10000: episode: 568, duration: 0.001s, episode steps:   1, steps per second: 691, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  621/10000: episode: 569, duration: 0.002s, episode steps:   1, steps per second: 666, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  622/10000: episode: 570, duration: 0.002s, episode steps:   1, steps per second: 423, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  623/10000: episode: 571, duration: 0.001s, episode steps:   1, steps per second: 679, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  627/10000: episode: 572, duration: 0.004s, episode steps:   4, steps per second: 1113, episode reward: 70.000, mean reward: 17.500 [-10.000, 100.000], mean action: 7.250 [4.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  628/10000: episode: 573, duration: 0.002s, episode steps:   1, steps per second: 592, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  629/10000: episode: 574, duration: 0.002s, episode steps:   1, steps per second: 592, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  630/10000: episode: 575, duration: 0.002s, episode steps:   1, steps per second: 659, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  631/10000: episode: 576, duration: 0.001s, episode steps:   1, steps per second: 680, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  632/10000: episode: 577, duration: 0.003s, episode steps:   1, steps per second: 381, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  633/10000: episode: 578, duration: 0.002s, episode steps:   1, steps per second: 516, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  634/10000: episode: 579, duration: 0.001s, episode steps:   1, steps per second: 668, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  635/10000: episode: 580, duration: 0.002s, episode steps:   1, steps per second: 641, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  638/10000: episode: 581, duration: 0.003s, episode steps:   3, steps per second: 973, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 3.333 [1.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  639/10000: episode: 582, duration: 0.002s, episode steps:   1, steps per second: 639, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  640/10000: episode: 583, duration: 0.002s, episode steps:   1, steps per second: 620, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  641/10000: episode: 584, duration: 0.002s, episode steps:   1, steps per second: 420, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  642/10000: episode: 585, duration: 0.002s, episode steps:   1, steps per second: 613, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  643/10000: episode: 586, duration: 0.002s, episode steps:   1, steps per second: 652, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  644/10000: episode: 587, duration: 0.001s, episode steps:   1, steps per second: 674, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  645/10000: episode: 588, duration: 0.002s, episode steps:   1, steps per second: 648, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  647/10000: episode: 589, duration: 0.002s, episode steps:   2, steps per second: 903, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [3.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  648/10000: episode: 590, duration: 0.001s, episode steps:   1, steps per second: 670, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  649/10000: episode: 591, duration: 0.001s, episode steps:   1, steps per second: 701, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  650/10000: episode: 592, duration: 0.002s, episode steps:   1, steps per second: 440, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  651/10000: episode: 593, duration: 0.002s, episode steps:   1, steps per second: 594, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  652/10000: episode: 594, duration: 0.002s, episode steps:   1, steps per second: 650, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  653/10000: episode: 595, duration: 0.002s, episode steps:   1, steps per second: 632, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  654/10000: episode: 596, duration: 0.002s, episode steps:   1, steps per second: 643, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  655/10000: episode: 597, duration: 0.001s, episode steps:   1, steps per second: 670, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  656/10000: episode: 598, duration: 0.001s, episode steps:   1, steps per second: 673, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  657/10000: episode: 599, duration: 0.003s, episode steps:   1, steps per second: 302, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  658/10000: episode: 600, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  659/10000: episode: 601, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  660/10000: episode: 602, duration: 0.004s, episode steps:   1, steps per second: 262, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  661/10000: episode: 603, duration: 0.005s, episode steps:   1, steps per second: 216, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  662/10000: episode: 604, duration: 0.005s, episode steps:   1, steps per second: 210, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  664/10000: episode: 605, duration: 0.007s, episode steps:   2, steps per second: 307, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [2.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  665/10000: episode: 606, duration: 0.004s, episode steps:   1, steps per second: 228, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  666/10000: episode: 607, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  667/10000: episode: 608, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  668/10000: episode: 609, duration: 0.004s, episode steps:   1, steps per second: 230, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  669/10000: episode: 610, duration: 0.006s, episode steps:   1, steps per second: 163, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  670/10000: episode: 611, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  671/10000: episode: 612, duration: 0.006s, episode steps:   1, steps per second: 155, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  673/10000: episode: 613, duration: 0.010s, episode steps:   2, steps per second: 196, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [4.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  674/10000: episode: 614, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  675/10000: episode: 615, duration: 0.005s, episode steps:   1, steps per second: 185, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  676/10000: episode: 616, duration: 0.004s, episode steps:   1, steps per second: 261, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  678/10000: episode: 617, duration: 0.006s, episode steps:   2, steps per second: 325, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [4.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  679/10000: episode: 618, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  680/10000: episode: 619, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  681/10000: episode: 620, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  682/10000: episode: 621, duration: 0.005s, episode steps:   1, steps per second: 206, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  683/10000: episode: 622, duration: 0.004s, episode steps:   1, steps per second: 259, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  684/10000: episode: 623, duration: 0.004s, episode steps:   1, steps per second: 271, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  685/10000: episode: 624, duration: 0.005s, episode steps:   1, steps per second: 221, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  686/10000: episode: 625, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  687/10000: episode: 626, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  688/10000: episode: 627, duration: 0.004s, episode steps:   1, steps per second: 256, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  690/10000: episode: 628, duration: 0.007s, episode steps:   2, steps per second: 275, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [0.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  691/10000: episode: 629, duration: 0.004s, episode steps:   1, steps per second: 238, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  692/10000: episode: 630, duration: 0.005s, episode steps:   1, steps per second: 208, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  693/10000: episode: 631, duration: 0.005s, episode steps:   1, steps per second: 191, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  694/10000: episode: 632, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  695/10000: episode: 633, duration: 0.007s, episode steps:   1, steps per second: 150, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  696/10000: episode: 634, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  697/10000: episode: 635, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  698/10000: episode: 636, duration: 0.007s, episode steps:   1, steps per second: 151, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  699/10000: episode: 637, duration: 0.004s, episode steps:   1, steps per second: 255, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  700/10000: episode: 638, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  701/10000: episode: 639, duration: 0.005s, episode steps:   1, steps per second: 184, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  702/10000: episode: 640, duration: 0.005s, episode steps:   1, steps per second: 201, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  703/10000: episode: 641, duration: 0.006s, episode steps:   1, steps per second: 168, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  704/10000: episode: 642, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  705/10000: episode: 643, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  708/10000: episode: 644, duration: 0.011s, episode steps:   3, steps per second: 280, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  709/10000: episode: 645, duration: 0.005s, episode steps:   1, steps per second: 212, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  710/10000: episode: 646, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  711/10000: episode: 647, duration: 0.006s, episode steps:   1, steps per second: 160, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  712/10000: episode: 648, duration: 0.006s, episode steps:   1, steps per second: 176, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  713/10000: episode: 649, duration: 0.004s, episode steps:   1, steps per second: 247, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  714/10000: episode: 650, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  715/10000: episode: 651, duration: 0.004s, episode steps:   1, steps per second: 242, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  717/10000: episode: 652, duration: 0.007s, episode steps:   2, steps per second: 307, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [5.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  718/10000: episode: 653, duration: 0.007s, episode steps:   1, steps per second: 147, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  720/10000: episode: 654, duration: 0.006s, episode steps:   2, steps per second: 316, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [8.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  721/10000: episode: 655, duration: 0.006s, episode steps:   1, steps per second: 174, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  722/10000: episode: 656, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  723/10000: episode: 657, duration: 0.006s, episode steps:   1, steps per second: 166, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  724/10000: episode: 658, duration: 0.005s, episode steps:   1, steps per second: 204, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  725/10000: episode: 659, duration: 0.004s, episode steps:   1, steps per second: 228, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  726/10000: episode: 660, duration: 0.006s, episode steps:   1, steps per second: 159, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  727/10000: episode: 661, duration: 0.003s, episode steps:   1, steps per second: 333, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  728/10000: episode: 662, duration: 0.003s, episode steps:   1, steps per second: 338, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  729/10000: episode: 663, duration: 0.003s, episode steps:   1, steps per second: 322, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  730/10000: episode: 664, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  731/10000: episode: 665, duration: 0.003s, episode steps:   1, steps per second: 341, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  732/10000: episode: 666, duration: 0.004s, episode steps:   1, steps per second: 274, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  733/10000: episode: 667, duration: 0.003s, episode steps:   1, steps per second: 332, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  734/10000: episode: 668, duration: 0.004s, episode steps:   1, steps per second: 275, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  735/10000: episode: 669, duration: 0.003s, episode steps:   1, steps per second: 336, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  736/10000: episode: 670, duration: 0.003s, episode steps:   1, steps per second: 375, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  738/10000: episode: 671, duration: 0.004s, episode steps:   2, steps per second: 445, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  739/10000: episode: 672, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  740/10000: episode: 673, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  741/10000: episode: 674, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  742/10000: episode: 675, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  743/10000: episode: 676, duration: 0.003s, episode steps:   1, steps per second: 330, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  745/10000: episode: 677, duration: 0.004s, episode steps:   2, steps per second: 470, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [1.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  746/10000: episode: 678, duration: 0.003s, episode steps:   1, steps per second: 341, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  747/10000: episode: 679, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  748/10000: episode: 680, duration: 0.005s, episode steps:   1, steps per second: 207, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  749/10000: episode: 681, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  751/10000: episode: 682, duration: 0.006s, episode steps:   2, steps per second: 316, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [4.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  752/10000: episode: 683, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  753/10000: episode: 684, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  754/10000: episode: 685, duration: 0.003s, episode steps:   1, steps per second: 300, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  755/10000: episode: 686, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  756/10000: episode: 687, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  757/10000: episode: 688, duration: 0.003s, episode steps:   1, steps per second: 311, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  758/10000: episode: 689, duration: 0.005s, episode steps:   1, steps per second: 211, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  759/10000: episode: 690, duration: 0.004s, episode steps:   1, steps per second: 236, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  760/10000: episode: 691, duration: 0.004s, episode steps:   1, steps per second: 232, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  761/10000: episode: 692, duration: 0.003s, episode steps:   1, steps per second: 291, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  762/10000: episode: 693, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  763/10000: episode: 694, duration: 0.003s, episode steps:   1, steps per second: 348, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  764/10000: episode: 695, duration: 0.003s, episode steps:   1, steps per second: 343, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  765/10000: episode: 696, duration: 0.003s, episode steps:   1, steps per second: 362, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  766/10000: episode: 697, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  767/10000: episode: 698, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  769/10000: episode: 699, duration: 0.005s, episode steps:   2, steps per second: 426, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [1.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  770/10000: episode: 700, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  771/10000: episode: 701, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  772/10000: episode: 702, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  773/10000: episode: 703, duration: 0.003s, episode steps:   1, steps per second: 363, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  774/10000: episode: 704, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  775/10000: episode: 705, duration: 0.005s, episode steps:   1, steps per second: 200, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  776/10000: episode: 706, duration: 0.004s, episode steps:   1, steps per second: 252, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  777/10000: episode: 707, duration: 0.005s, episode steps:   1, steps per second: 202, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  779/10000: episode: 708, duration: 0.005s, episode steps:   2, steps per second: 434, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.000 [0.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  780/10000: episode: 709, duration: 0.003s, episode steps:   1, steps per second: 328, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  781/10000: episode: 710, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  782/10000: episode: 711, duration: 0.004s, episode steps:   1, steps per second: 264, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  783/10000: episode: 712, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  784/10000: episode: 713, duration: 0.003s, episode steps:   1, steps per second: 353, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  785/10000: episode: 714, duration: 0.003s, episode steps:   1, steps per second: 350, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  786/10000: episode: 715, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  787/10000: episode: 716, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  788/10000: episode: 717, duration: 0.003s, episode steps:   1, steps per second: 349, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  789/10000: episode: 718, duration: 0.003s, episode steps:   1, steps per second: 360, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  790/10000: episode: 719, duration: 0.005s, episode steps:   1, steps per second: 194, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  791/10000: episode: 720, duration: 0.003s, episode steps:   1, steps per second: 299, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  792/10000: episode: 721, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  793/10000: episode: 722, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  795/10000: episode: 723, duration: 0.006s, episode steps:   2, steps per second: 352, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  796/10000: episode: 724, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  797/10000: episode: 725, duration: 0.004s, episode steps:   1, steps per second: 266, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  798/10000: episode: 726, duration: 0.003s, episode steps:   1, steps per second: 324, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  799/10000: episode: 727, duration: 0.004s, episode steps:   1, steps per second: 277, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  800/10000: episode: 728, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  802/10000: episode: 729, duration: 0.007s, episode steps:   2, steps per second: 283, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [4.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  803/10000: episode: 730, duration: 0.003s, episode steps:   1, steps per second: 349, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  804/10000: episode: 731, duration: 0.003s, episode steps:   1, steps per second: 335, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  805/10000: episode: 732, duration: 0.003s, episode steps:   1, steps per second: 311, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  806/10000: episode: 733, duration: 0.003s, episode steps:   1, steps per second: 307, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  807/10000: episode: 734, duration: 0.003s, episode steps:   1, steps per second: 349, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  808/10000: episode: 735, duration: 0.003s, episode steps:   1, steps per second: 356, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  809/10000: episode: 736, duration: 0.003s, episode steps:   1, steps per second: 357, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  810/10000: episode: 737, duration: 0.003s, episode steps:   1, steps per second: 288, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  811/10000: episode: 738, duration: 0.003s, episode steps:   1, steps per second: 354, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  812/10000: episode: 739, duration: 0.003s, episode steps:   1, steps per second: 365, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  813/10000: episode: 740, duration: 0.003s, episode steps:   1, steps per second: 359, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  814/10000: episode: 741, duration: 0.004s, episode steps:   1, steps per second: 274, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  815/10000: episode: 742, duration: 0.003s, episode steps:   1, steps per second: 346, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  816/10000: episode: 743, duration: 0.003s, episode steps:   1, steps per second: 370, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  818/10000: episode: 744, duration: 0.004s, episode steps:   2, steps per second: 498, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [5.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  819/10000: episode: 745, duration: 0.003s, episode steps:   1, steps per second: 330, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  820/10000: episode: 746, duration: 0.003s, episode steps:   1, steps per second: 312, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  821/10000: episode: 747, duration: 0.003s, episode steps:   1, steps per second: 348, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  822/10000: episode: 748, duration: 0.003s, episode steps:   1, steps per second: 309, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  823/10000: episode: 749, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  824/10000: episode: 750, duration: 0.003s, episode steps:   1, steps per second: 314, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  825/10000: episode: 751, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  826/10000: episode: 752, duration: 0.003s, episode steps:   1, steps per second: 334, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  827/10000: episode: 753, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  828/10000: episode: 754, duration: 0.005s, episode steps:   1, steps per second: 209, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  830/10000: episode: 755, duration: 0.005s, episode steps:   2, steps per second: 396, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [4.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  831/10000: episode: 756, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  834/10000: episode: 757, duration: 0.007s, episode steps:   3, steps per second: 415, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 5.333 [4.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  835/10000: episode: 758, duration: 0.003s, episode steps:   1, steps per second: 326, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  836/10000: episode: 759, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  837/10000: episode: 760, duration: 0.003s, episode steps:   1, steps per second: 299, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  838/10000: episode: 761, duration: 0.003s, episode steps:   1, steps per second: 320, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  839/10000: episode: 762, duration: 0.004s, episode steps:   1, steps per second: 281, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  840/10000: episode: 763, duration: 0.005s, episode steps:   1, steps per second: 187, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  841/10000: episode: 764, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  842/10000: episode: 765, duration: 0.004s, episode steps:   1, steps per second: 243, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  844/10000: episode: 766, duration: 0.007s, episode steps:   2, steps per second: 303, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [1.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  845/10000: episode: 767, duration: 0.004s, episode steps:   1, steps per second: 270, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  847/10000: episode: 768, duration: 0.004s, episode steps:   2, steps per second: 477, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [1.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  848/10000: episode: 769, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  850/10000: episode: 770, duration: 0.005s, episode steps:   2, steps per second: 400, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [4.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  851/10000: episode: 771, duration: 0.004s, episode steps:   1, steps per second: 239, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  852/10000: episode: 772, duration: 0.004s, episode steps:   1, steps per second: 262, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  854/10000: episode: 773, duration: 0.005s, episode steps:   2, steps per second: 381, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [9.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  855/10000: episode: 774, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  856/10000: episode: 775, duration: 0.004s, episode steps:   1, steps per second: 251, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  857/10000: episode: 776, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  859/10000: episode: 777, duration: 0.004s, episode steps:   2, steps per second: 456, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.000 [9.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  861/10000: episode: 778, duration: 0.004s, episode steps:   2, steps per second: 454, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  862/10000: episode: 779, duration: 0.005s, episode steps:   1, steps per second: 203, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  864/10000: episode: 780, duration: 0.005s, episode steps:   2, steps per second: 387, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [5.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  865/10000: episode: 781, duration: 0.003s, episode steps:   1, steps per second: 312, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  866/10000: episode: 782, duration: 0.005s, episode steps:   1, steps per second: 219, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  867/10000: episode: 783, duration: 0.003s, episode steps:   1, steps per second: 326, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  868/10000: episode: 784, duration: 0.003s, episode steps:   1, steps per second: 315, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  869/10000: episode: 785, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  870/10000: episode: 786, duration: 0.003s, episode steps:   1, steps per second: 297, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  871/10000: episode: 787, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  872/10000: episode: 788, duration: 0.003s, episode steps:   1, steps per second: 332, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  873/10000: episode: 789, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  874/10000: episode: 790, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  876/10000: episode: 791, duration: 0.006s, episode steps:   2, steps per second: 338, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [7.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  877/10000: episode: 792, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  878/10000: episode: 793, duration: 0.003s, episode steps:   1, steps per second: 291, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  879/10000: episode: 794, duration: 0.003s, episode steps:   1, steps per second: 367, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  880/10000: episode: 795, duration: 0.003s, episode steps:   1, steps per second: 305, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  881/10000: episode: 796, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  883/10000: episode: 797, duration: 0.005s, episode steps:   2, steps per second: 428, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [1.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  884/10000: episode: 798, duration: 0.003s, episode steps:   1, steps per second: 321, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  885/10000: episode: 799, duration: 0.004s, episode steps:   1, steps per second: 246, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  886/10000: episode: 800, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  887/10000: episode: 801, duration: 0.003s, episode steps:   1, steps per second: 307, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  888/10000: episode: 802, duration: 0.003s, episode steps:   1, steps per second: 368, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  889/10000: episode: 803, duration: 0.003s, episode steps:   1, steps per second: 341, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  890/10000: episode: 804, duration: 0.004s, episode steps:   1, steps per second: 269, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  891/10000: episode: 805, duration: 0.003s, episode steps:   1, steps per second: 329, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  892/10000: episode: 806, duration: 0.003s, episode steps:   1, steps per second: 363, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  893/10000: episode: 807, duration: 0.003s, episode steps:   1, steps per second: 343, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  894/10000: episode: 808, duration: 0.005s, episode steps:   1, steps per second: 193, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  895/10000: episode: 809, duration: 0.004s, episode steps:   1, steps per second: 245, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  896/10000: episode: 810, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  897/10000: episode: 811, duration: 0.005s, episode steps:   1, steps per second: 217, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  898/10000: episode: 812, duration: 0.004s, episode steps:   1, steps per second: 279, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  899/10000: episode: 813, duration: 0.004s, episode steps:   1, steps per second: 268, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  900/10000: episode: 814, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  901/10000: episode: 815, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  902/10000: episode: 816, duration: 0.004s, episode steps:   1, steps per second: 257, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  903/10000: episode: 817, duration: 0.005s, episode steps:   1, steps per second: 195, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  904/10000: episode: 818, duration: 0.005s, episode steps:   1, steps per second: 205, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  905/10000: episode: 819, duration: 0.004s, episode steps:   1, steps per second: 226, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  906/10000: episode: 820, duration: 0.004s, episode steps:   1, steps per second: 223, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  907/10000: episode: 821, duration: 0.004s, episode steps:   1, steps per second: 250, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  908/10000: episode: 822, duration: 0.003s, episode steps:   1, steps per second: 364, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  909/10000: episode: 823, duration: 0.003s, episode steps:   1, steps per second: 343, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  910/10000: episode: 824, duration: 0.003s, episode steps:   1, steps per second: 294, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  911/10000: episode: 825, duration: 0.004s, episode steps:   1, steps per second: 277, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  912/10000: episode: 826, duration: 0.003s, episode steps:   1, steps per second: 380, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  913/10000: episode: 827, duration: 0.003s, episode steps:   1, steps per second: 362, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  914/10000: episode: 828, duration: 0.003s, episode steps:   1, steps per second: 370, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  915/10000: episode: 829, duration: 0.004s, episode steps:   1, steps per second: 284, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  916/10000: episode: 830, duration: 0.003s, episode steps:   1, steps per second: 308, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  917/10000: episode: 831, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  918/10000: episode: 832, duration: 0.003s, episode steps:   1, steps per second: 363, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  919/10000: episode: 833, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  920/10000: episode: 834, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  921/10000: episode: 835, duration: 0.003s, episode steps:   1, steps per second: 336, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  922/10000: episode: 836, duration: 0.003s, episode steps:   1, steps per second: 318, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  923/10000: episode: 837, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  924/10000: episode: 838, duration: 0.003s, episode steps:   1, steps per second: 326, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  926/10000: episode: 839, duration: 0.005s, episode steps:   2, steps per second: 442, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [1.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  927/10000: episode: 840, duration: 0.005s, episode steps:   1, steps per second: 220, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  928/10000: episode: 841, duration: 0.003s, episode steps:   1, steps per second: 327, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  929/10000: episode: 842, duration: 0.003s, episode steps:   1, steps per second: 312, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  930/10000: episode: 843, duration: 0.003s, episode steps:   1, steps per second: 311, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  931/10000: episode: 844, duration: 0.004s, episode steps:   1, steps per second: 224, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  932/10000: episode: 845, duration: 0.003s, episode steps:   1, steps per second: 304, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  933/10000: episode: 846, duration: 0.003s, episode steps:   1, steps per second: 347, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  934/10000: episode: 847, duration: 0.003s, episode steps:   1, steps per second: 336, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  935/10000: episode: 848, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  936/10000: episode: 849, duration: 0.003s, episode steps:   1, steps per second: 321, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  937/10000: episode: 850, duration: 0.003s, episode steps:   1, steps per second: 320, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  938/10000: episode: 851, duration: 0.003s, episode steps:   1, steps per second: 357, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  940/10000: episode: 852, duration: 0.006s, episode steps:   2, steps per second: 346, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.000 [9.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  941/10000: episode: 853, duration: 0.003s, episode steps:   1, steps per second: 342, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  942/10000: episode: 854, duration: 0.003s, episode steps:   1, steps per second: 293, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  943/10000: episode: 855, duration: 0.003s, episode steps:   1, steps per second: 296, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  944/10000: episode: 856, duration: 0.004s, episode steps:   1, steps per second: 249, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  945/10000: episode: 857, duration: 0.004s, episode steps:   1, steps per second: 258, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  946/10000: episode: 858, duration: 0.003s, episode steps:   1, steps per second: 358, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  947/10000: episode: 859, duration: 0.003s, episode steps:   1, steps per second: 377, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  948/10000: episode: 860, duration: 0.004s, episode steps:   1, steps per second: 260, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  949/10000: episode: 861, duration: 0.003s, episode steps:   1, steps per second: 353, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  950/10000: episode: 862, duration: 0.003s, episode steps:   1, steps per second: 319, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  951/10000: episode: 863, duration: 0.004s, episode steps:   1, steps per second: 240, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  952/10000: episode: 864, duration: 0.003s, episode steps:   1, steps per second: 301, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  953/10000: episode: 865, duration: 0.003s, episode steps:   1, steps per second: 292, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  954/10000: episode: 866, duration: 0.005s, episode steps:   1, steps per second: 222, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  955/10000: episode: 867, duration: 0.004s, episode steps:   1, steps per second: 262, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  956/10000: episode: 868, duration: 0.003s, episode steps:   1, steps per second: 301, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  957/10000: episode: 869, duration: 0.004s, episode steps:   1, steps per second: 228, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  958/10000: episode: 870, duration: 0.003s, episode steps:   1, steps per second: 286, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  959/10000: episode: 871, duration: 0.004s, episode steps:   1, steps per second: 233, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  960/10000: episode: 872, duration: 0.003s, episode steps:   1, steps per second: 323, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  961/10000: episode: 873, duration: 0.003s, episode steps:   1, steps per second: 337, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  962/10000: episode: 874, duration: 0.004s, episode steps:   1, steps per second: 280, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  963/10000: episode: 875, duration: 0.003s, episode steps:   1, steps per second: 298, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  964/10000: episode: 876, duration: 0.003s, episode steps:   1, steps per second: 322, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  965/10000: episode: 877, duration: 0.003s, episode steps:   1, steps per second: 340, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  966/10000: episode: 878, duration: 0.003s, episode steps:   1, steps per second: 327, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  967/10000: episode: 879, duration: 0.004s, episode steps:   1, steps per second: 262, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  968/10000: episode: 880, duration: 0.004s, episode steps:   1, steps per second: 256, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: --, accuracy: --, mean_q: --\n",
      "  969/10000: episode: 881, duration: 0.003s, episode steps:   1, steps per second: 314, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  970/10000: episode: 882, duration: 0.004s, episode steps:   1, steps per second: 278, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  971/10000: episode: 883, duration: 0.004s, episode steps:   1, steps per second: 244, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  972/10000: episode: 884, duration: 0.003s, episode steps:   1, steps per second: 321, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  973/10000: episode: 885, duration: 0.003s, episode steps:   1, steps per second: 345, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  974/10000: episode: 886, duration: 0.004s, episode steps:   1, steps per second: 265, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  975/10000: episode: 887, duration: 0.004s, episode steps:   1, steps per second: 276, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  976/10000: episode: 888, duration: 0.003s, episode steps:   1, steps per second: 331, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  977/10000: episode: 889, duration: 0.003s, episode steps:   1, steps per second: 326, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      "  978/10000: episode: 890, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  980/10000: episode: 891, duration: 0.005s, episode steps:   2, steps per second: 429, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [1.000, 5.000],  loss: --, accuracy: --, mean_q: --\n",
      "  981/10000: episode: 892, duration: 0.003s, episode steps:   1, steps per second: 346, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  982/10000: episode: 893, duration: 0.003s, episode steps:   1, steps per second: 339, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: --, accuracy: --, mean_q: --\n",
      "  983/10000: episode: 894, duration: 0.004s, episode steps:   1, steps per second: 234, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: --, accuracy: --, mean_q: --\n",
      "  984/10000: episode: 895, duration: 0.003s, episode steps:   1, steps per second: 287, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  986/10000: episode: 896, duration: 0.004s, episode steps:   2, steps per second: 474, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [1.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  987/10000: episode: 897, duration: 0.003s, episode steps:   1, steps per second: 376, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  988/10000: episode: 898, duration: 0.004s, episode steps:   1, steps per second: 238, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  989/10000: episode: 899, duration: 0.003s, episode steps:   1, steps per second: 336, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  990/10000: episode: 900, duration: 0.003s, episode steps:   1, steps per second: 354, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: --, accuracy: --, mean_q: --\n",
      "  991/10000: episode: 901, duration: 0.003s, episode steps:   1, steps per second: 364, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  992/10000: episode: 902, duration: 0.005s, episode steps:   1, steps per second: 214, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: --, accuracy: --, mean_q: --\n",
      "  993/10000: episode: 903, duration: 0.004s, episode steps:   1, steps per second: 254, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: --, accuracy: --, mean_q: --\n",
      "  994/10000: episode: 904, duration: 0.003s, episode steps:   1, steps per second: 319, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  995/10000: episode: 905, duration: 0.003s, episode steps:   1, steps per second: 380, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: --, accuracy: --, mean_q: --\n",
      "  996/10000: episode: 906, duration: 0.004s, episode steps:   1, steps per second: 272, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: --, accuracy: --, mean_q: --\n",
      "  997/10000: episode: 907, duration: 0.003s, episode steps:   1, steps per second: 344, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: --, accuracy: --, mean_q: --\n",
      "  998/10000: episode: 908, duration: 0.003s, episode steps:   1, steps per second: 358, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n",
      "  999/10000: episode: 909, duration: 0.003s, episode steps:   1, steps per second: 364, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      " 1000/10000: episode: 910, duration: 0.004s, episode steps:   1, steps per second: 263, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: --, accuracy: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\משתמש\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1001/10000: episode: 911, duration: 1.153s, episode steps:   1, steps per second:   1, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: --, accuracy: --, mean_q: --\n",
      " 1002/10000: episode: 912, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 195.889999, accuracy: 0.843750, mean_q: 113.232330\n",
      " 1003/10000: episode: 913, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 70.562363, accuracy: 0.906250, mean_q: 106.747871\n",
      " 1004/10000: episode: 914, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 8.325943, accuracy: 1.000000, mean_q: 103.206612\n",
      " 1006/10000: episode: 915, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [3.000, 9.000],  loss: 116.491699, accuracy: 0.875000, mean_q: 98.301941\n",
      " 1007/10000: episode: 916, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 122.171539, accuracy: 0.906250, mean_q: 97.047340\n",
      " 1008/10000: episode: 917, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 12.560904, accuracy: 1.000000, mean_q: 98.950874\n",
      " 1009/10000: episode: 918, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 40.795120, accuracy: 0.937500, mean_q: 98.660583\n",
      " 1010/10000: episode: 919, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 106.272552, accuracy: 0.906250, mean_q: 99.630569\n",
      " 1011/10000: episode: 920, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 112.538399, accuracy: 0.843750, mean_q: 100.732918\n",
      " 1012/10000: episode: 921, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.739551, accuracy: 1.000000, mean_q: 102.168190\n",
      " 1013/10000: episode: 922, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 6.032176, accuracy: 1.000000, mean_q: 102.471725\n",
      " 1014/10000: episode: 923, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 48.731335, accuracy: 0.937500, mean_q: 102.363785\n",
      " 1015/10000: episode: 924, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 40.711536, accuracy: 0.937500, mean_q: 103.309875\n",
      " 1016/10000: episode: 925, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 37.191582, accuracy: 0.968750, mean_q: 103.025024\n",
      " 1017/10000: episode: 926, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 40.611515, accuracy: 0.937500, mean_q: 102.225372\n",
      " 1018/10000: episode: 927, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 31.825539, accuracy: 0.968750, mean_q: 101.555862\n",
      " 1019/10000: episode: 928, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 14.716548, accuracy: 0.937500, mean_q: 100.873398\n",
      " 1020/10000: episode: 929, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.826762, accuracy: 1.000000, mean_q: 101.079071\n",
      " 1023/10000: episode: 930, duration: 0.035s, episode steps:   3, steps per second:  85, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 5.667 [0.000, 11.000],  loss: 40.202663, accuracy: 0.927083, mean_q: 100.953438\n",
      " 1024/10000: episode: 931, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 33.686962, accuracy: 0.937500, mean_q: 101.320709\n",
      " 1025/10000: episode: 932, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 54.779739, accuracy: 0.875000, mean_q: 101.124512\n",
      " 1027/10000: episode: 933, duration: 0.023s, episode steps:   2, steps per second:  85, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [4.000, 5.000],  loss: 41.267040, accuracy: 0.953125, mean_q: 102.069122\n",
      " 1028/10000: episode: 934, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 35.963440, accuracy: 0.906250, mean_q: 101.844513\n",
      " 1029/10000: episode: 935, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 74.770706, accuracy: 0.875000, mean_q: 101.491989\n",
      " 1030/10000: episode: 936, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.190714, accuracy: 0.968750, mean_q: 101.351410\n",
      " 1050/10000: episode: 937, duration: 0.167s, episode steps:  20, steps per second: 120, episode reward: -90.000, mean reward: -4.500 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 18.336754, accuracy: 0.853125, mean_q: 100.874832\n",
      " 1051/10000: episode: 938, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.996209, accuracy: 0.875000, mean_q: 100.182419\n",
      " 1052/10000: episode: 939, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 87.590286, accuracy: 0.718750, mean_q: 99.804161\n",
      " 1053/10000: episode: 940, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.429155, accuracy: 0.937500, mean_q: 100.363541\n",
      " 1055/10000: episode: 941, duration: 0.020s, episode steps:   2, steps per second:  98, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [3.000, 6.000],  loss: 16.204365, accuracy: 0.718750, mean_q: 100.720665\n",
      " 1056/10000: episode: 942, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.405015, accuracy: 0.750000, mean_q: 101.221985\n",
      " 1057/10000: episode: 943, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 8.815891, accuracy: 0.750000, mean_q: 101.139420\n",
      " 1058/10000: episode: 944, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 41.872353, accuracy: 0.718750, mean_q: 101.244736\n",
      " 1059/10000: episode: 945, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 10.236951, accuracy: 0.718750, mean_q: 101.309967\n",
      " 1060/10000: episode: 946, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 29.230318, accuracy: 0.593750, mean_q: 102.798607\n",
      " 1062/10000: episode: 947, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 19.480167, accuracy: 0.656250, mean_q: 101.559586\n",
      " 1063/10000: episode: 948, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 56.063362, accuracy: 0.656250, mean_q: 101.512520\n",
      " 1083/10000: episode: 949, duration: 0.149s, episode steps:  20, steps per second: 134, episode reward: -200.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.050 [7.000, 8.000],  loss: 8.920755, accuracy: 0.645312, mean_q: 101.540474\n",
      " 1084/10000: episode: 950, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 29.933006, accuracy: 0.531250, mean_q: 102.201233\n",
      " 1100/10000: episode: 951, duration: 0.105s, episode steps:  16, steps per second: 153, episode reward: -50.000, mean reward: -3.125 [-10.000, 100.000], mean action: 3.750 [1.000, 4.000],  loss: 18.397026, accuracy: 0.710938, mean_q: 100.889755\n",
      " 1101/10000: episode: 952, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 5.830799, accuracy: 0.750000, mean_q: 101.178009\n",
      " 1121/10000: episode: 953, duration: 0.148s, episode steps:  20, steps per second: 135, episode reward: -200.000, mean reward: -10.000 [-10.000, -10.000], mean action: 9.750 [5.000, 10.000],  loss: 18.845835, accuracy: 0.764063, mean_q: 101.081902\n",
      " 1122/10000: episode: 954, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 12.351734, accuracy: 0.687500, mean_q: 100.882202\n",
      " 1123/10000: episode: 955, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 16.925432, accuracy: 0.750000, mean_q: 100.554413\n",
      " 1125/10000: episode: 956, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [0.000, 8.000],  loss: 107.821045, accuracy: 0.593750, mean_q: 100.659569\n",
      " 1126/10000: episode: 957, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.872629, accuracy: 0.656250, mean_q: 100.637283\n",
      " 1127/10000: episode: 958, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 16.073673, accuracy: 0.625000, mean_q: 101.321831\n",
      " 1128/10000: episode: 959, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 10.306740, accuracy: 0.531250, mean_q: 102.108658\n",
      " 1148/10000: episode: 960, duration: 0.281s, episode steps:  20, steps per second:  71, episode reward: -200.000, mean reward: -10.000 [-10.000, -10.000], mean action: 5.900 [3.000, 7.000],  loss: 7.315814, accuracy: 0.545313, mean_q: 102.125412\n",
      " 1149/10000: episode: 961, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 6.793239, accuracy: 0.343750, mean_q: 100.764427\n",
      " 1169/10000: episode: 962, duration: 0.344s, episode steps:  20, steps per second:  58, episode reward: -200.000, mean reward: -10.000 [-10.000, -10.000], mean action: 8.950 [8.000, 9.000],  loss: 13.804919, accuracy: 0.589063, mean_q: 101.079124\n",
      " 1170/10000: episode: 963, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 8.263176, accuracy: 0.625000, mean_q: 101.303261\n",
      " 1171/10000: episode: 964, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.415455, accuracy: 0.656250, mean_q: 100.317413\n",
      " 1172/10000: episode: 965, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 5.274012, accuracy: 0.781250, mean_q: 99.270897\n",
      " 1173/10000: episode: 966, duration: 0.033s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 9.506899, accuracy: 0.531250, mean_q: 100.092201\n",
      " 1174/10000: episode: 967, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 4.713242, accuracy: 0.750000, mean_q: 99.182419\n",
      " 1177/10000: episode: 968, duration: 0.084s, episode steps:   3, steps per second:  36, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 8.667 [8.000, 10.000],  loss: 3.326021, accuracy: 0.645833, mean_q: 100.953850\n",
      " 1178/10000: episode: 969, duration: 0.044s, episode steps:   1, steps per second:  23, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 5.606071, accuracy: 0.718750, mean_q: 101.755997\n",
      " 1179/10000: episode: 970, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 154.508133, accuracy: 0.750000, mean_q: 100.634819\n",
      " 1180/10000: episode: 971, duration: 0.042s, episode steps:   1, steps per second:  24, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 6.685953, accuracy: 0.781250, mean_q: 99.784660\n",
      " 1181/10000: episode: 972, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 11.982679, accuracy: 0.625000, mean_q: 99.821274\n",
      " 1182/10000: episode: 973, duration: 0.038s, episode steps:   1, steps per second:  26, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 8.906676, accuracy: 0.687500, mean_q: 98.899651\n",
      " 1200/10000: episode: 974, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: -70.000, mean reward: -3.889 [-10.000, 100.000], mean action: 7.556 [1.000, 9.000],  loss: 16.506786, accuracy: 0.720486, mean_q: 100.797775\n",
      " 1201/10000: episode: 975, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 8.147358, accuracy: 0.625000, mean_q: 100.687561\n",
      " 1202/10000: episode: 976, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.708214, accuracy: 0.812500, mean_q: 101.011436\n",
      " 1203/10000: episode: 977, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.233869, accuracy: 0.812500, mean_q: 100.649384\n",
      " 1204/10000: episode: 978, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 11.745131, accuracy: 0.750000, mean_q: 99.863510\n",
      " 1205/10000: episode: 979, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 13.165274, accuracy: 0.625000, mean_q: 99.624535\n",
      " 1206/10000: episode: 980, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 5.706452, accuracy: 0.750000, mean_q: 100.007263\n",
      " 1211/10000: episode: 981, duration: 0.053s, episode steps:   5, steps per second:  95, episode reward: 60.000, mean reward: 12.000 [-10.000, 100.000], mean action: 5.000 [4.000, 9.000],  loss: 3.052100, accuracy: 0.775000, mean_q: 100.346878\n",
      " 1212/10000: episode: 982, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 6.970724, accuracy: 0.750000, mean_q: 100.038216\n",
      " 1214/10000: episode: 983, duration: 0.027s, episode steps:   2, steps per second:  73, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: 3.278289, accuracy: 0.703125, mean_q: 99.600174\n",
      " 1215/10000: episode: 984, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.537582, accuracy: 0.625000, mean_q: 99.268387\n",
      " 1216/10000: episode: 985, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 165.359848, accuracy: 0.875000, mean_q: 99.683479\n",
      " 1217/10000: episode: 986, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.973046, accuracy: 0.656250, mean_q: 98.483597\n",
      " 1218/10000: episode: 987, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 4.354448, accuracy: 0.750000, mean_q: 99.079010\n",
      " 1219/10000: episode: 988, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.437659, accuracy: 0.750000, mean_q: 99.338409\n",
      " 1220/10000: episode: 989, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 131.636566, accuracy: 0.687500, mean_q: 100.148155\n",
      " 1221/10000: episode: 990, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 7.527333, accuracy: 0.593750, mean_q: 99.419235\n",
      " 1222/10000: episode: 991, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 7.065768, accuracy: 0.812500, mean_q: 99.620758\n",
      " 1223/10000: episode: 992, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 9.090266, accuracy: 0.625000, mean_q: 99.700745\n",
      " 1224/10000: episode: 993, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.873883, accuracy: 0.718750, mean_q: 100.756271\n",
      " 1225/10000: episode: 994, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 14.272083, accuracy: 0.750000, mean_q: 101.152252\n",
      " 1227/10000: episode: 995, duration: 0.018s, episode steps:   2, steps per second: 113, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: 5.675494, accuracy: 0.781250, mean_q: 101.154877\n",
      " 1228/10000: episode: 996, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 8.498455, accuracy: 0.718750, mean_q: 100.402245\n",
      " 1231/10000: episode: 997, duration: 0.027s, episode steps:   3, steps per second: 110, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 3.667 [0.000, 10.000],  loss: 5.138975, accuracy: 0.750000, mean_q: 100.468506\n",
      " 1232/10000: episode: 998, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.318594, accuracy: 0.906250, mean_q: 100.314651\n",
      " 1234/10000: episode: 999, duration: 0.023s, episode steps:   2, steps per second:  86, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [1.000, 9.000],  loss: 1.908662, accuracy: 0.765625, mean_q: 100.460983\n",
      " 1235/10000: episode: 1000, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.594729, accuracy: 0.843750, mean_q: 100.127716\n",
      " 1236/10000: episode: 1001, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.175325, accuracy: 0.812500, mean_q: 100.047791\n",
      " 1237/10000: episode: 1002, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.592708, accuracy: 0.812500, mean_q: 100.265747\n",
      " 1238/10000: episode: 1003, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 9.697438, accuracy: 0.781250, mean_q: 100.322647\n",
      " 1239/10000: episode: 1004, duration: 0.036s, episode steps:   1, steps per second:  28, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.121844, accuracy: 0.812500, mean_q: 100.443596\n",
      " 1241/10000: episode: 1005, duration: 0.045s, episode steps:   2, steps per second:  44, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [4.000, 9.000],  loss: 3.202887, accuracy: 0.734375, mean_q: 100.096283\n",
      " 1242/10000: episode: 1006, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.970446, accuracy: 0.781250, mean_q: 99.773743\n",
      " 1243/10000: episode: 1007, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.981746, accuracy: 0.750000, mean_q: 99.562164\n",
      " 1244/10000: episode: 1008, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.382161, accuracy: 0.843750, mean_q: 100.040695\n",
      " 1245/10000: episode: 1009, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 4.411546, accuracy: 0.718750, mean_q: 100.497887\n",
      " 1246/10000: episode: 1010, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.968935, accuracy: 0.781250, mean_q: 100.530510\n",
      " 1247/10000: episode: 1011, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.679465, accuracy: 0.781250, mean_q: 100.329712\n",
      " 1248/10000: episode: 1012, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.290420, accuracy: 0.812500, mean_q: 100.048317\n",
      " 1249/10000: episode: 1013, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.747865, accuracy: 0.812500, mean_q: 99.806396\n",
      " 1251/10000: episode: 1014, duration: 0.019s, episode steps:   2, steps per second: 107, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 0.500 [0.000, 1.000],  loss: 3.064386, accuracy: 0.671875, mean_q: 100.024139\n",
      " 1252/10000: episode: 1015, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.386578, accuracy: 0.718750, mean_q: 100.462105\n",
      " 1253/10000: episode: 1016, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.936591, accuracy: 0.812500, mean_q: 100.353775\n",
      " 1254/10000: episode: 1017, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.539538, accuracy: 0.687500, mean_q: 100.253983\n",
      " 1255/10000: episode: 1018, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 10.974189, accuracy: 0.687500, mean_q: 100.126129\n",
      " 1256/10000: episode: 1019, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.993726, accuracy: 0.718750, mean_q: 100.221542\n",
      " 1257/10000: episode: 1020, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.830109, accuracy: 0.875000, mean_q: 100.425461\n",
      " 1258/10000: episode: 1021, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 6.982956, accuracy: 0.687500, mean_q: 100.044235\n",
      " 1259/10000: episode: 1022, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.235282, accuracy: 0.843750, mean_q: 100.038620\n",
      " 1260/10000: episode: 1023, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.206777, accuracy: 0.875000, mean_q: 99.607025\n",
      " 1261/10000: episode: 1024, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.547507, accuracy: 0.906250, mean_q: 99.642685\n",
      " 1262/10000: episode: 1025, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.817533, accuracy: 0.781250, mean_q: 100.103600\n",
      " 1263/10000: episode: 1026, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.544132, accuracy: 0.718750, mean_q: 100.134422\n",
      " 1264/10000: episode: 1027, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 163.861572, accuracy: 0.781250, mean_q: 100.195755\n",
      " 1265/10000: episode: 1028, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 297.517944, accuracy: 0.625000, mean_q: 99.390549\n",
      " 1266/10000: episode: 1029, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.512788, accuracy: 0.843750, mean_q: 98.444458\n",
      " 1267/10000: episode: 1030, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 5.215423, accuracy: 0.906250, mean_q: 98.239281\n",
      " 1268/10000: episode: 1031, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 5.180731, accuracy: 0.781250, mean_q: 98.876236\n",
      " 1270/10000: episode: 1032, duration: 0.030s, episode steps:   2, steps per second:  68, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [1.000, 7.000],  loss: 78.670082, accuracy: 0.828125, mean_q: 100.454552\n",
      " 1271/10000: episode: 1033, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 160.368622, accuracy: 0.812500, mean_q: 100.500893\n",
      " 1280/10000: episode: 1034, duration: 0.103s, episode steps:   9, steps per second:  87, episode reward: 20.000, mean reward:  2.222 [-10.000, 100.000], mean action: 9.444 [5.000, 10.000],  loss: 21.340935, accuracy: 0.739583, mean_q: 100.137268\n",
      " 1281/10000: episode: 1035, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.145514, accuracy: 0.875000, mean_q: 99.555977\n",
      " 1282/10000: episode: 1036, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.410359, accuracy: 0.781250, mean_q: 99.862450\n",
      " 1283/10000: episode: 1037, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 5.359734, accuracy: 0.812500, mean_q: 100.369240\n",
      " 1284/10000: episode: 1038, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 4.410308, accuracy: 0.843750, mean_q: 101.491249\n",
      " 1285/10000: episode: 1039, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 130.872772, accuracy: 0.937500, mean_q: 101.373978\n",
      " 1286/10000: episode: 1040, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.578105, accuracy: 0.843750, mean_q: 100.139954\n",
      " 1288/10000: episode: 1041, duration: 0.024s, episode steps:   2, steps per second:  85, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.000 [0.000, 2.000],  loss: 4.059440, accuracy: 0.843750, mean_q: 99.086792\n",
      " 1289/10000: episode: 1042, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 11.888172, accuracy: 0.750000, mean_q: 99.912704\n",
      " 1290/10000: episode: 1043, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.906725, accuracy: 0.750000, mean_q: 100.811806\n",
      " 1291/10000: episode: 1044, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 7.135871, accuracy: 0.781250, mean_q: 101.336647\n",
      " 1292/10000: episode: 1045, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.955660, accuracy: 0.750000, mean_q: 101.136429\n",
      " 1293/10000: episode: 1046, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.528113, accuracy: 0.906250, mean_q: 99.350204\n",
      " 1294/10000: episode: 1047, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 8.983772, accuracy: 0.875000, mean_q: 99.091858\n",
      " 1295/10000: episode: 1048, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.783203, accuracy: 0.875000, mean_q: 98.766090\n",
      " 1296/10000: episode: 1049, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 132.330185, accuracy: 0.906250, mean_q: 99.398346\n",
      " 1297/10000: episode: 1050, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 6.832840, accuracy: 0.750000, mean_q: 100.418907\n",
      " 1298/10000: episode: 1051, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 5.040455, accuracy: 0.781250, mean_q: 100.237946\n",
      " 1299/10000: episode: 1052, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 9.323837, accuracy: 0.687500, mean_q: 100.559288\n",
      " 1300/10000: episode: 1053, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 7.136581, accuracy: 0.843750, mean_q: 100.117035\n",
      " 1301/10000: episode: 1054, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.693859, accuracy: 0.812500, mean_q: 100.660904\n",
      " 1302/10000: episode: 1055, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 4.709921, accuracy: 0.718750, mean_q: 100.221107\n",
      " 1303/10000: episode: 1056, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.070152, accuracy: 0.750000, mean_q: 100.236176\n",
      " 1304/10000: episode: 1057, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.949083, accuracy: 0.718750, mean_q: 100.927841\n",
      " 1305/10000: episode: 1058, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.840644, accuracy: 0.656250, mean_q: 100.329742\n",
      " 1306/10000: episode: 1059, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.990742, accuracy: 0.875000, mean_q: 100.129898\n",
      " 1307/10000: episode: 1060, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.389317, accuracy: 0.781250, mean_q: 100.279083\n",
      " 1308/10000: episode: 1061, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.725326, accuracy: 0.875000, mean_q: 100.220230\n",
      " 1309/10000: episode: 1062, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.634099, accuracy: 0.843750, mean_q: 99.801186\n",
      " 1310/10000: episode: 1063, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.528848, accuracy: 0.687500, mean_q: 99.962448\n",
      " 1311/10000: episode: 1064, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 152.935318, accuracy: 0.656250, mean_q: 100.062180\n",
      " 1312/10000: episode: 1065, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.626978, accuracy: 0.718750, mean_q: 99.958817\n",
      " 1313/10000: episode: 1066, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.600193, accuracy: 0.812500, mean_q: 99.597183\n",
      " 1314/10000: episode: 1067, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.034430, accuracy: 0.812500, mean_q: 99.943161\n",
      " 1315/10000: episode: 1068, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.372244, accuracy: 0.750000, mean_q: 100.812317\n",
      " 1316/10000: episode: 1069, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.481018, accuracy: 0.781250, mean_q: 100.778465\n",
      " 1317/10000: episode: 1070, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.944972, accuracy: 0.750000, mean_q: 100.758118\n",
      " 1318/10000: episode: 1071, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 163.352814, accuracy: 0.656250, mean_q: 100.381882\n",
      " 1319/10000: episode: 1072, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 134.778824, accuracy: 0.781250, mean_q: 99.521835\n",
      " 1320/10000: episode: 1073, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.997650, accuracy: 0.718750, mean_q: 98.277534\n",
      " 1321/10000: episode: 1074, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.563907, accuracy: 0.812500, mean_q: 98.532700\n",
      " 1322/10000: episode: 1075, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.691472, accuracy: 0.875000, mean_q: 99.463219\n",
      " 1323/10000: episode: 1076, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.818175, accuracy: 0.843750, mean_q: 100.019524\n",
      " 1324/10000: episode: 1077, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 128.444656, accuracy: 0.750000, mean_q: 100.629280\n",
      " 1325/10000: episode: 1078, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 4.618994, accuracy: 0.656250, mean_q: 100.649315\n",
      " 1326/10000: episode: 1079, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.264187, accuracy: 0.812500, mean_q: 100.315880\n",
      " 1327/10000: episode: 1080, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.639562, accuracy: 0.781250, mean_q: 100.228249\n",
      " 1328/10000: episode: 1081, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 7.483507, accuracy: 0.656250, mean_q: 99.611809\n",
      " 1329/10000: episode: 1082, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.563842, accuracy: 0.812500, mean_q: 100.008080\n",
      " 1331/10000: episode: 1083, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 1.512635, accuracy: 0.765625, mean_q: 99.946350\n",
      " 1332/10000: episode: 1084, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.408119, accuracy: 0.875000, mean_q: 99.987061\n",
      " 1333/10000: episode: 1085, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 5.310493, accuracy: 0.781250, mean_q: 100.341965\n",
      " 1334/10000: episode: 1086, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 4.950247, accuracy: 0.750000, mean_q: 99.860649\n",
      " 1335/10000: episode: 1087, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 142.531525, accuracy: 0.781250, mean_q: 100.245781\n",
      " 1336/10000: episode: 1088, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 7.196063, accuracy: 0.750000, mean_q: 100.002533\n",
      " 1338/10000: episode: 1089, duration: 0.018s, episode steps:   2, steps per second: 113, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.000 [7.000, 11.000],  loss: 3.441084, accuracy: 0.781250, mean_q: 99.786430\n",
      " 1339/10000: episode: 1090, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.504795, accuracy: 0.812500, mean_q: 100.201637\n",
      " 1340/10000: episode: 1091, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 4.306558, accuracy: 0.562500, mean_q: 100.279724\n",
      " 1341/10000: episode: 1092, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.017323, accuracy: 0.750000, mean_q: 100.629929\n",
      " 1342/10000: episode: 1093, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.493634, accuracy: 0.843750, mean_q: 100.658844\n",
      " 1344/10000: episode: 1094, duration: 0.022s, episode steps:   2, steps per second:  90, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [7.000, 10.000],  loss: 1.645774, accuracy: 0.687500, mean_q: 100.211517\n",
      " 1345/10000: episode: 1095, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.174762, accuracy: 0.750000, mean_q: 100.133087\n",
      " 1355/10000: episode: 1096, duration: 0.104s, episode steps:  10, steps per second:  96, episode reward: 10.000, mean reward:  1.000 [-10.000, 100.000], mean action: 8.100 [0.000, 9.000],  loss: 31.535147, accuracy: 0.690625, mean_q: 100.000214\n",
      " 1356/10000: episode: 1097, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 7.198954, accuracy: 0.812500, mean_q: 98.724274\n",
      " 1357/10000: episode: 1098, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 3.362946, accuracy: 0.781250, mean_q: 101.089966\n",
      " 1358/10000: episode: 1099, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.302107, accuracy: 0.906250, mean_q: 100.047867\n",
      " 1359/10000: episode: 1100, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.034014, accuracy: 0.875000, mean_q: 100.409622\n",
      " 1360/10000: episode: 1101, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 5.087672, accuracy: 0.718750, mean_q: 99.716805\n",
      " 1361/10000: episode: 1102, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 5.626141, accuracy: 0.843750, mean_q: 99.019028\n",
      " 1362/10000: episode: 1103, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.570176, accuracy: 0.750000, mean_q: 101.472572\n",
      " 1363/10000: episode: 1104, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.059655, accuracy: 0.781250, mean_q: 100.751701\n",
      " 1364/10000: episode: 1105, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.265532, accuracy: 0.625000, mean_q: 99.819580\n",
      " 1365/10000: episode: 1106, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.293368, accuracy: 0.843750, mean_q: 99.293167\n",
      " 1367/10000: episode: 1107, duration: 0.029s, episode steps:   2, steps per second:  68, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [0.000, 11.000],  loss: 2.495379, accuracy: 0.796875, mean_q: 99.811874\n",
      " 1368/10000: episode: 1108, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 156.680862, accuracy: 0.781250, mean_q: 100.025002\n",
      " 1369/10000: episode: 1109, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.455850, accuracy: 0.906250, mean_q: 99.775322\n",
      " 1370/10000: episode: 1110, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.158487, accuracy: 0.843750, mean_q: 99.422256\n",
      " 1371/10000: episode: 1111, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 285.929840, accuracy: 0.843750, mean_q: 100.389359\n",
      " 1372/10000: episode: 1112, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 8.735232, accuracy: 0.781250, mean_q: 98.535858\n",
      " 1373/10000: episode: 1113, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 120.854691, accuracy: 0.687500, mean_q: 98.054810\n",
      " 1374/10000: episode: 1114, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 12.048849, accuracy: 0.812500, mean_q: 98.149979\n",
      " 1375/10000: episode: 1115, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 9.704165, accuracy: 0.812500, mean_q: 99.095169\n",
      " 1376/10000: episode: 1116, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.498064, accuracy: 0.843750, mean_q: 101.051033\n",
      " 1377/10000: episode: 1117, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.269350, accuracy: 0.812500, mean_q: 101.268188\n",
      " 1378/10000: episode: 1118, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.306546, accuracy: 0.843750, mean_q: 100.347870\n",
      " 1379/10000: episode: 1119, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 9.617268, accuracy: 0.687500, mean_q: 99.278419\n",
      " 1380/10000: episode: 1120, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.769851, accuracy: 0.875000, mean_q: 99.417122\n",
      " 1382/10000: episode: 1121, duration: 0.019s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: 3.396353, accuracy: 0.796875, mean_q: 100.314407\n",
      " 1383/10000: episode: 1122, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 8.528251, accuracy: 0.687500, mean_q: 101.204971\n",
      " 1384/10000: episode: 1123, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 4.717865, accuracy: 0.718750, mean_q: 101.161530\n",
      " 1386/10000: episode: 1124, duration: 0.022s, episode steps:   2, steps per second:  92, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [1.000, 3.000],  loss: 3.438206, accuracy: 0.875000, mean_q: 100.688744\n",
      " 1388/10000: episode: 1125, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [0.000, 8.000],  loss: 1.021707, accuracy: 0.828125, mean_q: 100.039932\n",
      " 1390/10000: episode: 1126, duration: 0.019s, episode steps:   2, steps per second: 107, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [8.000, 9.000],  loss: 1.310691, accuracy: 0.828125, mean_q: 99.792557\n",
      " 1391/10000: episode: 1127, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.431267, accuracy: 0.906250, mean_q: 99.776077\n",
      " 1392/10000: episode: 1128, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.319534, accuracy: 0.812500, mean_q: 100.143051\n",
      " 1393/10000: episode: 1129, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 158.483749, accuracy: 0.718750, mean_q: 100.280151\n",
      " 1394/10000: episode: 1130, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.642651, accuracy: 0.750000, mean_q: 99.952621\n",
      " 1395/10000: episode: 1131, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.087677, accuracy: 0.781250, mean_q: 99.554352\n",
      " 1396/10000: episode: 1132, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.064727, accuracy: 0.843750, mean_q: 99.943199\n",
      " 1397/10000: episode: 1133, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.547817, accuracy: 0.875000, mean_q: 100.303833\n",
      " 1398/10000: episode: 1134, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.628472, accuracy: 0.718750, mean_q: 100.340363\n",
      " 1399/10000: episode: 1135, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.951376, accuracy: 0.843750, mean_q: 100.039467\n",
      " 1400/10000: episode: 1136, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.527825, accuracy: 0.781250, mean_q: 99.293961\n",
      " 1401/10000: episode: 1137, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.428833, accuracy: 0.750000, mean_q: 99.709793\n",
      " 1402/10000: episode: 1138, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.457642, accuracy: 0.750000, mean_q: 100.082443\n",
      " 1404/10000: episode: 1139, duration: 0.021s, episode steps:   2, steps per second:  96, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: 74.334305, accuracy: 0.765625, mean_q: 100.583778\n",
      " 1405/10000: episode: 1140, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.929163, accuracy: 0.812500, mean_q: 100.367340\n",
      " 1406/10000: episode: 1141, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.712519, accuracy: 0.718750, mean_q: 99.215530\n",
      " 1407/10000: episode: 1142, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.353297, accuracy: 0.906250, mean_q: 99.166779\n",
      " 1408/10000: episode: 1143, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 5.425014, accuracy: 0.718750, mean_q: 99.100929\n",
      " 1409/10000: episode: 1144, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.562025, accuracy: 0.906250, mean_q: 99.584198\n",
      " 1410/10000: episode: 1145, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.202775, accuracy: 0.843750, mean_q: 100.182419\n",
      " 1411/10000: episode: 1146, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.693135, accuracy: 0.875000, mean_q: 100.517365\n",
      " 1412/10000: episode: 1147, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 155.802673, accuracy: 0.812500, mean_q: 100.187775\n",
      " 1413/10000: episode: 1148, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.943079, accuracy: 0.875000, mean_q: 99.657776\n",
      " 1415/10000: episode: 1149, duration: 0.017s, episode steps:   2, steps per second: 116, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [2.000, 4.000],  loss: 2.935122, accuracy: 0.765625, mean_q: 99.250717\n",
      " 1416/10000: episode: 1150, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 8.952284, accuracy: 0.781250, mean_q: 99.742691\n",
      " 1417/10000: episode: 1151, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.462219, accuracy: 0.875000, mean_q: 101.011353\n",
      " 1418/10000: episode: 1152, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.591347, accuracy: 0.750000, mean_q: 100.679337\n",
      " 1420/10000: episode: 1153, duration: 0.028s, episode steps:   2, steps per second:  71, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [3.000, 8.000],  loss: 2.894494, accuracy: 0.765625, mean_q: 100.382370\n",
      " 1421/10000: episode: 1154, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 146.674149, accuracy: 0.718750, mean_q: 100.568138\n",
      " 1422/10000: episode: 1155, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.048625, accuracy: 0.750000, mean_q: 99.422714\n",
      " 1423/10000: episode: 1156, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.749913, accuracy: 0.875000, mean_q: 98.710976\n",
      " 1424/10000: episode: 1157, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.154298, accuracy: 0.906250, mean_q: 99.792633\n",
      " 1426/10000: episode: 1158, duration: 0.028s, episode steps:   2, steps per second:  71, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [7.000, 8.000],  loss: 3.708156, accuracy: 0.890625, mean_q: 100.007233\n",
      " 1427/10000: episode: 1159, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.891174, accuracy: 0.812500, mean_q: 99.852684\n",
      " 1428/10000: episode: 1160, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.211539, accuracy: 0.781250, mean_q: 99.996887\n",
      " 1429/10000: episode: 1161, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.056103, accuracy: 0.750000, mean_q: 100.464363\n",
      " 1430/10000: episode: 1162, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.449893, accuracy: 0.812500, mean_q: 100.498665\n",
      " 1431/10000: episode: 1163, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 4.076045, accuracy: 0.812500, mean_q: 100.421394\n",
      " 1432/10000: episode: 1164, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.777504, accuracy: 0.843750, mean_q: 99.907532\n",
      " 1433/10000: episode: 1165, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.666499, accuracy: 0.812500, mean_q: 99.666840\n",
      " 1434/10000: episode: 1166, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.106141, accuracy: 0.750000, mean_q: 99.574768\n",
      " 1435/10000: episode: 1167, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.400271, accuracy: 0.812500, mean_q: 99.780075\n",
      " 1436/10000: episode: 1168, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.441940, accuracy: 0.656250, mean_q: 100.321808\n",
      " 1437/10000: episode: 1169, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 151.704681, accuracy: 0.875000, mean_q: 100.826309\n",
      " 1440/10000: episode: 1170, duration: 0.036s, episode steps:   3, steps per second:  84, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 3.667 [1.000, 6.000],  loss: 1.945222, accuracy: 0.822917, mean_q: 99.979584\n",
      " 1442/10000: episode: 1171, duration: 0.031s, episode steps:   2, steps per second:  65, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.000 [0.000, 2.000],  loss: 6.690924, accuracy: 0.812500, mean_q: 99.697273\n",
      " 1443/10000: episode: 1172, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.852797, accuracy: 0.781250, mean_q: 99.894646\n",
      " 1444/10000: episode: 1173, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.938577, accuracy: 0.906250, mean_q: 100.270203\n",
      " 1445/10000: episode: 1174, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.718032, accuracy: 0.906250, mean_q: 100.639145\n",
      " 1446/10000: episode: 1175, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.671254, accuracy: 0.906250, mean_q: 100.627609\n",
      " 1447/10000: episode: 1176, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.710557, accuracy: 0.843750, mean_q: 100.048409\n",
      " 1448/10000: episode: 1177, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 148.225845, accuracy: 0.750000, mean_q: 99.820457\n",
      " 1449/10000: episode: 1178, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.612204, accuracy: 0.781250, mean_q: 99.421120\n",
      " 1450/10000: episode: 1179, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.553639, accuracy: 0.875000, mean_q: 99.706985\n",
      " 1451/10000: episode: 1180, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.787148, accuracy: 0.906250, mean_q: 99.979477\n",
      " 1452/10000: episode: 1181, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.939292, accuracy: 0.968750, mean_q: 100.844101\n",
      " 1453/10000: episode: 1182, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.162500, accuracy: 0.781250, mean_q: 100.355743\n",
      " 1454/10000: episode: 1183, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 134.985947, accuracy: 0.812500, mean_q: 100.112595\n",
      " 1455/10000: episode: 1184, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 146.659607, accuracy: 0.718750, mean_q: 99.692413\n",
      " 1457/10000: episode: 1185, duration: 0.024s, episode steps:   2, steps per second:  85, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [3.000, 8.000],  loss: 4.002642, accuracy: 0.796875, mean_q: 98.839462\n",
      " 1458/10000: episode: 1186, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.344079, accuracy: 0.812500, mean_q: 99.250000\n",
      " 1460/10000: episode: 1187, duration: 0.022s, episode steps:   2, steps per second:  90, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [0.000, 8.000],  loss: 69.229225, accuracy: 0.781250, mean_q: 100.666626\n",
      " 1461/10000: episode: 1188, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.271448, accuracy: 0.687500, mean_q: 100.413345\n",
      " 1462/10000: episode: 1189, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 158.428604, accuracy: 0.687500, mean_q: 100.112038\n",
      " 1463/10000: episode: 1190, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.568088, accuracy: 0.843750, mean_q: 99.146446\n",
      " 1464/10000: episode: 1191, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.639647, accuracy: 0.781250, mean_q: 99.276352\n",
      " 1465/10000: episode: 1192, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 8.364510, accuracy: 0.812500, mean_q: 99.784897\n",
      " 1466/10000: episode: 1193, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.355513, accuracy: 0.843750, mean_q: 100.800568\n",
      " 1467/10000: episode: 1194, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.418174, accuracy: 0.781250, mean_q: 100.904427\n",
      " 1469/10000: episode: 1195, duration: 0.023s, episode steps:   2, steps per second:  87, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [4.000, 9.000],  loss: 6.005098, accuracy: 0.812500, mean_q: 100.279053\n",
      " 1470/10000: episode: 1196, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.685208, accuracy: 0.812500, mean_q: 100.636841\n",
      " 1471/10000: episode: 1197, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.006597, accuracy: 0.937500, mean_q: 100.403252\n",
      " 1472/10000: episode: 1198, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 7.536613, accuracy: 0.812500, mean_q: 99.836250\n",
      " 1473/10000: episode: 1199, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.980429, accuracy: 0.812500, mean_q: 99.468147\n",
      " 1474/10000: episode: 1200, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 6.556758, accuracy: 0.718750, mean_q: 99.469254\n",
      " 1475/10000: episode: 1201, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 9.663589, accuracy: 0.687500, mean_q: 99.622871\n",
      " 1479/10000: episode: 1202, duration: 0.035s, episode steps:   4, steps per second: 114, episode reward: 70.000, mean reward: 17.500 [-10.000, 100.000], mean action: 7.250 [5.000, 11.000],  loss: 4.504064, accuracy: 0.742188, mean_q: 100.560402\n",
      " 1480/10000: episode: 1203, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.155366, accuracy: 0.906250, mean_q: 100.829910\n",
      " 1481/10000: episode: 1204, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 7.451900, accuracy: 0.687500, mean_q: 100.444824\n",
      " 1482/10000: episode: 1205, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 8.881124, accuracy: 0.750000, mean_q: 99.915680\n",
      " 1483/10000: episode: 1206, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.405778, accuracy: 0.750000, mean_q: 100.242569\n",
      " 1484/10000: episode: 1207, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 4.281357, accuracy: 0.687500, mean_q: 100.267151\n",
      " 1485/10000: episode: 1208, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.426577, accuracy: 0.750000, mean_q: 99.990936\n",
      " 1486/10000: episode: 1209, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.869679, accuracy: 0.656250, mean_q: 100.541489\n",
      " 1487/10000: episode: 1210, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.635184, accuracy: 0.781250, mean_q: 100.542953\n",
      " 1488/10000: episode: 1211, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 6.975240, accuracy: 0.562500, mean_q: 100.754044\n",
      " 1489/10000: episode: 1212, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.322717, accuracy: 0.812500, mean_q: 100.821808\n",
      " 1490/10000: episode: 1213, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.089768, accuracy: 0.875000, mean_q: 100.236603\n",
      " 1491/10000: episode: 1214, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 115.778763, accuracy: 0.750000, mean_q: 99.845459\n",
      " 1492/10000: episode: 1215, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 5.697884, accuracy: 0.875000, mean_q: 98.729187\n",
      " 1493/10000: episode: 1216, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 4.277975, accuracy: 0.750000, mean_q: 99.494072\n",
      " 1512/10000: episode: 1217, duration: 0.127s, episode steps:  19, steps per second: 150, episode reward: -80.000, mean reward: -4.211 [-10.000, 100.000], mean action: 1.947 [1.000, 11.000],  loss: 34.223053, accuracy: 0.766447, mean_q: 100.003998\n",
      " 1513/10000: episode: 1218, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.163028, accuracy: 0.875000, mean_q: 98.441452\n",
      " 1514/10000: episode: 1219, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 4.126131, accuracy: 0.781250, mean_q: 99.223419\n",
      " 1515/10000: episode: 1220, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.261476, accuracy: 0.812500, mean_q: 100.027199\n",
      " 1516/10000: episode: 1221, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.088462, accuracy: 0.750000, mean_q: 100.930176\n",
      " 1517/10000: episode: 1222, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.918820, accuracy: 0.875000, mean_q: 101.275063\n",
      " 1518/10000: episode: 1223, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.664550, accuracy: 0.718750, mean_q: 101.210533\n",
      " 1519/10000: episode: 1224, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 135.370621, accuracy: 0.781250, mean_q: 100.559265\n",
      " 1520/10000: episode: 1225, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 6.632593, accuracy: 0.781250, mean_q: 98.763992\n",
      " 1521/10000: episode: 1226, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 9.589082, accuracy: 0.687500, mean_q: 99.192223\n",
      " 1522/10000: episode: 1227, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 5.138190, accuracy: 0.843750, mean_q: 99.773712\n",
      " 1523/10000: episode: 1228, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 130.035004, accuracy: 0.843750, mean_q: 100.615509\n",
      " 1524/10000: episode: 1229, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 5.224215, accuracy: 0.875000, mean_q: 99.994637\n",
      " 1525/10000: episode: 1230, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.427371, accuracy: 0.781250, mean_q: 100.160782\n",
      " 1527/10000: episode: 1231, duration: 0.020s, episode steps:   2, steps per second: 100, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.500 [10.000, 11.000],  loss: 133.934113, accuracy: 0.781250, mean_q: 99.822227\n",
      " 1529/10000: episode: 1232, duration: 0.023s, episode steps:   2, steps per second:  85, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [1.000, 6.000],  loss: 5.557374, accuracy: 0.796875, mean_q: 98.645927\n",
      " 1530/10000: episode: 1233, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 144.132965, accuracy: 0.718750, mean_q: 98.512482\n",
      " 1531/10000: episode: 1234, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 9.748810, accuracy: 0.687500, mean_q: 99.007584\n",
      " 1532/10000: episode: 1235, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.769846, accuracy: 0.781250, mean_q: 99.845047\n",
      " 1533/10000: episode: 1236, duration: 0.024s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 6.985348, accuracy: 0.812500, mean_q: 100.816513\n",
      " 1534/10000: episode: 1237, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 6.615043, accuracy: 0.656250, mean_q: 101.444427\n",
      " 1535/10000: episode: 1238, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.028288, accuracy: 0.781250, mean_q: 100.724701\n",
      " 1536/10000: episode: 1239, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 5.163865, accuracy: 0.781250, mean_q: 100.869499\n",
      " 1537/10000: episode: 1240, duration: 0.022s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.399079, accuracy: 0.843750, mean_q: 100.222923\n",
      " 1538/10000: episode: 1241, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.862575, accuracy: 0.843750, mean_q: 100.267273\n",
      " 1539/10000: episode: 1242, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 11.562084, accuracy: 0.781250, mean_q: 100.565483\n",
      " 1540/10000: episode: 1243, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 5.953915, accuracy: 0.843750, mean_q: 100.360565\n",
      " 1541/10000: episode: 1244, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.684846, accuracy: 0.781250, mean_q: 100.683304\n",
      " 1542/10000: episode: 1245, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.851399, accuracy: 0.750000, mean_q: 100.539246\n",
      " 1543/10000: episode: 1246, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.682108, accuracy: 0.687500, mean_q: 100.293480\n",
      " 1544/10000: episode: 1247, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.241239, accuracy: 0.750000, mean_q: 99.616135\n",
      " 1545/10000: episode: 1248, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.365599, accuracy: 0.937500, mean_q: 99.833672\n",
      " 1546/10000: episode: 1249, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.771340, accuracy: 0.781250, mean_q: 100.317276\n",
      " 1547/10000: episode: 1250, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.377483, accuracy: 0.906250, mean_q: 99.997810\n",
      " 1549/10000: episode: 1251, duration: 0.021s, episode steps:   2, steps per second:  96, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [5.000, 7.000],  loss: 1.294301, accuracy: 0.890625, mean_q: 100.366196\n",
      " 1550/10000: episode: 1252, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 156.203918, accuracy: 0.843750, mean_q: 100.218185\n",
      " 1551/10000: episode: 1253, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.734459, accuracy: 0.875000, mean_q: 99.412788\n",
      " 1552/10000: episode: 1254, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.948831, accuracy: 0.781250, mean_q: 98.967552\n",
      " 1553/10000: episode: 1255, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.654434, accuracy: 0.843750, mean_q: 99.037003\n",
      " 1554/10000: episode: 1256, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.637759, accuracy: 0.906250, mean_q: 99.601120\n",
      " 1555/10000: episode: 1257, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.909094, accuracy: 0.718750, mean_q: 100.243050\n",
      " 1556/10000: episode: 1258, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 5.909965, accuracy: 0.687500, mean_q: 100.110336\n",
      " 1573/10000: episode: 1259, duration: 0.201s, episode steps:  17, steps per second:  85, episode reward: -60.000, mean reward: -3.529 [-10.000, 100.000], mean action: 5.412 [5.000, 9.000],  loss: 10.078569, accuracy: 0.726103, mean_q: 100.261490\n",
      " 1574/10000: episode: 1260, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.507419, accuracy: 0.875000, mean_q: 100.403854\n",
      " 1575/10000: episode: 1261, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 152.258789, accuracy: 0.812500, mean_q: 100.211113\n",
      " 1576/10000: episode: 1262, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.701198, accuracy: 0.812500, mean_q: 99.987335\n",
      " 1577/10000: episode: 1263, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.675821, accuracy: 0.781250, mean_q: 99.834679\n",
      " 1578/10000: episode: 1264, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.555646, accuracy: 0.781250, mean_q: 99.748558\n",
      " 1579/10000: episode: 1265, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.885331, accuracy: 0.750000, mean_q: 99.915024\n",
      " 1580/10000: episode: 1266, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.910545, accuracy: 0.843750, mean_q: 100.420753\n",
      " 1582/10000: episode: 1267, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: 68.334618, accuracy: 0.781250, mean_q: 100.150826\n",
      " 1584/10000: episode: 1268, duration: 0.030s, episode steps:   2, steps per second:  67, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [2.000, 4.000],  loss: 3.951173, accuracy: 0.750000, mean_q: 99.932823\n",
      " 1585/10000: episode: 1269, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 5.931532, accuracy: 0.750000, mean_q: 99.883781\n",
      " 1587/10000: episode: 1270, duration: 0.031s, episode steps:   2, steps per second:  65, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [0.000, 5.000],  loss: 4.108790, accuracy: 0.750000, mean_q: 100.429947\n",
      " 1588/10000: episode: 1271, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.827627, accuracy: 0.625000, mean_q: 100.552040\n",
      " 1590/10000: episode: 1272, duration: 0.025s, episode steps:   2, steps per second:  81, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [1.000, 9.000],  loss: 1.772041, accuracy: 0.765625, mean_q: 100.017250\n",
      " 1591/10000: episode: 1273, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 4.463052, accuracy: 0.750000, mean_q: 99.735641\n",
      " 1592/10000: episode: 1274, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.818205, accuracy: 0.843750, mean_q: 100.216209\n",
      " 1593/10000: episode: 1275, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.418411, accuracy: 0.843750, mean_q: 100.403000\n",
      " 1594/10000: episode: 1276, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.102136, accuracy: 0.812500, mean_q: 100.335823\n",
      " 1595/10000: episode: 1277, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.260249, accuracy: 0.812500, mean_q: 99.894821\n",
      " 1597/10000: episode: 1278, duration: 0.022s, episode steps:   2, steps per second:  90, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [9.000, 10.000],  loss: 1.550038, accuracy: 0.734375, mean_q: 100.133224\n",
      " 1598/10000: episode: 1279, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.525478, accuracy: 0.906250, mean_q: 99.998161\n",
      " 1599/10000: episode: 1280, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.138802, accuracy: 0.937500, mean_q: 99.853981\n",
      " 1600/10000: episode: 1281, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.765866, accuracy: 0.750000, mean_q: 99.851990\n",
      " 1601/10000: episode: 1282, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 149.883575, accuracy: 0.718750, mean_q: 99.792374\n",
      " 1602/10000: episode: 1283, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.498072, accuracy: 0.750000, mean_q: 99.397964\n",
      " 1606/10000: episode: 1284, duration: 0.036s, episode steps:   4, steps per second: 111, episode reward: 70.000, mean reward: 17.500 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: 1.349238, accuracy: 0.773438, mean_q: 99.552704\n",
      " 1607/10000: episode: 1285, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.740172, accuracy: 0.718750, mean_q: 100.433929\n",
      " 1608/10000: episode: 1286, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.449773, accuracy: 0.750000, mean_q: 100.668488\n",
      " 1609/10000: episode: 1287, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.049171, accuracy: 0.781250, mean_q: 100.385086\n",
      " 1610/10000: episode: 1288, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.216145, accuracy: 0.812500, mean_q: 100.040497\n",
      " 1612/10000: episode: 1289, duration: 0.018s, episode steps:   2, steps per second: 112, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [1.000, 7.000],  loss: 1.441908, accuracy: 0.890625, mean_q: 99.686295\n",
      " 1613/10000: episode: 1290, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.231071, accuracy: 0.718750, mean_q: 99.878990\n",
      " 1614/10000: episode: 1291, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.915127, accuracy: 0.843750, mean_q: 99.436852\n",
      " 1615/10000: episode: 1292, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.933921, accuracy: 0.687500, mean_q: 99.556358\n",
      " 1616/10000: episode: 1293, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.122239, accuracy: 0.906250, mean_q: 100.105774\n",
      " 1617/10000: episode: 1294, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.194256, accuracy: 0.812500, mean_q: 100.103035\n",
      " 1618/10000: episode: 1295, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.526215, accuracy: 0.718750, mean_q: 100.172630\n",
      " 1619/10000: episode: 1296, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.299510, accuracy: 0.750000, mean_q: 100.184509\n",
      " 1620/10000: episode: 1297, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.485963, accuracy: 0.750000, mean_q: 100.101280\n",
      " 1621/10000: episode: 1298, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.922733, accuracy: 0.843750, mean_q: 99.632240\n",
      " 1622/10000: episode: 1299, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.277277, accuracy: 0.906250, mean_q: 99.478378\n",
      " 1623/10000: episode: 1300, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.643081, accuracy: 0.781250, mean_q: 99.007500\n",
      " 1624/10000: episode: 1301, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.655282, accuracy: 0.968750, mean_q: 99.367889\n",
      " 1625/10000: episode: 1302, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.078353, accuracy: 0.875000, mean_q: 99.786453\n",
      " 1626/10000: episode: 1303, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.778432, accuracy: 0.875000, mean_q: 100.051559\n",
      " 1627/10000: episode: 1304, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.776909, accuracy: 0.875000, mean_q: 100.164642\n",
      " 1628/10000: episode: 1305, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.223630, accuracy: 0.656250, mean_q: 100.114204\n",
      " 1629/10000: episode: 1306, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.170775, accuracy: 0.781250, mean_q: 100.277016\n",
      " 1630/10000: episode: 1307, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.316388, accuracy: 0.781250, mean_q: 100.210976\n",
      " 1631/10000: episode: 1308, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.424099, accuracy: 0.687500, mean_q: 100.430038\n",
      " 1632/10000: episode: 1309, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.418756, accuracy: 0.781250, mean_q: 100.470062\n",
      " 1636/10000: episode: 1310, duration: 0.036s, episode steps:   4, steps per second: 112, episode reward: 70.000, mean reward: 17.500 [-10.000, 100.000], mean action: 6.000 [2.000, 10.000],  loss: 1.743658, accuracy: 0.812500, mean_q: 100.112106\n",
      " 1637/10000: episode: 1311, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.734398, accuracy: 0.875000, mean_q: 100.230591\n",
      " 1638/10000: episode: 1312, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.241931, accuracy: 0.906250, mean_q: 100.187225\n",
      " 1639/10000: episode: 1313, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 156.064789, accuracy: 0.812500, mean_q: 100.162186\n",
      " 1640/10000: episode: 1314, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.427973, accuracy: 0.843750, mean_q: 98.732574\n",
      " 1642/10000: episode: 1315, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [7.000, 10.000],  loss: 2.107326, accuracy: 0.781250, mean_q: 99.239288\n",
      " 1643/10000: episode: 1316, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 144.302704, accuracy: 0.843750, mean_q: 99.401665\n",
      " 1644/10000: episode: 1317, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.907872, accuracy: 0.906250, mean_q: 99.873672\n",
      " 1645/10000: episode: 1318, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.421902, accuracy: 0.781250, mean_q: 100.227768\n",
      " 1647/10000: episode: 1319, duration: 0.018s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.500 [10.000, 11.000],  loss: 66.261734, accuracy: 0.765625, mean_q: 99.716888\n",
      " 1648/10000: episode: 1320, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.993647, accuracy: 0.781250, mean_q: 99.427902\n",
      " 1649/10000: episode: 1321, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 133.900711, accuracy: 0.656250, mean_q: 99.818840\n",
      " 1650/10000: episode: 1322, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.680589, accuracy: 0.812500, mean_q: 99.730827\n",
      " 1651/10000: episode: 1323, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.738354, accuracy: 0.718750, mean_q: 99.841805\n",
      " 1652/10000: episode: 1324, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.643923, accuracy: 0.812500, mean_q: 100.183228\n",
      " 1654/10000: episode: 1325, duration: 0.022s, episode steps:   2, steps per second:  91, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [3.000, 11.000],  loss: 3.694956, accuracy: 0.812500, mean_q: 100.433243\n",
      " 1655/10000: episode: 1326, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.006951, accuracy: 0.718750, mean_q: 99.810272\n",
      " 1656/10000: episode: 1327, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.878346, accuracy: 0.843750, mean_q: 100.086929\n",
      " 1657/10000: episode: 1328, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.464766, accuracy: 0.875000, mean_q: 99.797852\n",
      " 1658/10000: episode: 1329, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 7.631307, accuracy: 0.781250, mean_q: 99.658569\n",
      " 1659/10000: episode: 1330, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.945896, accuracy: 0.781250, mean_q: 99.562302\n",
      " 1660/10000: episode: 1331, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 5.541368, accuracy: 0.687500, mean_q: 100.304749\n",
      " 1661/10000: episode: 1332, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.143779, accuracy: 0.843750, mean_q: 100.352341\n",
      " 1662/10000: episode: 1333, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.901196, accuracy: 0.843750, mean_q: 100.108017\n",
      " 1663/10000: episode: 1334, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.348805, accuracy: 0.812500, mean_q: 100.592392\n",
      " 1664/10000: episode: 1335, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.172310, accuracy: 0.812500, mean_q: 100.333504\n",
      " 1665/10000: episode: 1336, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.166029, accuracy: 0.750000, mean_q: 100.639023\n",
      " 1666/10000: episode: 1337, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.198057, accuracy: 0.812500, mean_q: 100.367821\n",
      " 1667/10000: episode: 1338, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.434140, accuracy: 0.750000, mean_q: 100.454308\n",
      " 1668/10000: episode: 1339, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.690203, accuracy: 0.718750, mean_q: 99.459175\n",
      " 1669/10000: episode: 1340, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.011818, accuracy: 0.906250, mean_q: 99.457893\n",
      " 1670/10000: episode: 1341, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 142.320358, accuracy: 0.687500, mean_q: 99.869156\n",
      " 1671/10000: episode: 1342, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.093974, accuracy: 0.843750, mean_q: 99.719879\n",
      " 1672/10000: episode: 1343, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.113732, accuracy: 0.718750, mean_q: 99.961952\n",
      " 1673/10000: episode: 1344, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.628796, accuracy: 0.875000, mean_q: 100.168915\n",
      " 1674/10000: episode: 1345, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.109860, accuracy: 0.593750, mean_q: 100.132599\n",
      " 1675/10000: episode: 1346, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 3.346032, accuracy: 0.750000, mean_q: 100.250565\n",
      " 1676/10000: episode: 1347, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.425278, accuracy: 0.781250, mean_q: 100.106964\n",
      " 1677/10000: episode: 1348, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.131379, accuracy: 0.875000, mean_q: 99.806839\n",
      " 1678/10000: episode: 1349, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.789400, accuracy: 0.718750, mean_q: 99.722702\n",
      " 1679/10000: episode: 1350, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.192512, accuracy: 0.781250, mean_q: 100.093613\n",
      " 1680/10000: episode: 1351, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.371631, accuracy: 0.875000, mean_q: 100.401596\n",
      " 1681/10000: episode: 1352, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.101969, accuracy: 0.843750, mean_q: 100.111740\n",
      " 1682/10000: episode: 1353, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.029649, accuracy: 0.750000, mean_q: 99.435989\n",
      " 1684/10000: episode: 1354, duration: 0.027s, episode steps:   2, steps per second:  73, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [5.000, 9.000],  loss: 1.941912, accuracy: 0.796875, mean_q: 99.612564\n",
      " 1685/10000: episode: 1355, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.737273, accuracy: 0.812500, mean_q: 99.921890\n",
      " 1686/10000: episode: 1356, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.514324, accuracy: 0.687500, mean_q: 100.448341\n",
      " 1687/10000: episode: 1357, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.917723, accuracy: 0.750000, mean_q: 100.718796\n",
      " 1688/10000: episode: 1358, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.204697, accuracy: 0.687500, mean_q: 100.563812\n",
      " 1689/10000: episode: 1359, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 4.240943, accuracy: 0.750000, mean_q: 100.704460\n",
      " 1691/10000: episode: 1360, duration: 0.033s, episode steps:   2, steps per second:  61, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 0.500 [0.000, 1.000],  loss: 4.620230, accuracy: 0.843750, mean_q: 100.288765\n",
      " 1692/10000: episode: 1361, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.635686, accuracy: 0.843750, mean_q: 100.141319\n",
      " 1693/10000: episode: 1362, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.874818, accuracy: 0.781250, mean_q: 100.377235\n",
      " 1694/10000: episode: 1363, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.395483, accuracy: 0.906250, mean_q: 100.421036\n",
      " 1695/10000: episode: 1364, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.729178, accuracy: 0.718750, mean_q: 100.112503\n",
      " 1696/10000: episode: 1365, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.578694, accuracy: 0.781250, mean_q: 99.708420\n",
      " 1698/10000: episode: 1366, duration: 0.022s, episode steps:   2, steps per second:  90, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [2.000, 10.000],  loss: 0.628526, accuracy: 0.828125, mean_q: 99.863045\n",
      " 1700/10000: episode: 1367, duration: 0.021s, episode steps:   2, steps per second:  97, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [0.000, 8.000],  loss: 0.974282, accuracy: 0.843750, mean_q: 100.185959\n",
      " 1701/10000: episode: 1368, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.586678, accuracy: 0.843750, mean_q: 100.450493\n",
      " 1703/10000: episode: 1369, duration: 0.020s, episode steps:   2, steps per second: 101, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [1.000, 8.000],  loss: 0.949883, accuracy: 0.843750, mean_q: 100.185883\n",
      " 1704/10000: episode: 1370, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.278311, accuracy: 0.843750, mean_q: 99.888855\n",
      " 1705/10000: episode: 1371, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.794068, accuracy: 0.781250, mean_q: 100.005974\n",
      " 1708/10000: episode: 1372, duration: 0.026s, episode steps:   3, steps per second: 117, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 8.000 [4.000, 10.000],  loss: 0.602817, accuracy: 0.770833, mean_q: 99.951134\n",
      " 1709/10000: episode: 1373, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.618388, accuracy: 0.750000, mean_q: 99.841888\n",
      " 1711/10000: episode: 1374, duration: 0.019s, episode steps:   2, steps per second: 104, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [2.000, 6.000],  loss: 0.240696, accuracy: 0.734375, mean_q: 99.981262\n",
      " 1712/10000: episode: 1375, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 146.576050, accuracy: 0.812500, mean_q: 100.141556\n",
      " 1713/10000: episode: 1376, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.617410, accuracy: 0.750000, mean_q: 99.695404\n",
      " 1714/10000: episode: 1377, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.145821, accuracy: 0.875000, mean_q: 99.458649\n",
      " 1715/10000: episode: 1378, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.318035, accuracy: 0.906250, mean_q: 99.758194\n",
      " 1716/10000: episode: 1379, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.644193, accuracy: 0.781250, mean_q: 99.943031\n",
      " 1717/10000: episode: 1380, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.607316, accuracy: 0.812500, mean_q: 100.453796\n",
      " 1718/10000: episode: 1381, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.032077, accuracy: 0.812500, mean_q: 100.203056\n",
      " 1719/10000: episode: 1382, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.155646, accuracy: 0.843750, mean_q: 100.081604\n",
      " 1721/10000: episode: 1383, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [4.000, 7.000],  loss: 69.465584, accuracy: 0.796875, mean_q: 99.734848\n",
      " 1722/10000: episode: 1384, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.210346, accuracy: 0.687500, mean_q: 99.208511\n",
      " 1723/10000: episode: 1385, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.520988, accuracy: 0.812500, mean_q: 99.415482\n",
      " 1724/10000: episode: 1386, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.696496, accuracy: 0.750000, mean_q: 100.085175\n",
      " 1725/10000: episode: 1387, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 5.038134, accuracy: 0.718750, mean_q: 99.932281\n",
      " 1726/10000: episode: 1388, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.166809, accuracy: 0.937500, mean_q: 100.004181\n",
      " 1727/10000: episode: 1389, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.574825, accuracy: 0.812500, mean_q: 99.864410\n",
      " 1728/10000: episode: 1390, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.388283, accuracy: 0.718750, mean_q: 100.003082\n",
      " 1729/10000: episode: 1391, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.755251, accuracy: 0.687500, mean_q: 100.552635\n",
      " 1731/10000: episode: 1392, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [2.000, 7.000],  loss: 63.183235, accuracy: 0.765625, mean_q: 100.152466\n",
      " 1732/10000: episode: 1393, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.379591, accuracy: 0.906250, mean_q: 99.929863\n",
      " 1733/10000: episode: 1394, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.938078, accuracy: 0.687500, mean_q: 99.700020\n",
      " 1734/10000: episode: 1395, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 5.595007, accuracy: 0.750000, mean_q: 101.093742\n",
      " 1735/10000: episode: 1396, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.230880, accuracy: 0.843750, mean_q: 100.341064\n",
      " 1736/10000: episode: 1397, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.030866, accuracy: 0.718750, mean_q: 99.497826\n",
      " 1737/10000: episode: 1398, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.300955, accuracy: 0.812500, mean_q: 100.183746\n",
      " 1738/10000: episode: 1399, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.565219, accuracy: 0.781250, mean_q: 100.053322\n",
      " 1739/10000: episode: 1400, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.199467, accuracy: 0.843750, mean_q: 99.427971\n",
      " 1740/10000: episode: 1401, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 4.040047, accuracy: 0.750000, mean_q: 99.683716\n",
      " 1741/10000: episode: 1402, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.560824, accuracy: 0.656250, mean_q: 99.619759\n",
      " 1742/10000: episode: 1403, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 140.954712, accuracy: 0.718750, mean_q: 98.966133\n",
      " 1743/10000: episode: 1404, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.004048, accuracy: 0.781250, mean_q: 99.394775\n",
      " 1744/10000: episode: 1405, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.081698, accuracy: 0.750000, mean_q: 99.704880\n",
      " 1745/10000: episode: 1406, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.910419, accuracy: 0.687500, mean_q: 101.093018\n",
      " 1746/10000: episode: 1407, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 161.413620, accuracy: 0.781250, mean_q: 100.902321\n",
      " 1747/10000: episode: 1408, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.198071, accuracy: 0.656250, mean_q: 99.903519\n",
      " 1748/10000: episode: 1409, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.392902, accuracy: 0.781250, mean_q: 99.044334\n",
      " 1749/10000: episode: 1410, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.283305, accuracy: 0.781250, mean_q: 99.879097\n",
      " 1750/10000: episode: 1411, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.215485, accuracy: 0.656250, mean_q: 101.159233\n",
      " 1770/10000: episode: 1412, duration: 0.129s, episode steps:  20, steps per second: 155, episode reward: -200.000, mean reward: -10.000 [-10.000, -10.000], mean action: 7.750 [3.000, 8.000],  loss: 21.780975, accuracy: 0.748438, mean_q: 100.054733\n",
      " 1771/10000: episode: 1413, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 117.624718, accuracy: 0.656250, mean_q: 99.782707\n",
      " 1772/10000: episode: 1414, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 96.388786, accuracy: 0.687500, mean_q: 99.423424\n",
      " 1773/10000: episode: 1415, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 7.825060, accuracy: 0.656250, mean_q: 100.062759\n",
      " 1774/10000: episode: 1416, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.576815, accuracy: 0.843750, mean_q: 99.792358\n",
      " 1775/10000: episode: 1417, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 6.354940, accuracy: 0.718750, mean_q: 101.848038\n",
      " 1776/10000: episode: 1418, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.030779, accuracy: 0.812500, mean_q: 101.362320\n",
      " 1777/10000: episode: 1419, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 9.014706, accuracy: 0.718750, mean_q: 101.444534\n",
      " 1779/10000: episode: 1420, duration: 0.019s, episode steps:   2, steps per second: 103, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [1.000, 8.000],  loss: 3.395010, accuracy: 0.765625, mean_q: 100.327187\n",
      " 1781/10000: episode: 1421, duration: 0.019s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [0.000, 3.000],  loss: 5.777885, accuracy: 0.718750, mean_q: 100.469528\n",
      " 1792/10000: episode: 1422, duration: 0.074s, episode steps:  11, steps per second: 148, episode reward:  0.000, mean reward:  0.000 [-10.000, 100.000], mean action: 9.273 [3.000, 10.000],  loss: 2.455113, accuracy: 0.755682, mean_q: 100.124954\n",
      " 1793/10000: episode: 1423, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.786990, accuracy: 0.718750, mean_q: 100.226044\n",
      " 1795/10000: episode: 1424, duration: 0.020s, episode steps:   2, steps per second:  98, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [3.000, 4.000],  loss: 1.390600, accuracy: 0.843750, mean_q: 100.284073\n",
      " 1796/10000: episode: 1425, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.068481, accuracy: 0.531250, mean_q: 99.919815\n",
      " 1797/10000: episode: 1426, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.769736, accuracy: 0.750000, mean_q: 99.840103\n",
      " 1798/10000: episode: 1427, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.820551, accuracy: 0.812500, mean_q: 100.235870\n",
      " 1799/10000: episode: 1428, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.179109, accuracy: 0.781250, mean_q: 100.680222\n",
      " 1800/10000: episode: 1429, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.563469, accuracy: 0.843750, mean_q: 100.372131\n",
      " 1801/10000: episode: 1430, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.681222, accuracy: 0.781250, mean_q: 99.928169\n",
      " 1802/10000: episode: 1431, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 148.080261, accuracy: 0.687500, mean_q: 99.609665\n",
      " 1805/10000: episode: 1432, duration: 0.025s, episode steps:   3, steps per second: 120, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 8.333 [4.000, 11.000],  loss: 1.701256, accuracy: 0.833333, mean_q: 99.474976\n",
      " 1806/10000: episode: 1433, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.957484, accuracy: 0.718750, mean_q: 100.318977\n",
      " 1807/10000: episode: 1434, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.605236, accuracy: 0.812500, mean_q: 100.143707\n",
      " 1808/10000: episode: 1435, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 137.646255, accuracy: 0.875000, mean_q: 100.593475\n",
      " 1809/10000: episode: 1436, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.672394, accuracy: 0.906250, mean_q: 99.809189\n",
      " 1810/10000: episode: 1437, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.545249, accuracy: 0.812500, mean_q: 99.594536\n",
      " 1811/10000: episode: 1438, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.968573, accuracy: 0.812500, mean_q: 99.623764\n",
      " 1812/10000: episode: 1439, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.645055, accuracy: 0.812500, mean_q: 99.668877\n",
      " 1813/10000: episode: 1440, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.792618, accuracy: 0.906250, mean_q: 99.661865\n",
      " 1814/10000: episode: 1441, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.719823, accuracy: 0.812500, mean_q: 99.732964\n",
      " 1815/10000: episode: 1442, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.147285, accuracy: 0.781250, mean_q: 100.438683\n",
      " 1816/10000: episode: 1443, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.621946, accuracy: 0.718750, mean_q: 99.949997\n",
      " 1817/10000: episode: 1444, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.361790, accuracy: 0.718750, mean_q: 100.266434\n",
      " 1819/10000: episode: 1445, duration: 0.045s, episode steps:   2, steps per second:  44, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [4.000, 11.000],  loss: 74.097610, accuracy: 0.718750, mean_q: 99.508102\n",
      " 1820/10000: episode: 1446, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.901512, accuracy: 0.656250, mean_q: 99.322128\n",
      " 1821/10000: episode: 1447, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 6.923957, accuracy: 0.687500, mean_q: 98.660492\n",
      " 1822/10000: episode: 1448, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.385082, accuracy: 0.687500, mean_q: 100.848419\n",
      " 1823/10000: episode: 1449, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.795557, accuracy: 0.812500, mean_q: 99.780563\n",
      " 1824/10000: episode: 1450, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 9.510650, accuracy: 0.718750, mean_q: 99.059837\n",
      " 1825/10000: episode: 1451, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 129.887894, accuracy: 0.843750, mean_q: 99.298553\n",
      " 1826/10000: episode: 1452, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 6.087322, accuracy: 0.812500, mean_q: 99.558304\n",
      " 1827/10000: episode: 1453, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.356815, accuracy: 0.812500, mean_q: 99.869232\n",
      " 1828/10000: episode: 1454, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.933791, accuracy: 0.718750, mean_q: 100.375076\n",
      " 1829/10000: episode: 1455, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.876667, accuracy: 0.843750, mean_q: 100.558372\n",
      " 1830/10000: episode: 1456, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.817212, accuracy: 0.906250, mean_q: 100.003586\n",
      " 1831/10000: episode: 1457, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.376911, accuracy: 0.781250, mean_q: 100.180954\n",
      " 1832/10000: episode: 1458, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.487514, accuracy: 0.812500, mean_q: 100.277405\n",
      " 1833/10000: episode: 1459, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.538879, accuracy: 0.906250, mean_q: 100.415344\n",
      " 1834/10000: episode: 1460, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.343113, accuracy: 0.843750, mean_q: 100.048996\n",
      " 1835/10000: episode: 1461, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.196830, accuracy: 0.687500, mean_q: 100.368896\n",
      " 1836/10000: episode: 1462, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.431574, accuracy: 0.875000, mean_q: 100.404861\n",
      " 1837/10000: episode: 1463, duration: 0.027s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 3.226917, accuracy: 0.750000, mean_q: 100.032120\n",
      " 1838/10000: episode: 1464, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.474003, accuracy: 0.750000, mean_q: 99.689812\n",
      " 1839/10000: episode: 1465, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.679915, accuracy: 0.812500, mean_q: 99.894661\n",
      " 1840/10000: episode: 1466, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.412533, accuracy: 0.718750, mean_q: 100.368256\n",
      " 1841/10000: episode: 1467, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.355777, accuracy: 0.812500, mean_q: 100.720207\n",
      " 1843/10000: episode: 1468, duration: 0.047s, episode steps:   2, steps per second:  43, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [6.000, 9.000],  loss: 1.268563, accuracy: 0.750000, mean_q: 100.352013\n",
      " 1844/10000: episode: 1469, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.298452, accuracy: 0.906250, mean_q: 99.989090\n",
      " 1845/10000: episode: 1470, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 9.386995, accuracy: 0.687500, mean_q: 100.050659\n",
      " 1846/10000: episode: 1471, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.995221, accuracy: 0.875000, mean_q: 100.462387\n",
      " 1847/10000: episode: 1472, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.901912, accuracy: 0.781250, mean_q: 100.477203\n",
      " 1848/10000: episode: 1473, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.264564, accuracy: 0.812500, mean_q: 100.321785\n",
      " 1850/10000: episode: 1474, duration: 0.051s, episode steps:   2, steps per second:  39, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [1.000, 9.000],  loss: 0.699872, accuracy: 0.890625, mean_q: 99.770027\n",
      " 1851/10000: episode: 1475, duration: 0.032s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.380399, accuracy: 0.812500, mean_q: 99.776947\n",
      " 1852/10000: episode: 1476, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.892332, accuracy: 0.718750, mean_q: 100.155090\n",
      " 1853/10000: episode: 1477, duration: 0.049s, episode steps:   1, steps per second:  21, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.850959, accuracy: 0.718750, mean_q: 100.353706\n",
      " 1854/10000: episode: 1478, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.671962, accuracy: 0.687500, mean_q: 100.225525\n",
      " 1855/10000: episode: 1479, duration: 0.051s, episode steps:   1, steps per second:  20, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.527140, accuracy: 0.781250, mean_q: 99.700699\n",
      " 1856/10000: episode: 1480, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.017065, accuracy: 0.781250, mean_q: 98.677963\n",
      " 1857/10000: episode: 1481, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.494010, accuracy: 0.968750, mean_q: 99.462746\n",
      " 1858/10000: episode: 1482, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.935142, accuracy: 0.718750, mean_q: 100.240372\n",
      " 1859/10000: episode: 1483, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.983230, accuracy: 0.750000, mean_q: 100.205750\n",
      " 1861/10000: episode: 1484, duration: 0.019s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [0.000, 7.000],  loss: 0.577635, accuracy: 0.859375, mean_q: 99.869843\n",
      " 1862/10000: episode: 1485, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.686759, accuracy: 0.718750, mean_q: 99.985100\n",
      " 1863/10000: episode: 1486, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.706553, accuracy: 0.781250, mean_q: 99.892914\n",
      " 1864/10000: episode: 1487, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.432748, accuracy: 0.812500, mean_q: 100.208458\n",
      " 1865/10000: episode: 1488, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.701358, accuracy: 0.843750, mean_q: 99.979477\n",
      " 1866/10000: episode: 1489, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.368721, accuracy: 0.781250, mean_q: 99.709152\n",
      " 1868/10000: episode: 1490, duration: 0.019s, episode steps:   2, steps per second: 103, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: 1.240586, accuracy: 0.765625, mean_q: 99.856293\n",
      " 1869/10000: episode: 1491, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.000387, accuracy: 0.843750, mean_q: 100.438515\n",
      " 1870/10000: episode: 1492, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.332417, accuracy: 0.781250, mean_q: 100.521698\n",
      " 1871/10000: episode: 1493, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.392546, accuracy: 0.750000, mean_q: 100.601349\n",
      " 1873/10000: episode: 1494, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [8.000, 11.000],  loss: 1.375780, accuracy: 0.656250, mean_q: 100.131790\n",
      " 1874/10000: episode: 1495, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.522833, accuracy: 0.875000, mean_q: 100.143089\n",
      " 1875/10000: episode: 1496, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.102623, accuracy: 0.812500, mean_q: 100.036484\n",
      " 1876/10000: episode: 1497, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.916179, accuracy: 0.812500, mean_q: 99.972382\n",
      " 1878/10000: episode: 1498, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [7.000, 9.000],  loss: 0.728309, accuracy: 0.812500, mean_q: 100.118851\n",
      " 1879/10000: episode: 1499, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.120519, accuracy: 0.812500, mean_q: 100.149063\n",
      " 1880/10000: episode: 1500, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.668366, accuracy: 0.906250, mean_q: 99.918961\n",
      " 1882/10000: episode: 1501, duration: 0.018s, episode steps:   2, steps per second: 109, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 0.480572, accuracy: 0.781250, mean_q: 99.923965\n",
      " 1883/10000: episode: 1502, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.561792, accuracy: 0.781250, mean_q: 99.996605\n",
      " 1884/10000: episode: 1503, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.602097, accuracy: 0.875000, mean_q: 99.973373\n",
      " 1885/10000: episode: 1504, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.097837, accuracy: 0.906250, mean_q: 100.046432\n",
      " 1886/10000: episode: 1505, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 151.922791, accuracy: 0.875000, mean_q: 99.887787\n",
      " 1887/10000: episode: 1506, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.444268, accuracy: 0.843750, mean_q: 99.422958\n",
      " 1888/10000: episode: 1507, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.566288, accuracy: 0.875000, mean_q: 99.366905\n",
      " 1889/10000: episode: 1508, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.539661, accuracy: 0.875000, mean_q: 99.203392\n",
      " 1890/10000: episode: 1509, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.591888, accuracy: 0.781250, mean_q: 99.607674\n",
      " 1891/10000: episode: 1510, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.282964, accuracy: 0.750000, mean_q: 100.423599\n",
      " 1892/10000: episode: 1511, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.725982, accuracy: 0.906250, mean_q: 100.549713\n",
      " 1893/10000: episode: 1512, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 98.998856, accuracy: 0.781250, mean_q: 100.420158\n",
      " 1894/10000: episode: 1513, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.160242, accuracy: 0.687500, mean_q: 99.290070\n",
      " 1895/10000: episode: 1514, duration: 0.070s, episode steps:   1, steps per second:  14, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.131805, accuracy: 0.875000, mean_q: 99.597832\n",
      " 1896/10000: episode: 1515, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.659870, accuracy: 0.875000, mean_q: 100.416687\n",
      " 1897/10000: episode: 1516, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.823874, accuracy: 0.906250, mean_q: 100.738960\n",
      " 1898/10000: episode: 1517, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 224.584869, accuracy: 0.812500, mean_q: 100.071518\n",
      " 1899/10000: episode: 1518, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 9.695835, accuracy: 0.812500, mean_q: 98.270020\n",
      " 1900/10000: episode: 1519, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 85.165840, accuracy: 0.500000, mean_q: 99.039619\n",
      " 1901/10000: episode: 1520, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.331927, accuracy: 0.843750, mean_q: 99.966522\n",
      " 1902/10000: episode: 1521, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.998979, accuracy: 0.781250, mean_q: 100.776367\n",
      " 1903/10000: episode: 1522, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 141.528809, accuracy: 0.750000, mean_q: 101.052597\n",
      " 1904/10000: episode: 1523, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.912751, accuracy: 0.812500, mean_q: 99.402336\n",
      " 1905/10000: episode: 1524, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.200824, accuracy: 0.812500, mean_q: 98.458313\n",
      " 1906/10000: episode: 1525, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.921389, accuracy: 0.875000, mean_q: 99.392281\n",
      " 1907/10000: episode: 1526, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.578845, accuracy: 0.750000, mean_q: 100.221741\n",
      " 1908/10000: episode: 1527, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.997493, accuracy: 0.750000, mean_q: 100.707207\n",
      " 1909/10000: episode: 1528, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.302553, accuracy: 0.750000, mean_q: 100.185379\n",
      " 1910/10000: episode: 1529, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.003954, accuracy: 0.750000, mean_q: 99.890915\n",
      " 1911/10000: episode: 1530, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.541334, accuracy: 0.843750, mean_q: 99.556580\n",
      " 1912/10000: episode: 1531, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 121.207764, accuracy: 0.843750, mean_q: 100.183075\n",
      " 1913/10000: episode: 1532, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.532755, accuracy: 0.843750, mean_q: 100.038033\n",
      " 1914/10000: episode: 1533, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.680354, accuracy: 0.718750, mean_q: 100.534439\n",
      " 1915/10000: episode: 1534, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.031076, accuracy: 0.812500, mean_q: 99.521759\n",
      " 1916/10000: episode: 1535, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.563971, accuracy: 0.843750, mean_q: 99.311401\n",
      " 1917/10000: episode: 1536, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 6.023979, accuracy: 0.875000, mean_q: 99.674591\n",
      " 1918/10000: episode: 1537, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.483409, accuracy: 0.687500, mean_q: 100.542465\n",
      " 1919/10000: episode: 1538, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.752678, accuracy: 0.812500, mean_q: 101.217361\n",
      " 1920/10000: episode: 1539, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.329717, accuracy: 0.750000, mean_q: 100.266281\n",
      " 1921/10000: episode: 1540, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.097797, accuracy: 0.906250, mean_q: 100.090561\n",
      " 1922/10000: episode: 1541, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.251811, accuracy: 0.843750, mean_q: 99.680161\n",
      " 1923/10000: episode: 1542, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.738567, accuracy: 0.812500, mean_q: 100.204422\n",
      " 1924/10000: episode: 1543, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.415545, accuracy: 0.906250, mean_q: 100.186371\n",
      " 1925/10000: episode: 1544, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.481663, accuracy: 0.812500, mean_q: 100.553383\n",
      " 1926/10000: episode: 1545, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.639848, accuracy: 0.812500, mean_q: 100.041458\n",
      " 1927/10000: episode: 1546, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.278449, accuracy: 0.781250, mean_q: 100.004715\n",
      " 1928/10000: episode: 1547, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.176230, accuracy: 0.781250, mean_q: 100.027664\n",
      " 1929/10000: episode: 1548, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.908051, accuracy: 0.750000, mean_q: 100.535698\n",
      " 1930/10000: episode: 1549, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.151399, accuracy: 0.687500, mean_q: 100.504257\n",
      " 1931/10000: episode: 1550, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 4.085222, accuracy: 0.750000, mean_q: 100.390228\n",
      " 1932/10000: episode: 1551, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.724916, accuracy: 0.843750, mean_q: 100.610641\n",
      " 1933/10000: episode: 1552, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.514518, accuracy: 0.718750, mean_q: 99.936867\n",
      " 1934/10000: episode: 1553, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.512071, accuracy: 0.875000, mean_q: 99.780144\n",
      " 1935/10000: episode: 1554, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.427425, accuracy: 0.750000, mean_q: 99.785316\n",
      " 1936/10000: episode: 1555, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.304424, accuracy: 0.812500, mean_q: 99.901352\n",
      " 1938/10000: episode: 1556, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 0.811045, accuracy: 0.781250, mean_q: 100.262215\n",
      " 1939/10000: episode: 1557, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.215283, accuracy: 0.906250, mean_q: 100.292816\n",
      " 1940/10000: episode: 1558, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.549968, accuracy: 0.625000, mean_q: 100.007500\n",
      " 1941/10000: episode: 1559, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.191289, accuracy: 0.875000, mean_q: 99.642075\n",
      " 1942/10000: episode: 1560, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.574278, accuracy: 0.906250, mean_q: 99.258461\n",
      " 1943/10000: episode: 1561, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.859971, accuracy: 0.843750, mean_q: 99.005585\n",
      " 1944/10000: episode: 1562, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.430670, accuracy: 0.781250, mean_q: 99.312347\n",
      " 1946/10000: episode: 1563, duration: 0.018s, episode steps:   2, steps per second: 110, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [2.000, 3.000],  loss: 0.563350, accuracy: 0.828125, mean_q: 100.196426\n",
      " 1947/10000: episode: 1564, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.776093, accuracy: 0.906250, mean_q: 100.642471\n",
      " 1949/10000: episode: 1565, duration: 0.019s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [5.000, 10.000],  loss: 78.454872, accuracy: 0.843750, mean_q: 99.784203\n",
      " 1950/10000: episode: 1566, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.805015, accuracy: 0.812500, mean_q: 99.228302\n",
      " 1951/10000: episode: 1567, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.031990, accuracy: 0.812500, mean_q: 99.670456\n",
      " 1952/10000: episode: 1568, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 5.808346, accuracy: 0.875000, mean_q: 100.382019\n",
      " 1953/10000: episode: 1569, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.918942, accuracy: 0.843750, mean_q: 100.185799\n",
      " 1954/10000: episode: 1570, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.507976, accuracy: 0.937500, mean_q: 100.164207\n",
      " 1955/10000: episode: 1571, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.290971, accuracy: 0.781250, mean_q: 99.538033\n",
      " 1956/10000: episode: 1572, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 7.978174, accuracy: 0.687500, mean_q: 99.694016\n",
      " 1957/10000: episode: 1573, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.166855, accuracy: 0.781250, mean_q: 100.173088\n",
      " 1958/10000: episode: 1574, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.438246, accuracy: 0.875000, mean_q: 99.461334\n",
      " 1959/10000: episode: 1575, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.897169, accuracy: 0.781250, mean_q: 100.142090\n",
      " 1960/10000: episode: 1576, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.528733, accuracy: 0.875000, mean_q: 100.232635\n",
      " 1961/10000: episode: 1577, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.637600, accuracy: 0.750000, mean_q: 100.435356\n",
      " 1962/10000: episode: 1578, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.849081, accuracy: 0.875000, mean_q: 100.417145\n",
      " 1963/10000: episode: 1579, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.829146, accuracy: 0.687500, mean_q: 100.113388\n",
      " 1964/10000: episode: 1580, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.746383, accuracy: 0.843750, mean_q: 99.869019\n",
      " 1965/10000: episode: 1581, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.577036, accuracy: 0.781250, mean_q: 99.514084\n",
      " 1966/10000: episode: 1582, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.912900, accuracy: 0.781250, mean_q: 99.395294\n",
      " 1967/10000: episode: 1583, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.233281, accuracy: 0.875000, mean_q: 100.321609\n",
      " 1968/10000: episode: 1584, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.350652, accuracy: 0.812500, mean_q: 100.287437\n",
      " 1969/10000: episode: 1585, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.453189, accuracy: 0.812500, mean_q: 100.469589\n",
      " 1970/10000: episode: 1586, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.841536, accuracy: 0.750000, mean_q: 99.901627\n",
      " 1971/10000: episode: 1587, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 131.558960, accuracy: 0.812500, mean_q: 99.851196\n",
      " 1973/10000: episode: 1588, duration: 0.021s, episode steps:   2, steps per second:  93, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [9.000, 10.000],  loss: 1.585570, accuracy: 0.796875, mean_q: 99.716064\n",
      " 1974/10000: episode: 1589, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 76.108398, accuracy: 0.593750, mean_q: 99.865997\n",
      " 1975/10000: episode: 1590, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.031098, accuracy: 0.906250, mean_q: 99.512512\n",
      " 1976/10000: episode: 1591, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.863013, accuracy: 0.812500, mean_q: 99.822220\n",
      " 1977/10000: episode: 1592, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.195348, accuracy: 0.843750, mean_q: 100.761864\n",
      " 1978/10000: episode: 1593, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.975961, accuracy: 0.781250, mean_q: 100.497208\n",
      " 1979/10000: episode: 1594, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.740457, accuracy: 0.875000, mean_q: 100.592972\n",
      " 1980/10000: episode: 1595, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.821973, accuracy: 0.687500, mean_q: 99.842628\n",
      " 1981/10000: episode: 1596, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.738867, accuracy: 0.781250, mean_q: 99.701126\n",
      " 1982/10000: episode: 1597, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.927774, accuracy: 0.875000, mean_q: 99.783066\n",
      " 1983/10000: episode: 1598, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 150.877411, accuracy: 0.812500, mean_q: 99.894432\n",
      " 1984/10000: episode: 1599, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.789690, accuracy: 0.812500, mean_q: 99.917809\n",
      " 1986/10000: episode: 1600, duration: 0.021s, episode steps:   2, steps per second:  96, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [3.000, 11.000],  loss: 1.285790, accuracy: 0.765625, mean_q: 99.910576\n",
      " 1987/10000: episode: 1601, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.042802, accuracy: 0.812500, mean_q: 99.802307\n",
      " 1988/10000: episode: 1602, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.380008, accuracy: 0.718750, mean_q: 99.905052\n",
      " 1989/10000: episode: 1603, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.907898, accuracy: 0.718750, mean_q: 99.870590\n",
      " 1990/10000: episode: 1604, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.456924, accuracy: 0.781250, mean_q: 100.745560\n",
      " 1991/10000: episode: 1605, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.517477, accuracy: 0.812500, mean_q: 99.966599\n",
      " 1992/10000: episode: 1606, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.911991, accuracy: 0.843750, mean_q: 100.110641\n",
      " 1993/10000: episode: 1607, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.365088, accuracy: 0.781250, mean_q: 100.092575\n",
      " 1994/10000: episode: 1608, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.353173, accuracy: 0.843750, mean_q: 100.119644\n",
      " 1995/10000: episode: 1609, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.389824, accuracy: 0.843750, mean_q: 100.273514\n",
      " 1996/10000: episode: 1610, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.239486, accuracy: 0.750000, mean_q: 100.073532\n",
      " 1997/10000: episode: 1611, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.167334, accuracy: 0.843750, mean_q: 100.155594\n",
      " 1998/10000: episode: 1612, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.348934, accuracy: 0.812500, mean_q: 100.082512\n",
      " 1999/10000: episode: 1613, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.273255, accuracy: 0.843750, mean_q: 99.869522\n",
      " 2001/10000: episode: 1614, duration: 0.029s, episode steps:   2, steps per second:  70, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.000 [9.000, 11.000],  loss: 0.281973, accuracy: 0.750000, mean_q: 100.118408\n",
      " 2002/10000: episode: 1615, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.406633, accuracy: 0.812500, mean_q: 100.126686\n",
      " 2003/10000: episode: 1616, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 153.855560, accuracy: 0.843750, mean_q: 100.340782\n",
      " 2004/10000: episode: 1617, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.500872, accuracy: 0.750000, mean_q: 99.425980\n",
      " 2005/10000: episode: 1618, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 155.900833, accuracy: 0.906250, mean_q: 99.403687\n",
      " 2006/10000: episode: 1619, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.048299, accuracy: 0.843750, mean_q: 98.998810\n",
      " 2007/10000: episode: 1620, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.362432, accuracy: 0.875000, mean_q: 99.829353\n",
      " 2008/10000: episode: 1621, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.679574, accuracy: 0.781250, mean_q: 100.287476\n",
      " 2009/10000: episode: 1622, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.952661, accuracy: 0.843750, mean_q: 100.924255\n",
      " 2010/10000: episode: 1623, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.853290, accuracy: 0.843750, mean_q: 100.991806\n",
      " 2011/10000: episode: 1624, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.793704, accuracy: 0.812500, mean_q: 100.109436\n",
      " 2013/10000: episode: 1625, duration: 0.018s, episode steps:   2, steps per second: 113, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [1.000, 10.000],  loss: 75.907074, accuracy: 0.843750, mean_q: 99.405670\n",
      " 2015/10000: episode: 1626, duration: 0.019s, episode steps:   2, steps per second: 104, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [2.000, 8.000],  loss: 3.043505, accuracy: 0.828125, mean_q: 99.020874\n",
      " 2016/10000: episode: 1627, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.706577, accuracy: 0.843750, mean_q: 100.730537\n",
      " 2017/10000: episode: 1628, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 152.172821, accuracy: 0.875000, mean_q: 100.837097\n",
      " 2018/10000: episode: 1629, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.859966, accuracy: 0.843750, mean_q: 98.732590\n",
      " 2019/10000: episode: 1630, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 141.622986, accuracy: 0.781250, mean_q: 98.732788\n",
      " 2020/10000: episode: 1631, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 26.868036, accuracy: 0.812500, mean_q: 98.778511\n",
      " 2021/10000: episode: 1632, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.877441, accuracy: 0.875000, mean_q: 100.211685\n",
      " 2023/10000: episode: 1633, duration: 0.020s, episode steps:   2, steps per second:  98, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [8.000, 9.000],  loss: 2.522077, accuracy: 0.812500, mean_q: 101.285507\n",
      " 2024/10000: episode: 1634, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.219025, accuracy: 0.875000, mean_q: 100.577301\n",
      " 2025/10000: episode: 1635, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 149.616501, accuracy: 0.750000, mean_q: 99.427139\n",
      " 2026/10000: episode: 1636, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.981710, accuracy: 0.750000, mean_q: 98.659317\n",
      " 2027/10000: episode: 1637, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.586990, accuracy: 0.718750, mean_q: 99.554138\n",
      " 2028/10000: episode: 1638, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 7.569820, accuracy: 0.593750, mean_q: 100.754700\n",
      " 2029/10000: episode: 1639, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.883036, accuracy: 0.875000, mean_q: 101.215813\n",
      " 2031/10000: episode: 1640, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: 2.805523, accuracy: 0.718750, mean_q: 100.733559\n",
      " 2032/10000: episode: 1641, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.296480, accuracy: 0.781250, mean_q: 99.107971\n",
      " 2035/10000: episode: 1642, duration: 0.032s, episode steps:   3, steps per second:  93, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 4.667 [1.000, 7.000],  loss: 4.321661, accuracy: 0.802083, mean_q: 99.039360\n",
      " 2036/10000: episode: 1643, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 4.183482, accuracy: 0.812500, mean_q: 100.520576\n",
      " 2037/10000: episode: 1644, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.761518, accuracy: 0.906250, mean_q: 100.929344\n",
      " 2038/10000: episode: 1645, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.004790, accuracy: 0.843750, mean_q: 99.896988\n",
      " 2039/10000: episode: 1646, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.776691, accuracy: 0.843750, mean_q: 99.326630\n",
      " 2040/10000: episode: 1647, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.568781, accuracy: 0.718750, mean_q: 99.912987\n",
      " 2041/10000: episode: 1648, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.651726, accuracy: 0.718750, mean_q: 100.043449\n",
      " 2042/10000: episode: 1649, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.791830, accuracy: 0.781250, mean_q: 100.331100\n",
      " 2044/10000: episode: 1650, duration: 0.019s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [3.000, 8.000],  loss: 0.476544, accuracy: 0.828125, mean_q: 99.884842\n",
      " 2045/10000: episode: 1651, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.678089, accuracy: 0.718750, mean_q: 99.490723\n",
      " 2053/10000: episode: 1652, duration: 0.057s, episode steps:   8, steps per second: 141, episode reward: 30.000, mean reward:  3.750 [-10.000, 100.000], mean action: 7.000 [3.000, 8.000],  loss: 34.954418, accuracy: 0.742188, mean_q: 99.608330\n",
      " 2054/10000: episode: 1653, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 5.440877, accuracy: 0.812500, mean_q: 100.338898\n",
      " 2055/10000: episode: 1654, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.303285, accuracy: 0.812500, mean_q: 100.355713\n",
      " 2056/10000: episode: 1655, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.909538, accuracy: 0.906250, mean_q: 100.574631\n",
      " 2057/10000: episode: 1656, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.314230, accuracy: 0.625000, mean_q: 99.623276\n",
      " 2058/10000: episode: 1657, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.354292, accuracy: 0.812500, mean_q: 99.485260\n",
      " 2059/10000: episode: 1658, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 10.869380, accuracy: 0.718750, mean_q: 100.391754\n",
      " 2060/10000: episode: 1659, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.727303, accuracy: 0.906250, mean_q: 100.626869\n",
      " 2062/10000: episode: 1660, duration: 0.018s, episode steps:   2, steps per second: 110, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [8.000, 11.000],  loss: 3.220962, accuracy: 0.812500, mean_q: 100.473938\n",
      " 2063/10000: episode: 1661, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.730664, accuracy: 0.781250, mean_q: 100.140305\n",
      " 2064/10000: episode: 1662, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.630229, accuracy: 0.906250, mean_q: 99.372292\n",
      " 2065/10000: episode: 1663, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.934210, accuracy: 0.843750, mean_q: 99.489456\n",
      " 2066/10000: episode: 1664, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.479362, accuracy: 0.875000, mean_q: 99.692123\n",
      " 2068/10000: episode: 1665, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [5.000, 8.000],  loss: 4.042259, accuracy: 0.765625, mean_q: 99.920258\n",
      " 2069/10000: episode: 1666, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.794002, accuracy: 0.781250, mean_q: 99.693359\n",
      " 2070/10000: episode: 1667, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.011952, accuracy: 0.843750, mean_q: 101.168701\n",
      " 2071/10000: episode: 1668, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.675979, accuracy: 0.843750, mean_q: 100.040833\n",
      " 2072/10000: episode: 1669, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.884779, accuracy: 0.750000, mean_q: 99.126938\n",
      " 2074/10000: episode: 1670, duration: 0.028s, episode steps:   2, steps per second:  72, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [3.000, 8.000],  loss: 1.082813, accuracy: 0.812500, mean_q: 99.946014\n",
      " 2075/10000: episode: 1671, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.620598, accuracy: 0.937500, mean_q: 100.448853\n",
      " 2076/10000: episode: 1672, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.245475, accuracy: 0.812500, mean_q: 99.875542\n",
      " 2077/10000: episode: 1673, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.294721, accuracy: 0.687500, mean_q: 99.673752\n",
      " 2078/10000: episode: 1674, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.896056, accuracy: 0.875000, mean_q: 99.924728\n",
      " 2079/10000: episode: 1675, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.096661, accuracy: 0.625000, mean_q: 100.498779\n",
      " 2080/10000: episode: 1676, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.535320, accuracy: 0.750000, mean_q: 100.012238\n",
      " 2081/10000: episode: 1677, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.630242, accuracy: 0.906250, mean_q: 99.856644\n",
      " 2082/10000: episode: 1678, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.466651, accuracy: 0.812500, mean_q: 100.040848\n",
      " 2083/10000: episode: 1679, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 154.415756, accuracy: 0.875000, mean_q: 99.939102\n",
      " 2084/10000: episode: 1680, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.872713, accuracy: 0.812500, mean_q: 99.362701\n",
      " 2085/10000: episode: 1681, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.224909, accuracy: 0.843750, mean_q: 99.609497\n",
      " 2086/10000: episode: 1682, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.622481, accuracy: 0.812500, mean_q: 99.921829\n",
      " 2088/10000: episode: 1683, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [0.000, 5.000],  loss: 2.599737, accuracy: 0.765625, mean_q: 100.767441\n",
      " 2089/10000: episode: 1684, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.008100, accuracy: 0.843750, mean_q: 101.284210\n",
      " 2090/10000: episode: 1685, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.500232, accuracy: 0.750000, mean_q: 100.567505\n",
      " 2091/10000: episode: 1686, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.990247, accuracy: 0.781250, mean_q: 99.371262\n",
      " 2092/10000: episode: 1687, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.983952, accuracy: 0.843750, mean_q: 98.511726\n",
      " 2093/10000: episode: 1688, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 10.271214, accuracy: 0.781250, mean_q: 98.443573\n",
      " 2094/10000: episode: 1689, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.031792, accuracy: 0.812500, mean_q: 100.679161\n",
      " 2095/10000: episode: 1690, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.622927, accuracy: 0.656250, mean_q: 101.426079\n",
      " 2096/10000: episode: 1691, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.480932, accuracy: 0.875000, mean_q: 100.646431\n",
      " 2097/10000: episode: 1692, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.254615, accuracy: 0.812500, mean_q: 100.518562\n",
      " 2098/10000: episode: 1693, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.693721, accuracy: 0.781250, mean_q: 99.583084\n",
      " 2099/10000: episode: 1694, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.857693, accuracy: 0.812500, mean_q: 99.201401\n",
      " 2100/10000: episode: 1695, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.551238, accuracy: 0.625000, mean_q: 100.171158\n",
      " 2101/10000: episode: 1696, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.565827, accuracy: 0.812500, mean_q: 100.916702\n",
      " 2102/10000: episode: 1697, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.879327, accuracy: 0.843750, mean_q: 100.523254\n",
      " 2103/10000: episode: 1698, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 5.298859, accuracy: 0.750000, mean_q: 99.475327\n",
      " 2104/10000: episode: 1699, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 4.594212, accuracy: 0.812500, mean_q: 99.812363\n",
      " 2105/10000: episode: 1700, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 4.315973, accuracy: 0.687500, mean_q: 100.036896\n",
      " 2106/10000: episode: 1701, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.688665, accuracy: 0.781250, mean_q: 100.607498\n",
      " 2107/10000: episode: 1702, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.190609, accuracy: 0.937500, mean_q: 100.472366\n",
      " 2108/10000: episode: 1703, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.714636, accuracy: 0.781250, mean_q: 99.563225\n",
      " 2109/10000: episode: 1704, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.684495, accuracy: 0.718750, mean_q: 99.585587\n",
      " 2110/10000: episode: 1705, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.037420, accuracy: 0.750000, mean_q: 99.986931\n",
      " 2111/10000: episode: 1706, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.779942, accuracy: 0.812500, mean_q: 99.909416\n",
      " 2112/10000: episode: 1707, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.719427, accuracy: 0.718750, mean_q: 100.130997\n",
      " 2113/10000: episode: 1708, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.491670, accuracy: 0.812500, mean_q: 99.595177\n",
      " 2114/10000: episode: 1709, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.452800, accuracy: 0.750000, mean_q: 99.162170\n",
      " 2115/10000: episode: 1710, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.764761, accuracy: 0.718750, mean_q: 99.407394\n",
      " 2116/10000: episode: 1711, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.862774, accuracy: 0.812500, mean_q: 99.889099\n",
      " 2117/10000: episode: 1712, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 4.987644, accuracy: 0.656250, mean_q: 100.844406\n",
      " 2118/10000: episode: 1713, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.661057, accuracy: 0.750000, mean_q: 100.662384\n",
      " 2119/10000: episode: 1714, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.695564, accuracy: 0.750000, mean_q: 100.456268\n",
      " 2120/10000: episode: 1715, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.877558, accuracy: 0.937500, mean_q: 100.004303\n",
      " 2121/10000: episode: 1716, duration: 0.027s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 27.503134, accuracy: 0.812500, mean_q: 99.364975\n",
      " 2122/10000: episode: 1717, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.679327, accuracy: 0.750000, mean_q: 99.481018\n",
      " 2123/10000: episode: 1718, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.414060, accuracy: 0.812500, mean_q: 100.407761\n",
      " 2124/10000: episode: 1719, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.299117, accuracy: 0.812500, mean_q: 100.816528\n",
      " 2125/10000: episode: 1720, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.403163, accuracy: 0.812500, mean_q: 99.947647\n",
      " 2126/10000: episode: 1721, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.492186, accuracy: 0.937500, mean_q: 100.268875\n",
      " 2127/10000: episode: 1722, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.786735, accuracy: 0.843750, mean_q: 100.005585\n",
      " 2128/10000: episode: 1723, duration: 0.029s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.075384, accuracy: 0.812500, mean_q: 99.819885\n",
      " 2129/10000: episode: 1724, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.933866, accuracy: 0.781250, mean_q: 100.110031\n",
      " 2131/10000: episode: 1725, duration: 0.040s, episode steps:   2, steps per second:  50, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [6.000, 11.000],  loss: 0.808255, accuracy: 0.796875, mean_q: 100.016556\n",
      " 2132/10000: episode: 1726, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.990288, accuracy: 0.812500, mean_q: 100.130577\n",
      " 2133/10000: episode: 1727, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.182642, accuracy: 0.843750, mean_q: 100.247810\n",
      " 2134/10000: episode: 1728, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.402679, accuracy: 0.812500, mean_q: 100.307709\n",
      " 2136/10000: episode: 1729, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [3.000, 6.000],  loss: 1.563422, accuracy: 0.828125, mean_q: 100.208481\n",
      " 2137/10000: episode: 1730, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 145.105453, accuracy: 0.843750, mean_q: 100.250366\n",
      " 2138/10000: episode: 1731, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.487186, accuracy: 0.750000, mean_q: 99.684433\n",
      " 2139/10000: episode: 1732, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.514803, accuracy: 0.781250, mean_q: 99.559608\n",
      " 2140/10000: episode: 1733, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.093125, accuracy: 0.781250, mean_q: 100.164841\n",
      " 2142/10000: episode: 1734, duration: 0.038s, episode steps:   2, steps per second:  53, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [4.000, 5.000],  loss: 67.919281, accuracy: 0.765625, mean_q: 100.051865\n",
      " 2143/10000: episode: 1735, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.456882, accuracy: 0.937500, mean_q: 99.995186\n",
      " 2144/10000: episode: 1736, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.425486, accuracy: 0.781250, mean_q: 99.468285\n",
      " 2145/10000: episode: 1737, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 18.542284, accuracy: 0.875000, mean_q: 99.591248\n",
      " 2147/10000: episode: 1738, duration: 0.035s, episode steps:   2, steps per second:  57, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [4.000, 8.000],  loss: 7.004879, accuracy: 0.812500, mean_q: 100.185852\n",
      " 2148/10000: episode: 1739, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.755266, accuracy: 0.906250, mean_q: 100.549278\n",
      " 2149/10000: episode: 1740, duration: 0.031s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.390119, accuracy: 0.812500, mean_q: 100.231277\n",
      " 2151/10000: episode: 1741, duration: 0.037s, episode steps:   2, steps per second:  54, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [0.000, 8.000],  loss: 0.574062, accuracy: 0.859375, mean_q: 99.706383\n",
      " 2152/10000: episode: 1742, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.326515, accuracy: 0.812500, mean_q: 100.122536\n",
      " 2153/10000: episode: 1743, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.584661, accuracy: 0.656250, mean_q: 100.084213\n",
      " 2154/10000: episode: 1744, duration: 0.029s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 77.239594, accuracy: 0.875000, mean_q: 100.235626\n",
      " 2155/10000: episode: 1745, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 124.440819, accuracy: 0.812500, mean_q: 99.551437\n",
      " 2156/10000: episode: 1746, duration: 0.041s, episode steps:   1, steps per second:  24, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.530892, accuracy: 0.781250, mean_q: 99.061829\n",
      " 2157/10000: episode: 1747, duration: 0.035s, episode steps:   1, steps per second:  29, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 5.535483, accuracy: 0.843750, mean_q: 99.619087\n",
      " 2158/10000: episode: 1748, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.097020, accuracy: 0.937500, mean_q: 100.212479\n",
      " 2159/10000: episode: 1749, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.426489, accuracy: 0.812500, mean_q: 100.442345\n",
      " 2160/10000: episode: 1750, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.599168, accuracy: 0.875000, mean_q: 98.773727\n",
      " 2162/10000: episode: 1751, duration: 0.019s, episode steps:   2, steps per second: 107, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [0.000, 6.000],  loss: 41.979446, accuracy: 0.796875, mean_q: 97.375679\n",
      " 2163/10000: episode: 1752, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.473705, accuracy: 0.875000, mean_q: 98.843971\n",
      " 2165/10000: episode: 1753, duration: 0.018s, episode steps:   2, steps per second: 109, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [2.000, 5.000],  loss: 3.259327, accuracy: 0.843750, mean_q: 100.867874\n",
      " 2166/10000: episode: 1754, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 20.552908, accuracy: 0.718750, mean_q: 100.304398\n",
      " 2167/10000: episode: 1755, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.317564, accuracy: 0.875000, mean_q: 101.610626\n",
      " 2168/10000: episode: 1756, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.447999, accuracy: 0.781250, mean_q: 100.066452\n",
      " 2169/10000: episode: 1757, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 14.276324, accuracy: 0.750000, mean_q: 100.152420\n",
      " 2170/10000: episode: 1758, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.432313, accuracy: 0.781250, mean_q: 99.310654\n",
      " 2171/10000: episode: 1759, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 105.069710, accuracy: 0.875000, mean_q: 99.522102\n",
      " 2172/10000: episode: 1760, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.130627, accuracy: 0.906250, mean_q: 100.361588\n",
      " 2173/10000: episode: 1761, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.859993, accuracy: 0.750000, mean_q: 100.431892\n",
      " 2174/10000: episode: 1762, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.307073, accuracy: 0.718750, mean_q: 100.578644\n",
      " 2175/10000: episode: 1763, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 18.012005, accuracy: 0.687500, mean_q: 100.381653\n",
      " 2176/10000: episode: 1764, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.448643, accuracy: 0.812500, mean_q: 99.770317\n",
      " 2177/10000: episode: 1765, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.149456, accuracy: 0.843750, mean_q: 99.953583\n",
      " 2178/10000: episode: 1766, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.021921, accuracy: 0.812500, mean_q: 100.666725\n",
      " 2180/10000: episode: 1767, duration: 0.018s, episode steps:   2, steps per second: 110, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [1.000, 6.000],  loss: 1.163874, accuracy: 0.718750, mean_q: 100.046921\n",
      " 2181/10000: episode: 1768, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.939758, accuracy: 0.750000, mean_q: 99.783150\n",
      " 2182/10000: episode: 1769, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.410947, accuracy: 0.875000, mean_q: 99.919617\n",
      " 2183/10000: episode: 1770, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.055754, accuracy: 0.937500, mean_q: 99.574226\n",
      " 2184/10000: episode: 1771, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.497648, accuracy: 0.843750, mean_q: 100.342453\n",
      " 2185/10000: episode: 1772, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.783597, accuracy: 0.875000, mean_q: 100.196259\n",
      " 2186/10000: episode: 1773, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 4.521753, accuracy: 0.687500, mean_q: 99.715599\n",
      " 2187/10000: episode: 1774, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.549964, accuracy: 0.812500, mean_q: 99.653000\n",
      " 2188/10000: episode: 1775, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.012011, accuracy: 0.718750, mean_q: 100.075317\n",
      " 2189/10000: episode: 1776, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 149.604401, accuracy: 0.875000, mean_q: 100.099182\n",
      " 2190/10000: episode: 1777, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.385658, accuracy: 0.718750, mean_q: 99.977516\n",
      " 2191/10000: episode: 1778, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.836600, accuracy: 0.812500, mean_q: 100.127449\n",
      " 2192/10000: episode: 1779, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 6.423130, accuracy: 0.812500, mean_q: 99.315651\n",
      " 2193/10000: episode: 1780, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 51.778179, accuracy: 0.781250, mean_q: 100.027878\n",
      " 2194/10000: episode: 1781, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 4.630066, accuracy: 0.843750, mean_q: 99.874313\n",
      " 2195/10000: episode: 1782, duration: 0.011s, episode steps:   1, steps per second:  91, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.203863, accuracy: 0.906250, mean_q: 99.924706\n",
      " 2196/10000: episode: 1783, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.761606, accuracy: 0.718750, mean_q: 100.051445\n",
      " 2197/10000: episode: 1784, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.798645, accuracy: 0.843750, mean_q: 100.872879\n",
      " 2198/10000: episode: 1785, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.284882, accuracy: 0.843750, mean_q: 100.361755\n",
      " 2199/10000: episode: 1786, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.861789, accuracy: 0.843750, mean_q: 99.715981\n",
      " 2200/10000: episode: 1787, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 4.406054, accuracy: 0.875000, mean_q: 99.666168\n",
      " 2201/10000: episode: 1788, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.600498, accuracy: 0.750000, mean_q: 99.825302\n",
      " 2203/10000: episode: 1789, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.000 [8.000, 10.000],  loss: 81.358337, accuracy: 0.750000, mean_q: 100.788712\n",
      " 2204/10000: episode: 1790, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.345979, accuracy: 0.812500, mean_q: 100.377510\n",
      " 2205/10000: episode: 1791, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.088716, accuracy: 0.875000, mean_q: 99.082710\n",
      " 2206/10000: episode: 1792, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.653257, accuracy: 0.875000, mean_q: 98.649666\n",
      " 2207/10000: episode: 1793, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.477585, accuracy: 0.843750, mean_q: 98.971252\n",
      " 2208/10000: episode: 1794, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.913596, accuracy: 0.875000, mean_q: 101.125534\n",
      " 2210/10000: episode: 1795, duration: 0.020s, episode steps:   2, steps per second: 100, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: 1.824169, accuracy: 0.812500, mean_q: 101.162689\n",
      " 2211/10000: episode: 1796, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.784257, accuracy: 0.750000, mean_q: 100.099152\n",
      " 2212/10000: episode: 1797, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.936947, accuracy: 0.750000, mean_q: 98.637299\n",
      " 2213/10000: episode: 1798, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.867486, accuracy: 0.875000, mean_q: 98.936691\n",
      " 2214/10000: episode: 1799, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.709184, accuracy: 0.843750, mean_q: 99.882309\n",
      " 2215/10000: episode: 1800, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.801858, accuracy: 0.843750, mean_q: 100.484047\n",
      " 2216/10000: episode: 1801, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.796646, accuracy: 0.968750, mean_q: 100.878403\n",
      " 2217/10000: episode: 1802, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.431821, accuracy: 0.781250, mean_q: 100.022835\n",
      " 2218/10000: episode: 1803, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.335303, accuracy: 0.781250, mean_q: 100.099609\n",
      " 2219/10000: episode: 1804, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.995724, accuracy: 0.906250, mean_q: 100.472748\n",
      " 2220/10000: episode: 1805, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 6.180120, accuracy: 0.843750, mean_q: 100.615051\n",
      " 2221/10000: episode: 1806, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.320727, accuracy: 0.875000, mean_q: 100.842682\n",
      " 2222/10000: episode: 1807, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.657804, accuracy: 0.781250, mean_q: 99.814178\n",
      " 2223/10000: episode: 1808, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.184247, accuracy: 0.875000, mean_q: 99.041801\n",
      " 2224/10000: episode: 1809, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 4.847951, accuracy: 0.843750, mean_q: 99.693245\n",
      " 2225/10000: episode: 1810, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.374865, accuracy: 0.781250, mean_q: 100.424438\n",
      " 2226/10000: episode: 1811, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.950648, accuracy: 0.812500, mean_q: 100.347328\n",
      " 2227/10000: episode: 1812, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.405994, accuracy: 0.843750, mean_q: 100.433907\n",
      " 2228/10000: episode: 1813, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.279915, accuracy: 0.687500, mean_q: 99.853699\n",
      " 2229/10000: episode: 1814, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.655139, accuracy: 0.750000, mean_q: 99.676476\n",
      " 2230/10000: episode: 1815, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.433794, accuracy: 0.750000, mean_q: 99.909195\n",
      " 2231/10000: episode: 1816, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.621996, accuracy: 0.781250, mean_q: 100.002853\n",
      " 2232/10000: episode: 1817, duration: 0.011s, episode steps:   1, steps per second:  92, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.338679, accuracy: 0.812500, mean_q: 100.043625\n",
      " 2233/10000: episode: 1818, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.615833, accuracy: 0.843750, mean_q: 99.728439\n",
      " 2234/10000: episode: 1819, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.826967, accuracy: 0.812500, mean_q: 99.252831\n",
      " 2235/10000: episode: 1820, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.641445, accuracy: 0.718750, mean_q: 99.557053\n",
      " 2237/10000: episode: 1821, duration: 0.019s, episode steps:   2, steps per second: 103, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.000 [9.000, 11.000],  loss: 4.005805, accuracy: 0.765625, mean_q: 99.968231\n",
      " 2238/10000: episode: 1822, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.923069, accuracy: 0.812500, mean_q: 100.127548\n",
      " 2239/10000: episode: 1823, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.309863, accuracy: 0.906250, mean_q: 99.837997\n",
      " 2240/10000: episode: 1824, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.972093, accuracy: 0.875000, mean_q: 99.969048\n",
      " 2241/10000: episode: 1825, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 5.258253, accuracy: 0.718750, mean_q: 99.785576\n",
      " 2243/10000: episode: 1826, duration: 0.018s, episode steps:   2, steps per second: 109, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 0.500 [0.000, 1.000],  loss: 1.762438, accuracy: 0.921875, mean_q: 99.272430\n",
      " 2244/10000: episode: 1827, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.769689, accuracy: 0.781250, mean_q: 99.573151\n",
      " 2246/10000: episode: 1828, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [1.000, 9.000],  loss: 77.742088, accuracy: 0.781250, mean_q: 99.936623\n",
      " 2247/10000: episode: 1829, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 4.343655, accuracy: 0.781250, mean_q: 99.586914\n",
      " 2248/10000: episode: 1830, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 7.396428, accuracy: 0.750000, mean_q: 98.887421\n",
      " 2249/10000: episode: 1831, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 4.586098, accuracy: 0.781250, mean_q: 100.499710\n",
      " 2250/10000: episode: 1832, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.151112, accuracy: 0.687500, mean_q: 100.974716\n",
      " 2251/10000: episode: 1833, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.168814, accuracy: 0.812500, mean_q: 100.269150\n",
      " 2252/10000: episode: 1834, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.950180, accuracy: 0.843750, mean_q: 99.399872\n",
      " 2253/10000: episode: 1835, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.171843, accuracy: 0.812500, mean_q: 99.107979\n",
      " 2254/10000: episode: 1836, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.414361, accuracy: 0.875000, mean_q: 99.942612\n",
      " 2255/10000: episode: 1837, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.817049, accuracy: 0.781250, mean_q: 100.529541\n",
      " 2256/10000: episode: 1838, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.327538, accuracy: 0.843750, mean_q: 100.751732\n",
      " 2257/10000: episode: 1839, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.369049, accuracy: 0.843750, mean_q: 100.080048\n",
      " 2258/10000: episode: 1840, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.712043, accuracy: 0.968750, mean_q: 100.154572\n",
      " 2260/10000: episode: 1841, duration: 0.020s, episode steps:   2, steps per second: 100, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [1.000, 4.000],  loss: 0.988921, accuracy: 0.812500, mean_q: 99.874672\n",
      " 2261/10000: episode: 1842, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.273571, accuracy: 0.687500, mean_q: 100.018585\n",
      " 2263/10000: episode: 1843, duration: 0.018s, episode steps:   2, steps per second: 109, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [0.000, 6.000],  loss: 1.043820, accuracy: 0.828125, mean_q: 100.130157\n",
      " 2264/10000: episode: 1844, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.788376, accuracy: 0.656250, mean_q: 100.274940\n",
      " 2265/10000: episode: 1845, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.270817, accuracy: 0.812500, mean_q: 99.930489\n",
      " 2266/10000: episode: 1846, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.046808, accuracy: 0.843750, mean_q: 99.588440\n",
      " 2267/10000: episode: 1847, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.804399, accuracy: 0.843750, mean_q: 99.917076\n",
      " 2269/10000: episode: 1848, duration: 0.018s, episode steps:   2, steps per second: 109, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [6.000, 11.000],  loss: 59.490490, accuracy: 0.812500, mean_q: 100.422455\n",
      " 2270/10000: episode: 1849, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.880286, accuracy: 0.937500, mean_q: 99.991287\n",
      " 2271/10000: episode: 1850, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 5.509577, accuracy: 0.843750, mean_q: 100.229729\n",
      " 2272/10000: episode: 1851, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.688946, accuracy: 0.875000, mean_q: 99.628868\n",
      " 2273/10000: episode: 1852, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.175633, accuracy: 0.812500, mean_q: 99.615295\n",
      " 2274/10000: episode: 1853, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.868292, accuracy: 0.718750, mean_q: 100.286720\n",
      " 2275/10000: episode: 1854, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.925405, accuracy: 0.937500, mean_q: 100.695633\n",
      " 2276/10000: episode: 1855, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.188816, accuracy: 0.875000, mean_q: 100.574989\n",
      " 2277/10000: episode: 1856, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.836402, accuracy: 0.781250, mean_q: 100.169853\n",
      " 2278/10000: episode: 1857, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 5.469887, accuracy: 0.843750, mean_q: 99.739372\n",
      " 2279/10000: episode: 1858, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.277793, accuracy: 0.812500, mean_q: 99.826523\n",
      " 2280/10000: episode: 1859, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.175038, accuracy: 0.781250, mean_q: 100.087173\n",
      " 2281/10000: episode: 1860, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.437768, accuracy: 0.750000, mean_q: 100.468002\n",
      " 2282/10000: episode: 1861, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.594962, accuracy: 0.687500, mean_q: 100.223999\n",
      " 2283/10000: episode: 1862, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.141830, accuracy: 0.906250, mean_q: 100.152718\n",
      " 2286/10000: episode: 1863, duration: 0.025s, episode steps:   3, steps per second: 122, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 7.333 [5.000, 11.000],  loss: 0.452838, accuracy: 0.802083, mean_q: 100.008629\n",
      " 2287/10000: episode: 1864, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.214345, accuracy: 0.843750, mean_q: 99.845871\n",
      " 2288/10000: episode: 1865, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.081056, accuracy: 0.781250, mean_q: 99.789764\n",
      " 2289/10000: episode: 1866, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.076361, accuracy: 0.843750, mean_q: 99.964844\n",
      " 2290/10000: episode: 1867, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.169531, accuracy: 0.843750, mean_q: 100.028847\n",
      " 2291/10000: episode: 1868, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.518384, accuracy: 0.843750, mean_q: 100.245384\n",
      " 2293/10000: episode: 1869, duration: 0.019s, episode steps:   2, steps per second: 107, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [2.000, 5.000],  loss: 0.165344, accuracy: 0.921875, mean_q: 99.938370\n",
      " 2294/10000: episode: 1870, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.172481, accuracy: 0.843750, mean_q: 99.829163\n",
      " 2295/10000: episode: 1871, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.789699, accuracy: 0.781250, mean_q: 99.635689\n",
      " 2296/10000: episode: 1872, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.791924, accuracy: 0.687500, mean_q: 99.917633\n",
      " 2297/10000: episode: 1873, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.271101, accuracy: 0.781250, mean_q: 100.032440\n",
      " 2298/10000: episode: 1874, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.356918, accuracy: 0.812500, mean_q: 100.031921\n",
      " 2299/10000: episode: 1875, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.229776, accuracy: 0.812500, mean_q: 99.813789\n",
      " 2300/10000: episode: 1876, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.753302, accuracy: 0.937500, mean_q: 100.182892\n",
      " 2301/10000: episode: 1877, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.052731, accuracy: 0.750000, mean_q: 99.788696\n",
      " 2302/10000: episode: 1878, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.800218, accuracy: 0.781250, mean_q: 99.282181\n",
      " 2303/10000: episode: 1879, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 22.119398, accuracy: 0.781250, mean_q: 99.643982\n",
      " 2304/10000: episode: 1880, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.667085, accuracy: 0.875000, mean_q: 99.875023\n",
      " 2305/10000: episode: 1881, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.416848, accuracy: 0.750000, mean_q: 100.113983\n",
      " 2306/10000: episode: 1882, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.620711, accuracy: 0.875000, mean_q: 100.135986\n",
      " 2307/10000: episode: 1883, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.242558, accuracy: 0.843750, mean_q: 100.072151\n",
      " 2308/10000: episode: 1884, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.510988, accuracy: 0.906250, mean_q: 99.714287\n",
      " 2309/10000: episode: 1885, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 149.937897, accuracy: 0.781250, mean_q: 100.263893\n",
      " 2310/10000: episode: 1886, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 20.163445, accuracy: 0.875000, mean_q: 99.449677\n",
      " 2311/10000: episode: 1887, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.406577, accuracy: 0.812500, mean_q: 99.131058\n",
      " 2312/10000: episode: 1888, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.901632, accuracy: 0.812500, mean_q: 100.572464\n",
      " 2313/10000: episode: 1889, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.022184, accuracy: 0.843750, mean_q: 99.703812\n",
      " 2314/10000: episode: 1890, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.058635, accuracy: 0.718750, mean_q: 99.524879\n",
      " 2315/10000: episode: 1891, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.929341, accuracy: 0.843750, mean_q: 100.208466\n",
      " 2316/10000: episode: 1892, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.874186, accuracy: 0.937500, mean_q: 100.128326\n",
      " 2317/10000: episode: 1893, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.213410, accuracy: 0.843750, mean_q: 99.881538\n",
      " 2318/10000: episode: 1894, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.788806, accuracy: 0.750000, mean_q: 100.230453\n",
      " 2319/10000: episode: 1895, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.195866, accuracy: 0.718750, mean_q: 100.053314\n",
      " 2320/10000: episode: 1896, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.802929, accuracy: 0.843750, mean_q: 100.167740\n",
      " 2321/10000: episode: 1897, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.286572, accuracy: 0.781250, mean_q: 99.960510\n",
      " 2322/10000: episode: 1898, duration: 0.047s, episode steps:   1, steps per second:  21, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 151.692657, accuracy: 0.843750, mean_q: 99.949631\n",
      " 2323/10000: episode: 1899, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.616568, accuracy: 0.812500, mean_q: 99.486359\n",
      " 2324/10000: episode: 1900, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.481702, accuracy: 0.781250, mean_q: 100.002983\n",
      " 2325/10000: episode: 1901, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.423376, accuracy: 0.812500, mean_q: 99.242340\n",
      " 2326/10000: episode: 1902, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.225269, accuracy: 0.843750, mean_q: 99.017319\n",
      " 2327/10000: episode: 1903, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.764039, accuracy: 0.843750, mean_q: 100.526001\n",
      " 2328/10000: episode: 1904, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.908814, accuracy: 0.875000, mean_q: 100.051117\n",
      " 2329/10000: episode: 1905, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.403821, accuracy: 0.937500, mean_q: 100.899063\n",
      " 2330/10000: episode: 1906, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.820307, accuracy: 0.812500, mean_q: 100.798088\n",
      " 2331/10000: episode: 1907, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.357435, accuracy: 0.843750, mean_q: 99.198212\n",
      " 2332/10000: episode: 1908, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 8.635125, accuracy: 0.812500, mean_q: 98.050858\n",
      " 2333/10000: episode: 1909, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.388770, accuracy: 0.812500, mean_q: 99.098106\n",
      " 2335/10000: episode: 1910, duration: 0.022s, episode steps:   2, steps per second:  91, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [0.000, 10.000],  loss: 2.884512, accuracy: 0.843750, mean_q: 100.159210\n",
      " 2336/10000: episode: 1911, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.973973, accuracy: 0.843750, mean_q: 100.051102\n",
      " 2337/10000: episode: 1912, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.596372, accuracy: 0.843750, mean_q: 101.244926\n",
      " 2340/10000: episode: 1913, duration: 0.025s, episode steps:   3, steps per second: 118, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 7.333 [5.000, 10.000],  loss: 2.906253, accuracy: 0.833333, mean_q: 100.720764\n",
      " 2341/10000: episode: 1914, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 156.077957, accuracy: 0.656250, mean_q: 100.392410\n",
      " 2343/10000: episode: 1915, duration: 0.018s, episode steps:   2, steps per second: 113, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [6.000, 7.000],  loss: 3.883913, accuracy: 0.796875, mean_q: 99.471100\n",
      " 2344/10000: episode: 1916, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.414793, accuracy: 0.812500, mean_q: 99.793625\n",
      " 2345/10000: episode: 1917, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.389395, accuracy: 0.843750, mean_q: 99.704788\n",
      " 2346/10000: episode: 1918, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.820578, accuracy: 0.812500, mean_q: 100.295929\n",
      " 2347/10000: episode: 1919, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.093076, accuracy: 0.593750, mean_q: 100.536949\n",
      " 2348/10000: episode: 1920, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.565551, accuracy: 0.750000, mean_q: 100.070137\n",
      " 2349/10000: episode: 1921, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.527369, accuracy: 0.812500, mean_q: 100.345879\n",
      " 2350/10000: episode: 1922, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.391753, accuracy: 0.781250, mean_q: 100.208519\n",
      " 2351/10000: episode: 1923, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.593082, accuracy: 0.875000, mean_q: 100.353111\n",
      " 2352/10000: episode: 1924, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.299208, accuracy: 0.843750, mean_q: 100.278481\n",
      " 2353/10000: episode: 1925, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.324872, accuracy: 0.750000, mean_q: 99.968582\n",
      " 2354/10000: episode: 1926, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.016556, accuracy: 0.812500, mean_q: 99.599426\n",
      " 2355/10000: episode: 1927, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.242926, accuracy: 0.718750, mean_q: 99.445473\n",
      " 2356/10000: episode: 1928, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.398655, accuracy: 0.781250, mean_q: 100.010475\n",
      " 2357/10000: episode: 1929, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.273664, accuracy: 0.781250, mean_q: 100.706924\n",
      " 2358/10000: episode: 1930, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.453437, accuracy: 0.718750, mean_q: 100.314148\n",
      " 2359/10000: episode: 1931, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.424761, accuracy: 0.875000, mean_q: 100.003311\n",
      " 2360/10000: episode: 1932, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.938296, accuracy: 0.937500, mean_q: 99.660492\n",
      " 2361/10000: episode: 1933, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.169654, accuracy: 0.875000, mean_q: 99.709671\n",
      " 2362/10000: episode: 1934, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.876288, accuracy: 0.875000, mean_q: 100.281097\n",
      " 2363/10000: episode: 1935, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 131.408875, accuracy: 0.843750, mean_q: 100.463287\n",
      " 2364/10000: episode: 1936, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.812250, accuracy: 0.781250, mean_q: 99.086121\n",
      " 2365/10000: episode: 1937, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.122280, accuracy: 0.906250, mean_q: 98.449310\n",
      " 2366/10000: episode: 1938, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.430152, accuracy: 0.781250, mean_q: 99.109138\n",
      " 2369/10000: episode: 1939, duration: 0.025s, episode steps:   3, steps per second: 118, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 4.333 [1.000, 7.000],  loss: 2.135050, accuracy: 0.791667, mean_q: 100.040565\n",
      " 2370/10000: episode: 1940, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.131339, accuracy: 0.843750, mean_q: 100.363235\n",
      " 2373/10000: episode: 1941, duration: 0.026s, episode steps:   3, steps per second: 117, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 5.000 [2.000, 10.000],  loss: 1.658603, accuracy: 0.833333, mean_q: 100.270821\n",
      " 2374/10000: episode: 1942, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 139.350479, accuracy: 0.718750, mean_q: 99.968437\n",
      " 2375/10000: episode: 1943, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.835713, accuracy: 0.750000, mean_q: 99.860535\n",
      " 2376/10000: episode: 1944, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.217063, accuracy: 0.937500, mean_q: 99.910507\n",
      " 2377/10000: episode: 1945, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.982467, accuracy: 0.875000, mean_q: 99.982407\n",
      " 2378/10000: episode: 1946, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.295263, accuracy: 0.718750, mean_q: 100.465683\n",
      " 2379/10000: episode: 1947, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.596019, accuracy: 0.875000, mean_q: 100.048538\n",
      " 2380/10000: episode: 1948, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 5.622762, accuracy: 0.687500, mean_q: 99.924484\n",
      " 2381/10000: episode: 1949, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.670785, accuracy: 0.718750, mean_q: 99.950935\n",
      " 2382/10000: episode: 1950, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.801350, accuracy: 0.750000, mean_q: 100.499176\n",
      " 2383/10000: episode: 1951, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.148231, accuracy: 0.656250, mean_q: 100.093163\n",
      " 2384/10000: episode: 1952, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 4.592318, accuracy: 0.812500, mean_q: 99.505203\n",
      " 2385/10000: episode: 1953, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.985635, accuracy: 0.843750, mean_q: 99.959747\n",
      " 2386/10000: episode: 1954, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.579829, accuracy: 0.812500, mean_q: 100.506119\n",
      " 2387/10000: episode: 1955, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.585199, accuracy: 0.781250, mean_q: 100.707832\n",
      " 2388/10000: episode: 1956, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.257864, accuracy: 0.750000, mean_q: 100.137589\n",
      " 2389/10000: episode: 1957, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.584198, accuracy: 0.750000, mean_q: 99.929512\n",
      " 2390/10000: episode: 1958, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 54.165596, accuracy: 0.718750, mean_q: 99.711105\n",
      " 2391/10000: episode: 1959, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.956041, accuracy: 0.750000, mean_q: 99.609482\n",
      " 2392/10000: episode: 1960, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.589297, accuracy: 0.781250, mean_q: 99.771759\n",
      " 2393/10000: episode: 1961, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.623609, accuracy: 1.000000, mean_q: 99.833252\n",
      " 2394/10000: episode: 1962, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.561443, accuracy: 0.812500, mean_q: 99.758682\n",
      " 2395/10000: episode: 1963, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.575707, accuracy: 0.906250, mean_q: 100.218300\n",
      " 2396/10000: episode: 1964, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.961173, accuracy: 0.875000, mean_q: 100.261276\n",
      " 2397/10000: episode: 1965, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.668201, accuracy: 0.718750, mean_q: 99.757187\n",
      " 2398/10000: episode: 1966, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.352186, accuracy: 0.781250, mean_q: 99.743256\n",
      " 2399/10000: episode: 1967, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.360693, accuracy: 0.718750, mean_q: 99.924675\n",
      " 2400/10000: episode: 1968, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 151.808258, accuracy: 0.781250, mean_q: 100.070496\n",
      " 2401/10000: episode: 1969, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 140.929901, accuracy: 0.750000, mean_q: 99.449585\n",
      " 2402/10000: episode: 1970, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 5.876233, accuracy: 0.781250, mean_q: 98.582321\n",
      " 2403/10000: episode: 1971, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.436553, accuracy: 0.843750, mean_q: 99.317657\n",
      " 2404/10000: episode: 1972, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.956234, accuracy: 0.781250, mean_q: 99.492950\n",
      " 2405/10000: episode: 1973, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.818231, accuracy: 0.906250, mean_q: 99.289841\n",
      " 2406/10000: episode: 1974, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.441590, accuracy: 0.656250, mean_q: 100.499313\n",
      " 2407/10000: episode: 1975, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.488610, accuracy: 0.781250, mean_q: 100.765228\n",
      " 2408/10000: episode: 1976, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.078311, accuracy: 0.718750, mean_q: 100.756210\n",
      " 2409/10000: episode: 1977, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.439873, accuracy: 0.875000, mean_q: 100.801056\n",
      " 2411/10000: episode: 1978, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [1.000, 10.000],  loss: 18.134491, accuracy: 0.781250, mean_q: 100.536774\n",
      " 2412/10000: episode: 1979, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 112.460884, accuracy: 0.718750, mean_q: 100.256668\n",
      " 2413/10000: episode: 1980, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.338439, accuracy: 0.812500, mean_q: 99.603928\n",
      " 2415/10000: episode: 1981, duration: 0.019s, episode steps:   2, steps per second: 103, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [0.000, 11.000],  loss: 2.452956, accuracy: 0.906250, mean_q: 99.881851\n",
      " 2416/10000: episode: 1982, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 7.120154, accuracy: 0.812500, mean_q: 99.786217\n",
      " 2417/10000: episode: 1983, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.492942, accuracy: 0.750000, mean_q: 100.468353\n",
      " 2418/10000: episode: 1984, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.653897, accuracy: 0.750000, mean_q: 100.115295\n",
      " 2419/10000: episode: 1985, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.145327, accuracy: 0.812500, mean_q: 100.902946\n",
      " 2420/10000: episode: 1986, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.716638, accuracy: 0.843750, mean_q: 101.198593\n",
      " 2421/10000: episode: 1987, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 5.851296, accuracy: 0.843750, mean_q: 100.126434\n",
      " 2422/10000: episode: 1988, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.033638, accuracy: 0.843750, mean_q: 99.365875\n",
      " 2423/10000: episode: 1989, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 4.613925, accuracy: 0.812500, mean_q: 99.578255\n",
      " 2424/10000: episode: 1990, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.487632, accuracy: 0.812500, mean_q: 99.948227\n",
      " 2425/10000: episode: 1991, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.779365, accuracy: 0.750000, mean_q: 100.398186\n",
      " 2426/10000: episode: 1992, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.358321, accuracy: 0.656250, mean_q: 100.361267\n",
      " 2427/10000: episode: 1993, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.182074, accuracy: 0.937500, mean_q: 99.913010\n",
      " 2428/10000: episode: 1994, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.497733, accuracy: 0.812500, mean_q: 99.985809\n",
      " 2430/10000: episode: 1995, duration: 0.018s, episode steps:   2, steps per second: 113, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.500 [6.000, 11.000],  loss: 62.047249, accuracy: 0.812500, mean_q: 99.950668\n",
      " 2431/10000: episode: 1996, duration: 0.011s, episode steps:   1, steps per second:  90, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.492388, accuracy: 0.937500, mean_q: 99.091827\n",
      " 2432/10000: episode: 1997, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.323531, accuracy: 0.812500, mean_q: 99.312042\n",
      " 2433/10000: episode: 1998, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 4.389168, accuracy: 0.781250, mean_q: 99.693359\n",
      " 2435/10000: episode: 1999, duration: 0.019s, episode steps:   2, steps per second: 104, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [6.000, 10.000],  loss: 33.468510, accuracy: 0.843750, mean_q: 99.920319\n",
      " 2436/10000: episode: 2000, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.474208, accuracy: 0.843750, mean_q: 99.973434\n",
      " 2437/10000: episode: 2001, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.069925, accuracy: 0.843750, mean_q: 100.020889\n",
      " 2438/10000: episode: 2002, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.524921, accuracy: 0.781250, mean_q: 100.130341\n",
      " 2439/10000: episode: 2003, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 9.141479, accuracy: 0.718750, mean_q: 100.310791\n",
      " 2440/10000: episode: 2004, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.873172, accuracy: 0.812500, mean_q: 100.430473\n",
      " 2441/10000: episode: 2005, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.715167, accuracy: 0.937500, mean_q: 100.654503\n",
      " 2442/10000: episode: 2006, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.407482, accuracy: 0.843750, mean_q: 100.125931\n",
      " 2443/10000: episode: 2007, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 32.027771, accuracy: 0.781250, mean_q: 100.270401\n",
      " 2444/10000: episode: 2008, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 52.507458, accuracy: 0.687500, mean_q: 100.235214\n",
      " 2445/10000: episode: 2009, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 148.840714, accuracy: 0.843750, mean_q: 99.900085\n",
      " 2446/10000: episode: 2010, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.436158, accuracy: 0.875000, mean_q: 99.148712\n",
      " 2447/10000: episode: 2011, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 33.619373, accuracy: 0.843750, mean_q: 99.584381\n",
      " 2448/10000: episode: 2012, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.169945, accuracy: 0.750000, mean_q: 100.239395\n",
      " 2449/10000: episode: 2013, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.194541, accuracy: 0.781250, mean_q: 100.177597\n",
      " 2451/10000: episode: 2014, duration: 0.019s, episode steps:   2, steps per second: 107, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [9.000, 10.000],  loss: 2.685798, accuracy: 0.828125, mean_q: 100.064514\n",
      " 2452/10000: episode: 2015, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.661981, accuracy: 0.875000, mean_q: 99.965714\n",
      " 2453/10000: episode: 2016, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.161752, accuracy: 0.843750, mean_q: 100.625961\n",
      " 2454/10000: episode: 2017, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.890225, accuracy: 0.812500, mean_q: 100.236664\n",
      " 2455/10000: episode: 2018, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.871668, accuracy: 0.718750, mean_q: 100.331757\n",
      " 2457/10000: episode: 2019, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: 0.617942, accuracy: 0.875000, mean_q: 99.719482\n",
      " 2458/10000: episode: 2020, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.275610, accuracy: 0.875000, mean_q: 100.050629\n",
      " 2459/10000: episode: 2021, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.274890, accuracy: 0.875000, mean_q: 100.283813\n",
      " 2460/10000: episode: 2022, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.765580, accuracy: 0.875000, mean_q: 100.399765\n",
      " 2462/10000: episode: 2023, duration: 0.018s, episode steps:   2, steps per second: 112, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: 0.485455, accuracy: 0.875000, mean_q: 100.114044\n",
      " 2463/10000: episode: 2024, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.303557, accuracy: 0.781250, mean_q: 99.963959\n",
      " 2464/10000: episode: 2025, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.345052, accuracy: 0.906250, mean_q: 100.095459\n",
      " 2465/10000: episode: 2026, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.527977, accuracy: 0.750000, mean_q: 100.267288\n",
      " 2467/10000: episode: 2027, duration: 0.020s, episode steps:   2, steps per second:  98, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.000 [1.000, 5.000],  loss: 0.231281, accuracy: 0.812500, mean_q: 99.858566\n",
      " 2468/10000: episode: 2028, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.257103, accuracy: 0.812500, mean_q: 99.433914\n",
      " 2469/10000: episode: 2029, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.780271, accuracy: 0.656250, mean_q: 99.447586\n",
      " 2471/10000: episode: 2030, duration: 0.022s, episode steps:   2, steps per second:  91, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [1.000, 4.000],  loss: 2.210864, accuracy: 0.843750, mean_q: 99.395187\n",
      " 2472/10000: episode: 2031, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.734499, accuracy: 0.875000, mean_q: 99.976547\n",
      " 2473/10000: episode: 2032, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.746357, accuracy: 0.843750, mean_q: 99.392395\n",
      " 2474/10000: episode: 2033, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 134.623764, accuracy: 0.812500, mean_q: 99.349228\n",
      " 2475/10000: episode: 2034, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.050962, accuracy: 0.906250, mean_q: 99.410614\n",
      " 2476/10000: episode: 2035, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.127558, accuracy: 0.843750, mean_q: 99.538948\n",
      " 2477/10000: episode: 2036, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.808798, accuracy: 0.812500, mean_q: 99.914650\n",
      " 2478/10000: episode: 2037, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.914696, accuracy: 0.781250, mean_q: 101.163582\n",
      " 2479/10000: episode: 2038, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.243872, accuracy: 0.843750, mean_q: 100.501534\n",
      " 2480/10000: episode: 2039, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.537935, accuracy: 0.718750, mean_q: 99.798592\n",
      " 2482/10000: episode: 2040, duration: 0.019s, episode steps:   2, steps per second: 106, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [1.000, 2.000],  loss: 2.443903, accuracy: 0.843750, mean_q: 100.152084\n",
      " 2483/10000: episode: 2041, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.134009, accuracy: 0.906250, mean_q: 100.276352\n",
      " 2484/10000: episode: 2042, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 7.514942, accuracy: 0.718750, mean_q: 100.485710\n",
      " 2485/10000: episode: 2043, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.278096, accuracy: 0.906250, mean_q: 100.126907\n",
      " 2486/10000: episode: 2044, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.443273, accuracy: 0.843750, mean_q: 100.395462\n",
      " 2487/10000: episode: 2045, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.177648, accuracy: 0.812500, mean_q: 99.667603\n",
      " 2488/10000: episode: 2046, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.312333, accuracy: 0.781250, mean_q: 100.007568\n",
      " 2489/10000: episode: 2047, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.899958, accuracy: 0.875000, mean_q: 100.079079\n",
      " 2490/10000: episode: 2048, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.694028, accuracy: 0.906250, mean_q: 100.071106\n",
      " 2491/10000: episode: 2049, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.125337, accuracy: 0.875000, mean_q: 99.814903\n",
      " 2492/10000: episode: 2050, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.708601, accuracy: 0.750000, mean_q: 99.968216\n",
      " 2493/10000: episode: 2051, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.300914, accuracy: 0.843750, mean_q: 100.230331\n",
      " 2494/10000: episode: 2052, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.249201, accuracy: 0.843750, mean_q: 100.130875\n",
      " 2495/10000: episode: 2053, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.349789, accuracy: 0.843750, mean_q: 100.149437\n",
      " 2496/10000: episode: 2054, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.680866, accuracy: 0.750000, mean_q: 99.813629\n",
      " 2497/10000: episode: 2055, duration: 0.065s, episode steps:   1, steps per second:  15, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.463483, accuracy: 0.875000, mean_q: 99.670662\n",
      " 2498/10000: episode: 2056, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.797176, accuracy: 0.843750, mean_q: 99.758720\n",
      " 2499/10000: episode: 2057, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.512532, accuracy: 0.875000, mean_q: 100.226654\n",
      " 2500/10000: episode: 2058, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.271034, accuracy: 0.875000, mean_q: 100.006058\n",
      " 2501/10000: episode: 2059, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 23.189850, accuracy: 0.906250, mean_q: 99.848839\n",
      " 2502/10000: episode: 2060, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.419112, accuracy: 0.875000, mean_q: 100.242439\n",
      " 2504/10000: episode: 2061, duration: 0.017s, episode steps:   2, steps per second: 114, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [3.000, 11.000],  loss: 0.697610, accuracy: 0.796875, mean_q: 100.325081\n",
      " 2505/10000: episode: 2062, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.387320, accuracy: 0.812500, mean_q: 100.312897\n",
      " 2506/10000: episode: 2063, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.472919, accuracy: 0.781250, mean_q: 100.010887\n",
      " 2507/10000: episode: 2064, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.444132, accuracy: 0.875000, mean_q: 99.917290\n",
      " 2508/10000: episode: 2065, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.133948, accuracy: 0.906250, mean_q: 100.085968\n",
      " 2509/10000: episode: 2066, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.891670, accuracy: 0.781250, mean_q: 100.099350\n",
      " 2510/10000: episode: 2067, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 150.378357, accuracy: 0.750000, mean_q: 99.926117\n",
      " 2511/10000: episode: 2068, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.943861, accuracy: 0.718750, mean_q: 99.095596\n",
      " 2512/10000: episode: 2069, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 129.490646, accuracy: 0.875000, mean_q: 98.955948\n",
      " 2513/10000: episode: 2070, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.850070, accuracy: 0.843750, mean_q: 99.203964\n",
      " 2514/10000: episode: 2071, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.695828, accuracy: 0.750000, mean_q: 100.576752\n",
      " 2515/10000: episode: 2072, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.206536, accuracy: 0.906250, mean_q: 99.958763\n",
      " 2516/10000: episode: 2073, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 7.240492, accuracy: 0.875000, mean_q: 99.548538\n",
      " 2517/10000: episode: 2074, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 4.028641, accuracy: 0.875000, mean_q: 99.237259\n",
      " 2518/10000: episode: 2075, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.804677, accuracy: 0.781250, mean_q: 99.988701\n",
      " 2519/10000: episode: 2076, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.356366, accuracy: 0.750000, mean_q: 100.892952\n",
      " 2520/10000: episode: 2077, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 8.298726, accuracy: 0.906250, mean_q: 101.422089\n",
      " 2521/10000: episode: 2078, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.871523, accuracy: 0.906250, mean_q: 99.462318\n",
      " 2522/10000: episode: 2079, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.687989, accuracy: 0.875000, mean_q: 99.215912\n",
      " 2523/10000: episode: 2080, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 5.735481, accuracy: 0.718750, mean_q: 99.524437\n",
      " 2524/10000: episode: 2081, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.286312, accuracy: 0.843750, mean_q: 100.435257\n",
      " 2525/10000: episode: 2082, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.303534, accuracy: 0.781250, mean_q: 101.831917\n",
      " 2526/10000: episode: 2083, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 4.673572, accuracy: 0.750000, mean_q: 99.789505\n",
      " 2527/10000: episode: 2084, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.012525, accuracy: 0.781250, mean_q: 99.123520\n",
      " 2528/10000: episode: 2085, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.434679, accuracy: 0.781250, mean_q: 99.078514\n",
      " 2529/10000: episode: 2086, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.160283, accuracy: 0.875000, mean_q: 99.810089\n",
      " 2530/10000: episode: 2087, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 143.938339, accuracy: 0.750000, mean_q: 100.811913\n",
      " 2532/10000: episode: 2088, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 35.589252, accuracy: 0.890625, mean_q: 100.065796\n",
      " 2533/10000: episode: 2089, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 5.420697, accuracy: 0.812500, mean_q: 99.529602\n",
      " 2535/10000: episode: 2090, duration: 0.018s, episode steps:   2, steps per second: 109, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: 1.834298, accuracy: 0.843750, mean_q: 99.949242\n",
      " 2536/10000: episode: 2091, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.680720, accuracy: 0.875000, mean_q: 100.315384\n",
      " 2537/10000: episode: 2092, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.880177, accuracy: 0.750000, mean_q: 100.418350\n",
      " 2538/10000: episode: 2093, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 4.525158, accuracy: 0.843750, mean_q: 99.719070\n",
      " 2539/10000: episode: 2094, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.925860, accuracy: 0.812500, mean_q: 99.235390\n",
      " 2540/10000: episode: 2095, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.593749, accuracy: 0.656250, mean_q: 100.197105\n",
      " 2541/10000: episode: 2096, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.335116, accuracy: 0.843750, mean_q: 100.200775\n",
      " 2542/10000: episode: 2097, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.124634, accuracy: 0.812500, mean_q: 100.208008\n",
      " 2543/10000: episode: 2098, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.942612, accuracy: 0.875000, mean_q: 99.745949\n",
      " 2544/10000: episode: 2099, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.512819, accuracy: 0.812500, mean_q: 99.668213\n",
      " 2545/10000: episode: 2100, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.848849, accuracy: 0.781250, mean_q: 99.847610\n",
      " 2546/10000: episode: 2101, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 128.243149, accuracy: 0.781250, mean_q: 100.531662\n",
      " 2547/10000: episode: 2102, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.329834, accuracy: 0.906250, mean_q: 99.770096\n",
      " 2548/10000: episode: 2103, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.927653, accuracy: 0.843750, mean_q: 99.700333\n",
      " 2549/10000: episode: 2104, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.789803, accuracy: 0.781250, mean_q: 100.017685\n",
      " 2550/10000: episode: 2105, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.903938, accuracy: 0.875000, mean_q: 99.754463\n",
      " 2551/10000: episode: 2106, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.838113, accuracy: 0.781250, mean_q: 99.559319\n",
      " 2552/10000: episode: 2107, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.503175, accuracy: 0.812500, mean_q: 99.957657\n",
      " 2553/10000: episode: 2108, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.232821, accuracy: 0.875000, mean_q: 100.364624\n",
      " 2554/10000: episode: 2109, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 153.209473, accuracy: 0.812500, mean_q: 100.245621\n",
      " 2555/10000: episode: 2110, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.089503, accuracy: 0.750000, mean_q: 99.671005\n",
      " 2556/10000: episode: 2111, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.699235, accuracy: 0.843750, mean_q: 99.240784\n",
      " 2557/10000: episode: 2112, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.637427, accuracy: 0.875000, mean_q: 99.589363\n",
      " 2558/10000: episode: 2113, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.267522, accuracy: 0.906250, mean_q: 100.257790\n",
      " 2559/10000: episode: 2114, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.684077, accuracy: 0.718750, mean_q: 100.371994\n",
      " 2560/10000: episode: 2115, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.477356, accuracy: 0.875000, mean_q: 99.855103\n",
      " 2561/10000: episode: 2116, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.627856, accuracy: 0.812500, mean_q: 99.471840\n",
      " 2562/10000: episode: 2117, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.357381, accuracy: 0.875000, mean_q: 99.770119\n",
      " 2563/10000: episode: 2118, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.703948, accuracy: 0.843750, mean_q: 100.403320\n",
      " 2564/10000: episode: 2119, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 3.157174, accuracy: 0.875000, mean_q: 100.246101\n",
      " 2565/10000: episode: 2120, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.588178, accuracy: 0.843750, mean_q: 99.937775\n",
      " 2566/10000: episode: 2121, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.532091, accuracy: 0.812500, mean_q: 99.983925\n",
      " 2567/10000: episode: 2122, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.374413, accuracy: 0.875000, mean_q: 99.780869\n",
      " 2568/10000: episode: 2123, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.780602, accuracy: 0.843750, mean_q: 99.987137\n",
      " 2569/10000: episode: 2124, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.223265, accuracy: 0.843750, mean_q: 100.139862\n",
      " 2570/10000: episode: 2125, duration: 0.012s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.462727, accuracy: 0.843750, mean_q: 100.196655\n",
      " 2571/10000: episode: 2126, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.456594, accuracy: 0.906250, mean_q: 100.463577\n",
      " 2572/10000: episode: 2127, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.811884, accuracy: 0.750000, mean_q: 99.910103\n",
      " 2573/10000: episode: 2128, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.238943, accuracy: 0.875000, mean_q: 99.983383\n",
      " 2574/10000: episode: 2129, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 5.765268, accuracy: 0.718750, mean_q: 100.053627\n",
      " 2575/10000: episode: 2130, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.247057, accuracy: 0.812500, mean_q: 99.781860\n",
      " 2576/10000: episode: 2131, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.992571, accuracy: 0.812500, mean_q: 99.331406\n",
      " 2577/10000: episode: 2132, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.062106, accuracy: 0.875000, mean_q: 98.239891\n",
      " 2578/10000: episode: 2133, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.910845, accuracy: 0.812500, mean_q: 99.657242\n",
      " 2579/10000: episode: 2134, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.558116, accuracy: 0.812500, mean_q: 100.902336\n",
      " 2580/10000: episode: 2135, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.143854, accuracy: 0.906250, mean_q: 100.747864\n",
      " 2581/10000: episode: 2136, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.708198, accuracy: 0.906250, mean_q: 99.620422\n",
      " 2582/10000: episode: 2137, duration: 0.011s, episode steps:   1, steps per second:  88, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.717751, accuracy: 0.843750, mean_q: 99.673973\n",
      " 2583/10000: episode: 2138, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.934345, accuracy: 0.875000, mean_q: 99.855087\n",
      " 2584/10000: episode: 2139, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.258454, accuracy: 0.906250, mean_q: 99.424438\n",
      " 2585/10000: episode: 2140, duration: 0.012s, episode steps:   1, steps per second:  86, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.918859, accuracy: 0.812500, mean_q: 100.350815\n",
      " 2586/10000: episode: 2141, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.547352, accuracy: 0.906250, mean_q: 100.144287\n",
      " 2587/10000: episode: 2142, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.799462, accuracy: 0.875000, mean_q: 99.639297\n",
      " 2588/10000: episode: 2143, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.883782, accuracy: 0.718750, mean_q: 99.910454\n",
      " 2589/10000: episode: 2144, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.698787, accuracy: 0.781250, mean_q: 100.553604\n",
      " 2590/10000: episode: 2145, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.041254, accuracy: 0.781250, mean_q: 100.606323\n",
      " 2591/10000: episode: 2146, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.925990, accuracy: 0.843750, mean_q: 100.527618\n",
      " 2592/10000: episode: 2147, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 4.720039, accuracy: 0.812500, mean_q: 100.497086\n",
      " 2593/10000: episode: 2148, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 4.562516, accuracy: 0.875000, mean_q: 99.756577\n",
      " 2594/10000: episode: 2149, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.550153, accuracy: 0.875000, mean_q: 99.705185\n",
      " 2595/10000: episode: 2150, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.610993, accuracy: 0.750000, mean_q: 100.065399\n",
      " 2596/10000: episode: 2151, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.881253, accuracy: 0.750000, mean_q: 99.923080\n",
      " 2597/10000: episode: 2152, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.037878, accuracy: 0.875000, mean_q: 99.835052\n",
      " 2600/10000: episode: 2153, duration: 0.023s, episode steps:   3, steps per second: 128, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 6.667 [3.000, 10.000],  loss: 0.650757, accuracy: 0.854167, mean_q: 100.242210\n",
      " 2602/10000: episode: 2154, duration: 0.018s, episode steps:   2, steps per second: 110, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.500 [9.000, 10.000],  loss: 3.673962, accuracy: 0.765625, mean_q: 99.938385\n",
      " 2603/10000: episode: 2155, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.449075, accuracy: 0.875000, mean_q: 99.677177\n",
      " 2604/10000: episode: 2156, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.744575, accuracy: 0.875000, mean_q: 100.092743\n",
      " 2605/10000: episode: 2157, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 4.326424, accuracy: 0.812500, mean_q: 100.698875\n",
      " 2606/10000: episode: 2158, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 56.295940, accuracy: 0.812500, mean_q: 100.193375\n",
      " 2607/10000: episode: 2159, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.892490, accuracy: 0.812500, mean_q: 99.709541\n",
      " 2608/10000: episode: 2160, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.620020, accuracy: 0.843750, mean_q: 99.533897\n",
      " 2609/10000: episode: 2161, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.494656, accuracy: 0.812500, mean_q: 99.843521\n",
      " 2610/10000: episode: 2162, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 3.290709, accuracy: 0.781250, mean_q: 99.964653\n",
      " 2611/10000: episode: 2163, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.314468, accuracy: 0.812500, mean_q: 100.226334\n",
      " 2612/10000: episode: 2164, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 4.330961, accuracy: 0.812500, mean_q: 100.094208\n",
      " 2614/10000: episode: 2165, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [2.000, 10.000],  loss: 75.173721, accuracy: 0.828125, mean_q: 100.259903\n",
      " 2616/10000: episode: 2166, duration: 0.018s, episode steps:   2, steps per second: 112, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [4.000, 7.000],  loss: 1.864874, accuracy: 0.859375, mean_q: 98.978233\n",
      " 2617/10000: episode: 2167, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.919482, accuracy: 0.750000, mean_q: 99.487053\n",
      " 2618/10000: episode: 2168, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.989711, accuracy: 0.875000, mean_q: 100.526108\n",
      " 2620/10000: episode: 2169, duration: 0.018s, episode steps:   2, steps per second: 111, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [0.000, 5.000],  loss: 1.071652, accuracy: 0.843750, mean_q: 100.437988\n",
      " 2621/10000: episode: 2170, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.410329, accuracy: 0.875000, mean_q: 99.759209\n",
      " 2622/10000: episode: 2171, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.772484, accuracy: 0.781250, mean_q: 99.670677\n",
      " 2623/10000: episode: 2172, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.110891, accuracy: 0.843750, mean_q: 99.834534\n",
      " 2624/10000: episode: 2173, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.914502, accuracy: 0.750000, mean_q: 100.514763\n",
      " 2625/10000: episode: 2174, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 47.121025, accuracy: 0.843750, mean_q: 100.593460\n",
      " 2626/10000: episode: 2175, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.980894, accuracy: 0.843750, mean_q: 99.666481\n",
      " 2627/10000: episode: 2176, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.884957, accuracy: 0.781250, mean_q: 99.290695\n",
      " 2628/10000: episode: 2177, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.980673, accuracy: 0.843750, mean_q: 99.709923\n",
      " 2629/10000: episode: 2178, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.751807, accuracy: 0.875000, mean_q: 100.521286\n",
      " 2630/10000: episode: 2179, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.452846, accuracy: 0.812500, mean_q: 100.551079\n",
      " 2632/10000: episode: 2180, duration: 0.020s, episode steps:   2, steps per second:  98, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.000 [0.000, 4.000],  loss: 0.721544, accuracy: 0.875000, mean_q: 100.176025\n",
      " 2633/10000: episode: 2181, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.899939, accuracy: 0.781250, mean_q: 99.882706\n",
      " 2634/10000: episode: 2182, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.592726, accuracy: 0.812500, mean_q: 99.753304\n",
      " 2635/10000: episode: 2183, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.524433, accuracy: 0.843750, mean_q: 99.550186\n",
      " 2636/10000: episode: 2184, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.213939, accuracy: 0.718750, mean_q: 99.945518\n",
      " 2637/10000: episode: 2185, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.547640, accuracy: 0.718750, mean_q: 100.060539\n",
      " 2638/10000: episode: 2186, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 148.596497, accuracy: 0.750000, mean_q: 100.066788\n",
      " 2640/10000: episode: 2187, duration: 0.023s, episode steps:   2, steps per second:  88, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 0.625134, accuracy: 0.906250, mean_q: 99.638130\n",
      " 2641/10000: episode: 2188, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.817136, accuracy: 0.687500, mean_q: 99.380722\n",
      " 2642/10000: episode: 2189, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.265113, accuracy: 0.812500, mean_q: 99.751846\n",
      " 2643/10000: episode: 2190, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.802611, accuracy: 0.781250, mean_q: 100.435654\n",
      " 2644/10000: episode: 2191, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.168985, accuracy: 0.843750, mean_q: 100.358505\n",
      " 2645/10000: episode: 2192, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.979303, accuracy: 0.843750, mean_q: 100.583649\n",
      " 2646/10000: episode: 2193, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.792477, accuracy: 0.906250, mean_q: 100.013794\n",
      " 2647/10000: episode: 2194, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.937311, accuracy: 0.812500, mean_q: 99.525543\n",
      " 2649/10000: episode: 2195, duration: 0.020s, episode steps:   2, steps per second: 101, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: 0.362532, accuracy: 0.828125, mean_q: 100.005295\n",
      " 2651/10000: episode: 2196, duration: 0.020s, episode steps:   2, steps per second: 100, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 3.500 [2.000, 5.000],  loss: 0.536147, accuracy: 0.765625, mean_q: 100.421844\n",
      " 2652/10000: episode: 2197, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.414112, accuracy: 0.906250, mean_q: 99.812035\n",
      " 2653/10000: episode: 2198, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.364415, accuracy: 0.781250, mean_q: 99.549591\n",
      " 2654/10000: episode: 2199, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.119633, accuracy: 0.875000, mean_q: 100.024323\n",
      " 2655/10000: episode: 2200, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.120614, accuracy: 0.812500, mean_q: 100.188019\n",
      " 2656/10000: episode: 2201, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 42.308723, accuracy: 0.781250, mean_q: 100.131035\n",
      " 2657/10000: episode: 2202, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.206630, accuracy: 0.812500, mean_q: 99.928925\n",
      " 2658/10000: episode: 2203, duration: 0.013s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 138.157883, accuracy: 0.718750, mean_q: 99.669807\n",
      " 2659/10000: episode: 2204, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.638280, accuracy: 0.875000, mean_q: 99.356079\n",
      " 2661/10000: episode: 2205, duration: 0.019s, episode steps:   2, steps per second: 104, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [7.000, 9.000],  loss: 1.210181, accuracy: 0.734375, mean_q: 99.869217\n",
      " 2663/10000: episode: 2206, duration: 0.018s, episode steps:   2, steps per second: 110, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [4.000, 5.000],  loss: 1.462226, accuracy: 0.734375, mean_q: 100.570480\n",
      " 2664/10000: episode: 2207, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.313978, accuracy: 0.812500, mean_q: 100.126465\n",
      " 2665/10000: episode: 2208, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.529728, accuracy: 0.812500, mean_q: 99.847816\n",
      " 2666/10000: episode: 2209, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 150.791779, accuracy: 0.843750, mean_q: 99.855606\n",
      " 2667/10000: episode: 2210, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.301445, accuracy: 0.750000, mean_q: 99.711800\n",
      " 2669/10000: episode: 2211, duration: 0.020s, episode steps:   2, steps per second: 101, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [5.000, 10.000],  loss: 1.789551, accuracy: 0.843750, mean_q: 99.310555\n",
      " 2670/10000: episode: 2212, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.692366, accuracy: 0.937500, mean_q: 99.857880\n",
      " 2671/10000: episode: 2213, duration: 0.015s, episode steps:   1, steps per second:  67, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.457909, accuracy: 0.718750, mean_q: 100.058563\n",
      " 2672/10000: episode: 2214, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.553041, accuracy: 0.843750, mean_q: 100.141922\n",
      " 2673/10000: episode: 2215, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.641620, accuracy: 0.906250, mean_q: 99.871780\n",
      " 2674/10000: episode: 2216, duration: 0.012s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.222260, accuracy: 0.843750, mean_q: 99.976250\n",
      " 2675/10000: episode: 2217, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.518366, accuracy: 0.906250, mean_q: 100.169861\n",
      " 2676/10000: episode: 2218, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 129.449188, accuracy: 0.875000, mean_q: 100.359299\n",
      " 2677/10000: episode: 2219, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.563313, accuracy: 0.937500, mean_q: 99.340118\n",
      " 2678/10000: episode: 2220, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.172298, accuracy: 0.906250, mean_q: 99.236855\n",
      " 2679/10000: episode: 2221, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.625045, accuracy: 0.937500, mean_q: 100.054535\n",
      " 2680/10000: episode: 2222, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.908911, accuracy: 0.843750, mean_q: 100.378990\n",
      " 2682/10000: episode: 2223, duration: 0.019s, episode steps:   2, steps per second: 105, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 9.000 [7.000, 11.000],  loss: 1.959306, accuracy: 0.859375, mean_q: 100.618484\n",
      " 2683/10000: episode: 2224, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.537167, accuracy: 0.843750, mean_q: 100.052719\n",
      " 2684/10000: episode: 2225, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.517199, accuracy: 0.812500, mean_q: 99.651680\n",
      " 2685/10000: episode: 2226, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.855068, accuracy: 0.906250, mean_q: 100.232849\n",
      " 2686/10000: episode: 2227, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.810979, accuracy: 0.812500, mean_q: 100.330811\n",
      " 2687/10000: episode: 2228, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.393305, accuracy: 0.906250, mean_q: 100.537918\n",
      " 2688/10000: episode: 2229, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.447313, accuracy: 0.812500, mean_q: 100.545097\n",
      " 2689/10000: episode: 2230, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.503974, accuracy: 0.843750, mean_q: 100.402481\n",
      " 2690/10000: episode: 2231, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.758033, accuracy: 0.906250, mean_q: 99.659515\n",
      " 2691/10000: episode: 2232, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.663138, accuracy: 0.781250, mean_q: 99.611595\n",
      " 2692/10000: episode: 2233, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.607852, accuracy: 0.812500, mean_q: 99.773270\n",
      " 2693/10000: episode: 2234, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.094923, accuracy: 0.937500, mean_q: 100.079414\n",
      " 2694/10000: episode: 2235, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.352507, accuracy: 0.812500, mean_q: 100.455399\n",
      " 2695/10000: episode: 2236, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.293512, accuracy: 0.781250, mean_q: 100.505829\n",
      " 2696/10000: episode: 2237, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.925824, accuracy: 0.718750, mean_q: 99.877563\n",
      " 2697/10000: episode: 2238, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.914970, accuracy: 0.906250, mean_q: 99.436157\n",
      " 2698/10000: episode: 2239, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.711578, accuracy: 0.781250, mean_q: 99.022842\n",
      " 2699/10000: episode: 2240, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.319436, accuracy: 0.750000, mean_q: 99.566055\n",
      " 2700/10000: episode: 2241, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.360613, accuracy: 0.906250, mean_q: 100.187683\n",
      " 2701/10000: episode: 2242, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 151.841843, accuracy: 0.781250, mean_q: 100.602180\n",
      " 2702/10000: episode: 2243, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.244399, accuracy: 0.875000, mean_q: 100.306793\n",
      " 2703/10000: episode: 2244, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 143.379684, accuracy: 0.906250, mean_q: 99.190010\n",
      " 2704/10000: episode: 2245, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 137.772232, accuracy: 0.750000, mean_q: 99.052963\n",
      " 2705/10000: episode: 2246, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 4.924271, accuracy: 0.875000, mean_q: 98.177094\n",
      " 2707/10000: episode: 2247, duration: 0.019s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [4.000, 5.000],  loss: 16.011148, accuracy: 0.843750, mean_q: 100.394272\n",
      " 2708/10000: episode: 2248, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.443938, accuracy: 0.843750, mean_q: 101.095688\n",
      " 2709/10000: episode: 2249, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.403529, accuracy: 0.843750, mean_q: 100.463867\n",
      " 2711/10000: episode: 2250, duration: 0.019s, episode steps:   2, steps per second: 107, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [3.000, 7.000],  loss: 3.186097, accuracy: 0.843750, mean_q: 99.879364\n",
      " 2712/10000: episode: 2251, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 2.403410, accuracy: 0.812500, mean_q: 100.170624\n",
      " 2713/10000: episode: 2252, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.933800, accuracy: 1.000000, mean_q: 100.133232\n",
      " 2714/10000: episode: 2253, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.719524, accuracy: 0.843750, mean_q: 100.621437\n",
      " 2715/10000: episode: 2254, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.301033, accuracy: 0.781250, mean_q: 100.110947\n",
      " 2716/10000: episode: 2255, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.463315, accuracy: 0.875000, mean_q: 99.627663\n",
      " 2717/10000: episode: 2256, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.819816, accuracy: 0.812500, mean_q: 99.186310\n",
      " 2718/10000: episode: 2257, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.945156, accuracy: 0.875000, mean_q: 99.939407\n",
      " 2719/10000: episode: 2258, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.323565, accuracy: 1.000000, mean_q: 100.193161\n",
      " 2720/10000: episode: 2259, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.588502, accuracy: 0.781250, mean_q: 100.491974\n",
      " 2722/10000: episode: 2260, duration: 0.019s, episode steps:   2, steps per second: 104, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [2.000, 10.000],  loss: 0.972597, accuracy: 0.812500, mean_q: 99.944702\n",
      " 2723/10000: episode: 2261, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 282.332031, accuracy: 0.781250, mean_q: 100.119286\n",
      " 2724/10000: episode: 2262, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.194296, accuracy: 0.843750, mean_q: 99.520493\n",
      " 2725/10000: episode: 2263, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.002789, accuracy: 0.906250, mean_q: 99.736984\n",
      " 2726/10000: episode: 2264, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.092786, accuracy: 0.812500, mean_q: 99.666977\n",
      " 2727/10000: episode: 2265, duration: 0.011s, episode steps:   1, steps per second:  87, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.919063, accuracy: 0.875000, mean_q: 100.221634\n",
      " 2728/10000: episode: 2266, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.931943, accuracy: 0.843750, mean_q: 100.542046\n",
      " 2729/10000: episode: 2267, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.778622, accuracy: 0.906250, mean_q: 100.509552\n",
      " 2730/10000: episode: 2268, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.490752, accuracy: 0.968750, mean_q: 100.762756\n",
      " 2731/10000: episode: 2269, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.944080, accuracy: 0.687500, mean_q: 100.487114\n",
      " 2732/10000: episode: 2270, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.599863, accuracy: 0.937500, mean_q: 100.096588\n",
      " 2733/10000: episode: 2271, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.085233, accuracy: 0.906250, mean_q: 99.324631\n",
      " 2735/10000: episode: 2272, duration: 0.018s, episode steps:   2, steps per second: 108, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [2.000, 9.000],  loss: 66.334755, accuracy: 0.890625, mean_q: 99.638031\n",
      " 2736/10000: episode: 2273, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.164360, accuracy: 0.843750, mean_q: 99.711342\n",
      " 2737/10000: episode: 2274, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 5.895804, accuracy: 0.750000, mean_q: 100.346329\n",
      " 2738/10000: episode: 2275, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.769404, accuracy: 0.843750, mean_q: 101.156265\n",
      " 2739/10000: episode: 2276, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.506864, accuracy: 0.843750, mean_q: 100.361740\n",
      " 2741/10000: episode: 2277, duration: 0.027s, episode steps:   2, steps per second:  74, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [2.000, 10.000],  loss: 2.286017, accuracy: 0.812500, mean_q: 100.078842\n",
      " 2742/10000: episode: 2278, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.271690, accuracy: 0.875000, mean_q: 99.653030\n",
      " 2743/10000: episode: 2279, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.697950, accuracy: 0.875000, mean_q: 100.470581\n",
      " 2744/10000: episode: 2280, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.693545, accuracy: 0.843750, mean_q: 100.614517\n",
      " 2745/10000: episode: 2281, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.314966, accuracy: 0.875000, mean_q: 99.677643\n",
      " 2746/10000: episode: 2282, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.676225, accuracy: 0.843750, mean_q: 99.440308\n",
      " 2747/10000: episode: 2283, duration: 0.013s, episode steps:   1, steps per second:  76, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.270334, accuracy: 0.937500, mean_q: 100.023376\n",
      " 2748/10000: episode: 2284, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.543036, accuracy: 0.781250, mean_q: 100.528366\n",
      " 2749/10000: episode: 2285, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 136.367737, accuracy: 0.781250, mean_q: 100.143372\n",
      " 2750/10000: episode: 2286, duration: 0.017s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.447728, accuracy: 0.875000, mean_q: 99.189667\n",
      " 2751/10000: episode: 2287, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.873843, accuracy: 0.843750, mean_q: 99.023346\n",
      " 2752/10000: episode: 2288, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 7.979370, accuracy: 0.781250, mean_q: 99.553085\n",
      " 2753/10000: episode: 2289, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.286587, accuracy: 0.968750, mean_q: 101.027496\n",
      " 2754/10000: episode: 2290, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.465954, accuracy: 1.000000, mean_q: 100.827393\n",
      " 2755/10000: episode: 2291, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 14.899555, accuracy: 0.750000, mean_q: 100.135078\n",
      " 2757/10000: episode: 2292, duration: 0.018s, episode steps:   2, steps per second: 112, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [0.000, 3.000],  loss: 32.578629, accuracy: 0.921875, mean_q: 100.055199\n",
      " 2758/10000: episode: 2293, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.277245, accuracy: 0.906250, mean_q: 100.100876\n",
      " 2759/10000: episode: 2294, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 7.991096, accuracy: 0.781250, mean_q: 99.775581\n",
      " 2760/10000: episode: 2295, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.347438, accuracy: 0.812500, mean_q: 100.193268\n",
      " 2761/10000: episode: 2296, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.089372, accuracy: 0.750000, mean_q: 100.499146\n",
      " 2762/10000: episode: 2297, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.870260, accuracy: 0.812500, mean_q: 100.233932\n",
      " 2763/10000: episode: 2298, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.549416, accuracy: 0.812500, mean_q: 99.787109\n",
      " 2764/10000: episode: 2299, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.652622, accuracy: 0.812500, mean_q: 99.583534\n",
      " 2765/10000: episode: 2300, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.165477, accuracy: 0.875000, mean_q: 99.899994\n",
      " 2766/10000: episode: 2301, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.634028, accuracy: 0.843750, mean_q: 100.241394\n",
      " 2767/10000: episode: 2302, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.265457, accuracy: 0.906250, mean_q: 100.281380\n",
      " 2768/10000: episode: 2303, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.105908, accuracy: 0.843750, mean_q: 100.556473\n",
      " 2769/10000: episode: 2304, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.605435, accuracy: 0.906250, mean_q: 99.868378\n",
      " 2770/10000: episode: 2305, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.502907, accuracy: 0.875000, mean_q: 99.865845\n",
      " 2771/10000: episode: 2306, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.587395, accuracy: 0.750000, mean_q: 100.030373\n",
      " 2772/10000: episode: 2307, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 141.631866, accuracy: 0.937500, mean_q: 100.334923\n",
      " 2773/10000: episode: 2308, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.607334, accuracy: 0.937500, mean_q: 99.488686\n",
      " 2774/10000: episode: 2309, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.590338, accuracy: 0.906250, mean_q: 99.467918\n",
      " 2775/10000: episode: 2310, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.563925, accuracy: 0.781250, mean_q: 99.453720\n",
      " 2776/10000: episode: 2311, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 4.141579, accuracy: 0.750000, mean_q: 99.514709\n",
      " 2777/10000: episode: 2312, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.204589, accuracy: 1.000000, mean_q: 100.048294\n",
      " 2778/10000: episode: 2313, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.591523, accuracy: 0.812500, mean_q: 99.347153\n",
      " 2779/10000: episode: 2314, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.263044, accuracy: 0.812500, mean_q: 99.482361\n",
      " 2781/10000: episode: 2315, duration: 0.032s, episode steps:   2, steps per second:  62, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [3.000, 11.000],  loss: 1.256566, accuracy: 0.781250, mean_q: 99.529205\n",
      " 2782/10000: episode: 2316, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 1.487573, accuracy: 0.906250, mean_q: 99.951706\n",
      " 2783/10000: episode: 2317, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.841768, accuracy: 0.843750, mean_q: 100.881767\n",
      " 2784/10000: episode: 2318, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.511679, accuracy: 0.843750, mean_q: 100.485092\n",
      " 2786/10000: episode: 2319, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 0.500 [0.000, 1.000],  loss: 1.196236, accuracy: 0.890625, mean_q: 99.412811\n",
      " 2787/10000: episode: 2320, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.274258, accuracy: 0.937500, mean_q: 99.106735\n",
      " 2788/10000: episode: 2321, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.271322, accuracy: 0.875000, mean_q: 100.197609\n",
      " 2789/10000: episode: 2322, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.407217, accuracy: 0.843750, mean_q: 100.429779\n",
      " 2790/10000: episode: 2323, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.751882, accuracy: 0.843750, mean_q: 100.292427\n",
      " 2791/10000: episode: 2324, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 4.681263, accuracy: 0.781250, mean_q: 99.853348\n",
      " 2792/10000: episode: 2325, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.889995, accuracy: 0.718750, mean_q: 100.008194\n",
      " 2793/10000: episode: 2326, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.950614, accuracy: 0.843750, mean_q: 100.499283\n",
      " 2794/10000: episode: 2327, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.069737, accuracy: 0.906250, mean_q: 100.302628\n",
      " 2795/10000: episode: 2328, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.931478, accuracy: 0.875000, mean_q: 99.464142\n",
      " 2796/10000: episode: 2329, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.428139, accuracy: 0.812500, mean_q: 99.829887\n",
      " 2797/10000: episode: 2330, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.477185, accuracy: 0.781250, mean_q: 99.807007\n",
      " 2798/10000: episode: 2331, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.934432, accuracy: 0.875000, mean_q: 100.575768\n",
      " 2799/10000: episode: 2332, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.522731, accuracy: 0.906250, mean_q: 100.224800\n",
      " 2800/10000: episode: 2333, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.292228, accuracy: 0.781250, mean_q: 100.040100\n",
      " 2801/10000: episode: 2334, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.503887, accuracy: 0.937500, mean_q: 99.688934\n",
      " 2803/10000: episode: 2335, duration: 0.031s, episode steps:   2, steps per second:  65, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [4.000, 8.000],  loss: 0.372248, accuracy: 0.875000, mean_q: 99.931442\n",
      " 2804/10000: episode: 2336, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 29.034195, accuracy: 0.750000, mean_q: 100.392838\n",
      " 2805/10000: episode: 2337, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.445935, accuracy: 0.875000, mean_q: 100.143799\n",
      " 2806/10000: episode: 2338, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.306960, accuracy: 0.968750, mean_q: 99.929642\n",
      " 2807/10000: episode: 2339, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.654812, accuracy: 0.781250, mean_q: 99.838455\n",
      " 2808/10000: episode: 2340, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.463435, accuracy: 0.843750, mean_q: 99.585426\n",
      " 2809/10000: episode: 2341, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.307039, accuracy: 0.843750, mean_q: 100.119972\n",
      " 2810/10000: episode: 2342, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.256298, accuracy: 0.812500, mean_q: 100.353889\n",
      " 2811/10000: episode: 2343, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.431568, accuracy: 0.906250, mean_q: 100.252228\n",
      " 2812/10000: episode: 2344, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.591744, accuracy: 0.812500, mean_q: 99.764114\n",
      " 2813/10000: episode: 2345, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.423042, accuracy: 0.875000, mean_q: 99.736633\n",
      " 2814/10000: episode: 2346, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.461627, accuracy: 0.781250, mean_q: 99.775871\n",
      " 2815/10000: episode: 2347, duration: 0.032s, episode steps:   1, steps per second:  31, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.298124, accuracy: 0.843750, mean_q: 100.238678\n",
      " 2816/10000: episode: 2348, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.086173, accuracy: 0.812500, mean_q: 100.544708\n",
      " 2817/10000: episode: 2349, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.305950, accuracy: 0.750000, mean_q: 100.111160\n",
      " 2818/10000: episode: 2350, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.872690, accuracy: 0.843750, mean_q: 99.717430\n",
      " 2819/10000: episode: 2351, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.209375, accuracy: 0.875000, mean_q: 99.701515\n",
      " 2820/10000: episode: 2352, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.137841, accuracy: 0.906250, mean_q: 100.231873\n",
      " 2821/10000: episode: 2353, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.589416, accuracy: 0.718750, mean_q: 100.404678\n",
      " 2822/10000: episode: 2354, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.296998, accuracy: 0.843750, mean_q: 100.154160\n",
      " 2823/10000: episode: 2355, duration: 0.028s, episode steps:   1, steps per second:  35, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.311606, accuracy: 0.875000, mean_q: 99.834969\n",
      " 2825/10000: episode: 2356, duration: 0.036s, episode steps:   2, steps per second:  56, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [3.000, 8.000],  loss: 0.722105, accuracy: 0.781250, mean_q: 99.688087\n",
      " 2826/10000: episode: 2357, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.599098, accuracy: 0.843750, mean_q: 100.037796\n",
      " 2827/10000: episode: 2358, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.644368, accuracy: 0.656250, mean_q: 100.534454\n",
      " 2828/10000: episode: 2359, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.750778, accuracy: 0.750000, mean_q: 100.467606\n",
      " 2829/10000: episode: 2360, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.643205, accuracy: 0.781250, mean_q: 100.106171\n",
      " 2830/10000: episode: 2361, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.474744, accuracy: 0.906250, mean_q: 99.684532\n",
      " 2831/10000: episode: 2362, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.768124, accuracy: 0.812500, mean_q: 99.318359\n",
      " 2832/10000: episode: 2363, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.070411, accuracy: 0.812500, mean_q: 100.125839\n",
      " 2833/10000: episode: 2364, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.374108, accuracy: 0.750000, mean_q: 100.630508\n",
      " 2834/10000: episode: 2365, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 151.932663, accuracy: 0.875000, mean_q: 100.055679\n",
      " 2835/10000: episode: 2366, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.434579, accuracy: 0.750000, mean_q: 98.355034\n",
      " 2836/10000: episode: 2367, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.270974, accuracy: 0.875000, mean_q: 98.494659\n",
      " 2837/10000: episode: 2368, duration: 0.054s, episode steps:   1, steps per second:  19, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.656029, accuracy: 0.812500, mean_q: 100.086533\n",
      " 2838/10000: episode: 2369, duration: 0.030s, episode steps:   1, steps per second:  33, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 3.776543, accuracy: 0.906250, mean_q: 100.745537\n",
      " 2839/10000: episode: 2370, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.588776, accuracy: 0.781250, mean_q: 100.972626\n",
      " 2840/10000: episode: 2371, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 4.638287, accuracy: 0.718750, mean_q: 100.788422\n",
      " 2841/10000: episode: 2372, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.604137, accuracy: 0.812500, mean_q: 100.370773\n",
      " 2843/10000: episode: 2373, duration: 0.031s, episode steps:   2, steps per second:  65, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 10.500 [10.000, 11.000],  loss: 1.737575, accuracy: 0.812500, mean_q: 99.931213\n",
      " 2844/10000: episode: 2374, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.475453, accuracy: 0.937500, mean_q: 100.401871\n",
      " 2845/10000: episode: 2375, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 67.798676, accuracy: 0.812500, mean_q: 99.780640\n",
      " 2846/10000: episode: 2376, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 2.765198, accuracy: 0.906250, mean_q: 99.877693\n",
      " 2848/10000: episode: 2377, duration: 0.030s, episode steps:   2, steps per second:  67, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [6.000, 10.000],  loss: 0.718287, accuracy: 0.859375, mean_q: 99.830833\n",
      " 2849/10000: episode: 2378, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.925124, accuracy: 0.781250, mean_q: 100.262817\n",
      " 2850/10000: episode: 2379, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.639276, accuracy: 0.781250, mean_q: 100.154213\n",
      " 2851/10000: episode: 2380, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.392169, accuracy: 0.781250, mean_q: 99.656509\n",
      " 2853/10000: episode: 2381, duration: 0.038s, episode steps:   2, steps per second:  52, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [3.000, 7.000],  loss: 0.735176, accuracy: 0.703125, mean_q: 100.104828\n",
      " 2854/10000: episode: 2382, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.936996, accuracy: 0.718750, mean_q: 100.039825\n",
      " 2855/10000: episode: 2383, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 30.611956, accuracy: 0.812500, mean_q: 100.188644\n",
      " 2856/10000: episode: 2384, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.599293, accuracy: 0.750000, mean_q: 99.932755\n",
      " 2857/10000: episode: 2385, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.336128, accuracy: 0.812500, mean_q: 99.718636\n",
      " 2858/10000: episode: 2386, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.610321, accuracy: 0.875000, mean_q: 99.709045\n",
      " 2859/10000: episode: 2387, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.583290, accuracy: 0.781250, mean_q: 99.861450\n",
      " 2860/10000: episode: 2388, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.736726, accuracy: 0.718750, mean_q: 99.955841\n",
      " 2861/10000: episode: 2389, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.350398, accuracy: 0.843750, mean_q: 100.016052\n",
      " 2862/10000: episode: 2390, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.419215, accuracy: 0.937500, mean_q: 99.891838\n",
      " 2863/10000: episode: 2391, duration: 0.037s, episode steps:   1, steps per second:  27, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.669654, accuracy: 0.937500, mean_q: 99.851524\n",
      " 2864/10000: episode: 2392, duration: 0.039s, episode steps:   1, steps per second:  26, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.157129, accuracy: 0.843750, mean_q: 100.014130\n",
      " 2865/10000: episode: 2393, duration: 0.030s, episode steps:   1, steps per second:  34, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.398468, accuracy: 0.843750, mean_q: 100.034340\n",
      " 2866/10000: episode: 2394, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.112821, accuracy: 0.812500, mean_q: 100.108017\n",
      " 2867/10000: episode: 2395, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.254704, accuracy: 0.906250, mean_q: 100.044937\n",
      " 2868/10000: episode: 2396, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.122240, accuracy: 0.843750, mean_q: 100.145912\n",
      " 2869/10000: episode: 2397, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.238409, accuracy: 0.781250, mean_q: 99.851135\n",
      " 2870/10000: episode: 2398, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.216510, accuracy: 0.781250, mean_q: 99.845444\n",
      " 2872/10000: episode: 2399, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [1.000, 7.000],  loss: 73.859909, accuracy: 0.734375, mean_q: 100.458984\n",
      " 2874/10000: episode: 2400, duration: 0.040s, episode steps:   2, steps per second:  49, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [0.000, 11.000],  loss: 1.201704, accuracy: 0.843750, mean_q: 99.356026\n",
      " 2875/10000: episode: 2401, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.889627, accuracy: 0.843750, mean_q: 99.226486\n",
      " 2876/10000: episode: 2402, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.431854, accuracy: 0.875000, mean_q: 100.628410\n",
      " 2878/10000: episode: 2403, duration: 0.028s, episode steps:   2, steps per second:  72, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [3.000, 8.000],  loss: 76.387009, accuracy: 0.890625, mean_q: 100.289650\n",
      " 2879/10000: episode: 2404, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.601778, accuracy: 0.875000, mean_q: 99.564331\n",
      " 2880/10000: episode: 2405, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.143784, accuracy: 0.781250, mean_q: 99.120239\n",
      " 2881/10000: episode: 2406, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.009395, accuracy: 0.812500, mean_q: 100.102371\n",
      " 2882/10000: episode: 2407, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.652187, accuracy: 0.937500, mean_q: 100.779884\n",
      " 2883/10000: episode: 2408, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.473498, accuracy: 0.843750, mean_q: 100.386169\n",
      " 2884/10000: episode: 2409, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.295945, accuracy: 0.781250, mean_q: 100.465500\n",
      " 2885/10000: episode: 2410, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.467104, accuracy: 0.843750, mean_q: 99.623131\n",
      " 2886/10000: episode: 2411, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.678957, accuracy: 0.875000, mean_q: 99.205322\n",
      " 2888/10000: episode: 2412, duration: 0.034s, episode steps:   2, steps per second:  60, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.000 [4.000, 8.000],  loss: 0.782311, accuracy: 0.875000, mean_q: 99.953537\n",
      " 2889/10000: episode: 2413, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.616848, accuracy: 1.000000, mean_q: 100.693192\n",
      " 2890/10000: episode: 2414, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.174418, accuracy: 0.843750, mean_q: 100.276520\n",
      " 2891/10000: episode: 2415, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.309019, accuracy: 0.968750, mean_q: 100.158180\n",
      " 2893/10000: episode: 2416, duration: 0.031s, episode steps:   2, steps per second:  64, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.500 [5.000, 6.000],  loss: 1.737593, accuracy: 0.843750, mean_q: 100.075684\n",
      " 2894/10000: episode: 2417, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 18.575769, accuracy: 0.843750, mean_q: 100.415771\n",
      " 2895/10000: episode: 2418, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 137.617691, accuracy: 0.812500, mean_q: 100.306442\n",
      " 2896/10000: episode: 2419, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.895028, accuracy: 0.843750, mean_q: 99.391571\n",
      " 2897/10000: episode: 2420, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 2.273870, accuracy: 0.843750, mean_q: 99.249832\n",
      " 2898/10000: episode: 2421, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 7.782347, accuracy: 0.875000, mean_q: 99.905266\n",
      " 2899/10000: episode: 2422, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.902786, accuracy: 0.812500, mean_q: 100.991272\n",
      " 2900/10000: episode: 2423, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.877901, accuracy: 0.750000, mean_q: 100.963051\n",
      " 2901/10000: episode: 2424, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.518871, accuracy: 0.906250, mean_q: 100.694862\n",
      " 2903/10000: episode: 2425, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [3.000, 11.000],  loss: 1.357376, accuracy: 0.890625, mean_q: 99.371307\n",
      " 2905/10000: episode: 2426, duration: 0.034s, episode steps:   2, steps per second:  59, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [3.000, 6.000],  loss: 2.279021, accuracy: 0.843750, mean_q: 99.443390\n",
      " 2906/10000: episode: 2427, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.334450, accuracy: 0.843750, mean_q: 100.766281\n",
      " 2907/10000: episode: 2428, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.489847, accuracy: 0.718750, mean_q: 100.639656\n",
      " 2908/10000: episode: 2429, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.425041, accuracy: 0.812500, mean_q: 100.090584\n",
      " 2909/10000: episode: 2430, duration: 0.056s, episode steps:   1, steps per second:  18, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 2.887142, accuracy: 0.875000, mean_q: 99.878891\n",
      " 2910/10000: episode: 2431, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 1.693306, accuracy: 0.843750, mean_q: 99.839050\n",
      " 2911/10000: episode: 2432, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.272273, accuracy: 0.781250, mean_q: 100.318192\n",
      " 2912/10000: episode: 2433, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.963542, accuracy: 0.843750, mean_q: 100.553345\n",
      " 2913/10000: episode: 2434, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.162275, accuracy: 0.781250, mean_q: 100.271118\n",
      " 2914/10000: episode: 2435, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.259116, accuracy: 0.781250, mean_q: 99.520660\n",
      " 2915/10000: episode: 2436, duration: 0.018s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.361481, accuracy: 0.843750, mean_q: 99.707764\n",
      " 2916/10000: episode: 2437, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.234766, accuracy: 0.875000, mean_q: 99.899712\n",
      " 2917/10000: episode: 2438, duration: 0.031s, episode steps:   1, steps per second:  32, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.278657, accuracy: 0.875000, mean_q: 100.176559\n",
      " 2918/10000: episode: 2439, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.122725, accuracy: 0.937500, mean_q: 100.078209\n",
      " 2919/10000: episode: 2440, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.506844, accuracy: 0.750000, mean_q: 99.954163\n",
      " 2921/10000: episode: 2441, duration: 0.032s, episode steps:   2, steps per second:  63, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.000 [6.000, 8.000],  loss: 0.495190, accuracy: 0.859375, mean_q: 100.111008\n",
      " 2922/10000: episode: 2442, duration: 0.021s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.976310, accuracy: 0.812500, mean_q: 100.488602\n",
      " 2923/10000: episode: 2443, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.050212, accuracy: 0.812500, mean_q: 100.063919\n",
      " 2924/10000: episode: 2444, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.184525, accuracy: 0.812500, mean_q: 98.592056\n",
      " 2925/10000: episode: 2445, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.030459, accuracy: 0.875000, mean_q: 98.602028\n",
      " 2926/10000: episode: 2446, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.962107, accuracy: 0.906250, mean_q: 99.959999\n",
      " 2927/10000: episode: 2447, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.935620, accuracy: 0.781250, mean_q: 100.613060\n",
      " 2928/10000: episode: 2448, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 3.844081, accuracy: 0.843750, mean_q: 99.861710\n",
      " 2929/10000: episode: 2449, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.950194, accuracy: 0.906250, mean_q: 99.787140\n",
      " 2930/10000: episode: 2450, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.787476, accuracy: 0.843750, mean_q: 100.014130\n",
      " 2931/10000: episode: 2451, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.280697, accuracy: 0.750000, mean_q: 99.782570\n",
      " 2932/10000: episode: 2452, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.621643, accuracy: 0.750000, mean_q: 99.934082\n",
      " 2933/10000: episode: 2453, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.516740, accuracy: 0.781250, mean_q: 100.205002\n",
      " 2934/10000: episode: 2454, duration: 0.025s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 3.288444, accuracy: 0.875000, mean_q: 99.822861\n",
      " 2935/10000: episode: 2455, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.256981, accuracy: 0.937500, mean_q: 99.687019\n",
      " 2936/10000: episode: 2456, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 15.216255, accuracy: 0.875000, mean_q: 99.404282\n",
      " 2937/10000: episode: 2457, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 153.394302, accuracy: 0.812500, mean_q: 100.164238\n",
      " 2938/10000: episode: 2458, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 3.978596, accuracy: 0.781250, mean_q: 99.945549\n",
      " 2939/10000: episode: 2459, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 3.583149, accuracy: 0.812500, mean_q: 99.613907\n",
      " 2940/10000: episode: 2460, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 3.011680, accuracy: 0.875000, mean_q: 98.829483\n",
      " 2941/10000: episode: 2461, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.536241, accuracy: 0.906250, mean_q: 99.811050\n",
      " 2942/10000: episode: 2462, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 137.656387, accuracy: 0.718750, mean_q: 100.433914\n",
      " 2944/10000: episode: 2463, duration: 0.027s, episode steps:   2, steps per second:  75, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [4.000, 11.000],  loss: 2.008589, accuracy: 0.843750, mean_q: 100.137863\n",
      " 2946/10000: episode: 2464, duration: 0.035s, episode steps:   2, steps per second:  58, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 7.500 [4.000, 11.000],  loss: 3.189526, accuracy: 0.765625, mean_q: 99.401886\n",
      " 2947/10000: episode: 2465, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.459320, accuracy: 0.937500, mean_q: 100.004990\n",
      " 2948/10000: episode: 2466, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 4.351027, accuracy: 0.875000, mean_q: 100.308594\n",
      " 2949/10000: episode: 2467, duration: 0.025s, episode steps:   1, steps per second:  40, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.751846, accuracy: 0.781250, mean_q: 100.594711\n",
      " 2950/10000: episode: 2468, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.311986, accuracy: 0.781250, mean_q: 100.251427\n",
      " 2953/10000: episode: 2469, duration: 0.041s, episode steps:   3, steps per second:  74, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 3.333 [2.000, 5.000],  loss: 1.353262, accuracy: 0.843750, mean_q: 99.987038\n",
      " 2954/10000: episode: 2470, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 133.136673, accuracy: 0.843750, mean_q: 100.341446\n",
      " 2955/10000: episode: 2471, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.969182, accuracy: 0.812500, mean_q: 100.110535\n",
      " 2956/10000: episode: 2472, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.797266, accuracy: 0.875000, mean_q: 99.595184\n",
      " 2957/10000: episode: 2473, duration: 0.023s, episode steps:   1, steps per second:  43, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 2.618569, accuracy: 0.906250, mean_q: 99.752899\n",
      " 2958/10000: episode: 2474, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 2.074323, accuracy: 0.656250, mean_q: 100.183472\n",
      " 2959/10000: episode: 2475, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 124.912399, accuracy: 0.875000, mean_q: 100.233231\n",
      " 2960/10000: episode: 2476, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.260631, accuracy: 0.906250, mean_q: 99.898285\n",
      " 2961/10000: episode: 2477, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.293525, accuracy: 0.843750, mean_q: 99.827301\n",
      " 2962/10000: episode: 2478, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 1.090439, accuracy: 0.687500, mean_q: 99.874405\n",
      " 2963/10000: episode: 2479, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 3.473418, accuracy: 0.750000, mean_q: 100.261635\n",
      " 2964/10000: episode: 2480, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 3.348368, accuracy: 0.656250, mean_q: 100.653137\n",
      " 2966/10000: episode: 2481, duration: 0.030s, episode steps:   2, steps per second:  66, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 6.500 [3.000, 10.000],  loss: 6.267170, accuracy: 0.859375, mean_q: 100.465599\n",
      " 2967/10000: episode: 2482, duration: 0.027s, episode steps:   1, steps per second:  37, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.512886, accuracy: 0.812500, mean_q: 100.289368\n",
      " 2968/10000: episode: 2483, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.426604, accuracy: 0.781250, mean_q: 100.380531\n",
      " 2969/10000: episode: 2484, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 5.397334, accuracy: 0.843750, mean_q: 100.221123\n",
      " 2970/10000: episode: 2485, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.692527, accuracy: 0.812500, mean_q: 100.065063\n",
      " 2971/10000: episode: 2486, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.035685, accuracy: 0.812500, mean_q: 99.892372\n",
      " 2972/10000: episode: 2487, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.045375, accuracy: 0.781250, mean_q: 99.561852\n",
      " 2973/10000: episode: 2488, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 2.042746, accuracy: 0.875000, mean_q: 99.598724\n",
      " 2974/10000: episode: 2489, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 2.410371, accuracy: 0.843750, mean_q: 99.833435\n",
      " 2975/10000: episode: 2490, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 2.974157, accuracy: 0.812500, mean_q: 100.609894\n",
      " 2976/10000: episode: 2491, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 2.356344, accuracy: 0.843750, mean_q: 100.332047\n",
      " 2977/10000: episode: 2492, duration: 0.024s, episode steps:   1, steps per second:  42, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.931351, accuracy: 0.875000, mean_q: 100.085625\n",
      " 2978/10000: episode: 2493, duration: 0.026s, episode steps:   1, steps per second:  38, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 2.460249, accuracy: 0.812500, mean_q: 99.678192\n",
      " 2979/10000: episode: 2494, duration: 0.046s, episode steps:   1, steps per second:  22, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.331517, accuracy: 0.781250, mean_q: 99.984413\n",
      " 2980/10000: episode: 2495, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.868120, accuracy: 0.843750, mean_q: 100.314499\n",
      " 2981/10000: episode: 2496, duration: 0.014s, episode steps:   1, steps per second:  74, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.928582, accuracy: 0.750000, mean_q: 100.281830\n",
      " 2982/10000: episode: 2497, duration: 0.016s, episode steps:   1, steps per second:  64, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.924090, accuracy: 0.750000, mean_q: 100.181427\n",
      " 2983/10000: episode: 2498, duration: 0.062s, episode steps:   1, steps per second:  16, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.662543, accuracy: 0.781250, mean_q: 100.010895\n",
      " 2984/10000: episode: 2499, duration: 0.022s, episode steps:   1, steps per second:  46, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.255999, accuracy: 0.875000, mean_q: 99.819168\n",
      " 2985/10000: episode: 2500, duration: 0.025s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.808193, accuracy: 0.625000, mean_q: 99.688423\n",
      " 2986/10000: episode: 2501, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.430973, accuracy: 0.843750, mean_q: 99.722595\n",
      " 2987/10000: episode: 2502, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.231594, accuracy: 0.781250, mean_q: 99.789108\n",
      " 2988/10000: episode: 2503, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.782209, accuracy: 0.843750, mean_q: 100.087616\n",
      " 2989/10000: episode: 2504, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.257490, accuracy: 0.781250, mean_q: 100.271477\n",
      " 2990/10000: episode: 2505, duration: 0.022s, episode steps:   1, steps per second:  45, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 151.345917, accuracy: 0.875000, mean_q: 100.519287\n",
      " 2991/10000: episode: 2506, duration: 0.017s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.623540, accuracy: 0.812500, mean_q: 99.321587\n",
      " 2992/10000: episode: 2507, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.769520, accuracy: 0.750000, mean_q: 99.266975\n",
      " 2993/10000: episode: 2508, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.368365, accuracy: 0.937500, mean_q: 99.652473\n",
      " 2994/10000: episode: 2509, duration: 0.024s, episode steps:   1, steps per second:  41, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.548471, accuracy: 0.937500, mean_q: 100.472748\n",
      " 2996/10000: episode: 2510, duration: 0.034s, episode steps:   2, steps per second:  58, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [0.000, 5.000],  loss: 1.536686, accuracy: 0.875000, mean_q: 99.628189\n",
      " 2997/10000: episode: 2511, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.196431, accuracy: 0.812500, mean_q: 99.839188\n",
      " 2998/10000: episode: 2512, duration: 0.028s, episode steps:   1, steps per second:  36, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 1.116247, accuracy: 0.906250, mean_q: 99.927414\n",
      " 2999/10000: episode: 2513, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.282420, accuracy: 0.875000, mean_q: 100.602470\n",
      " 3000/10000: episode: 2514, duration: 0.026s, episode steps:   1, steps per second:  39, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.382829, accuracy: 0.843750, mean_q: 100.419296\n",
      " 3001/10000: episode: 2515, duration: 0.021s, episode steps:   1, steps per second:  48, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 3.897699, accuracy: 0.781250, mean_q: 100.209511\n",
      " 3002/10000: episode: 2516, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.194964, accuracy: 1.000000, mean_q: 99.762756\n",
      " 3003/10000: episode: 2517, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.280839, accuracy: 0.812500, mean_q: 100.134567\n",
      " 3004/10000: episode: 2518, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.214784, accuracy: 0.843750, mean_q: 100.331696\n",
      " 3005/10000: episode: 2519, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.765168, accuracy: 0.937500, mean_q: 100.670822\n",
      " 3006/10000: episode: 2520, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.091115, accuracy: 0.875000, mean_q: 99.974030\n",
      " 3008/10000: episode: 2521, duration: 0.026s, episode steps:   2, steps per second:  77, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.000 [2.000, 6.000],  loss: 2.618591, accuracy: 0.859375, mean_q: 99.629768\n",
      " 3009/10000: episode: 2522, duration: 0.020s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.124275, accuracy: 0.937500, mean_q: 100.240135\n",
      " 3010/10000: episode: 2523, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.574079, accuracy: 0.843750, mean_q: 100.586853\n",
      " 3011/10000: episode: 2524, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.377374, accuracy: 0.875000, mean_q: 99.825783\n",
      " 3012/10000: episode: 2525, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 1.770012, accuracy: 0.937500, mean_q: 98.949333\n",
      " 3013/10000: episode: 2526, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.925578, accuracy: 0.843750, mean_q: 99.530350\n",
      " 3015/10000: episode: 2527, duration: 0.025s, episode steps:   2, steps per second:  79, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 2.500 [2.000, 3.000],  loss: 0.477617, accuracy: 0.843750, mean_q: 100.586472\n",
      " 3016/10000: episode: 2528, duration: 0.023s, episode steps:   1, steps per second:  44, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.123705, accuracy: 0.718750, mean_q: 100.320847\n",
      " 3017/10000: episode: 2529, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.602415, accuracy: 0.906250, mean_q: 99.308533\n",
      " 3018/10000: episode: 2530, duration: 0.019s, episode steps:   1, steps per second:  51, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.568690, accuracy: 0.906250, mean_q: 99.405113\n",
      " 3019/10000: episode: 2531, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 148.187073, accuracy: 0.812500, mean_q: 100.193420\n",
      " 3020/10000: episode: 2532, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.002046, accuracy: 0.968750, mean_q: 100.220184\n",
      " 3021/10000: episode: 2533, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.898409, accuracy: 0.843750, mean_q: 99.698090\n",
      " 3023/10000: episode: 2534, duration: 0.023s, episode steps:   2, steps per second:  88, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 1.500 [0.000, 3.000],  loss: 1.076021, accuracy: 0.843750, mean_q: 99.431458\n",
      " 3024/10000: episode: 2535, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.810051, accuracy: 0.906250, mean_q: 100.299187\n",
      " 3025/10000: episode: 2536, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.048167, accuracy: 0.781250, mean_q: 100.155746\n",
      " 3026/10000: episode: 2537, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.540162, accuracy: 0.843750, mean_q: 100.486313\n",
      " 3027/10000: episode: 2538, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.837025, accuracy: 0.812500, mean_q: 99.953537\n",
      " 3028/10000: episode: 2539, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.448665, accuracy: 0.906250, mean_q: 99.804619\n",
      " 3029/10000: episode: 2540, duration: 0.019s, episode steps:   1, steps per second:  52, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.358834, accuracy: 1.000000, mean_q: 99.593132\n",
      " 3030/10000: episode: 2541, duration: 0.021s, episode steps:   1, steps per second:  47, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.547758, accuracy: 0.781250, mean_q: 100.281158\n",
      " 3031/10000: episode: 2542, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 1.734940, accuracy: 0.843750, mean_q: 100.542404\n",
      " 3032/10000: episode: 2543, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.349065, accuracy: 0.906250, mean_q: 100.535347\n",
      " 3033/10000: episode: 2544, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.334279, accuracy: 0.843750, mean_q: 99.865143\n",
      " 3034/10000: episode: 2545, duration: 0.017s, episode steps:   1, steps per second:  58, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 0.724987, accuracy: 0.750000, mean_q: 99.734703\n",
      " 3035/10000: episode: 2546, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.119228, accuracy: 0.843750, mean_q: 100.159027\n",
      " 3036/10000: episode: 2547, duration: 0.018s, episode steps:   1, steps per second:  56, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.206985, accuracy: 0.906250, mean_q: 100.230331\n",
      " 3037/10000: episode: 2548, duration: 0.018s, episode steps:   1, steps per second:  57, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.919007, accuracy: 0.812500, mean_q: 99.990067\n",
      " 3038/10000: episode: 2549, duration: 0.018s, episode steps:   1, steps per second:  55, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.132363, accuracy: 0.906250, mean_q: 100.006134\n",
      " 3039/10000: episode: 2550, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 0.128575, accuracy: 0.843750, mean_q: 100.036430\n",
      " 3040/10000: episode: 2551, duration: 0.014s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.219256, accuracy: 0.906250, mean_q: 100.064117\n",
      " 3041/10000: episode: 2552, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.199201, accuracy: 0.875000, mean_q: 100.034126\n",
      " 3042/10000: episode: 2553, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.540140, accuracy: 0.750000, mean_q: 99.965988\n",
      " 3043/10000: episode: 2554, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 141.299210, accuracy: 0.750000, mean_q: 100.059456\n",
      " 3044/10000: episode: 2555, duration: 0.045s, episode steps:   1, steps per second:  22, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.489269, accuracy: 0.937500, mean_q: 100.010780\n",
      " 3045/10000: episode: 2556, duration: 0.020s, episode steps:   1, steps per second:  49, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.787718, accuracy: 0.843750, mean_q: 99.735687\n",
      " 3046/10000: episode: 2557, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.987335, accuracy: 0.843750, mean_q: 99.792305\n",
      " 3047/10000: episode: 2558, duration: 0.014s, episode steps:   1, steps per second:  73, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.409403, accuracy: 0.750000, mean_q: 100.198708\n",
      " 3048/10000: episode: 2559, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 1.119265, accuracy: 0.843750, mean_q: 100.055420\n",
      " 3049/10000: episode: 2560, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 151.953720, accuracy: 0.906250, mean_q: 99.815338\n",
      " 3050/10000: episode: 2561, duration: 0.014s, episode steps:   1, steps per second:  72, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 7.000 [7.000, 7.000],  loss: 0.992272, accuracy: 0.781250, mean_q: 99.729980\n",
      " 3051/10000: episode: 2562, duration: 0.015s, episode steps:   1, steps per second:  66, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 3.064790, accuracy: 0.906250, mean_q: 100.164558\n",
      " 3052/10000: episode: 2563, duration: 0.015s, episode steps:   1, steps per second:  65, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.159595, accuracy: 0.937500, mean_q: 100.088196\n",
      " 3053/10000: episode: 2564, duration: 0.016s, episode steps:   1, steps per second:  61, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.212464, accuracy: 0.781250, mean_q: 99.983826\n",
      " 3054/10000: episode: 2565, duration: 0.020s, episode steps:   1, steps per second:  50, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 1.056711, accuracy: 0.843750, mean_q: 100.202164\n",
      " 3055/10000: episode: 2566, duration: 0.019s, episode steps:   1, steps per second:  54, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.543524, accuracy: 0.812500, mean_q: 99.839584\n",
      " 3056/10000: episode: 2567, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.553306, accuracy: 0.843750, mean_q: 99.788345\n",
      " 3057/10000: episode: 2568, duration: 0.014s, episode steps:   1, steps per second:  71, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.807328, accuracy: 0.843750, mean_q: 100.177216\n",
      " 3058/10000: episode: 2569, duration: 0.015s, episode steps:   1, steps per second:  69, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.532192, accuracy: 0.843750, mean_q: 100.218483\n",
      " 3059/10000: episode: 2570, duration: 0.014s, episode steps:   1, steps per second:  70, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.015377, accuracy: 0.875000, mean_q: 100.766769\n",
      " 3060/10000: episode: 2571, duration: 0.015s, episode steps:   1, steps per second:  68, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.490776, accuracy: 0.906250, mean_q: 100.466019\n",
      " 3061/10000: episode: 2572, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 0.095363, accuracy: 0.937500, mean_q: 100.080635\n",
      " 3063/10000: episode: 2573, duration: 0.026s, episode steps:   2, steps per second:  76, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 4.500 [2.000, 7.000],  loss: 1.345963, accuracy: 0.812500, mean_q: 99.413475\n",
      " 3064/10000: episode: 2574, duration: 0.017s, episode steps:   1, steps per second:  59, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 141.545059, accuracy: 0.906250, mean_q: 99.635529\n",
      " 3065/10000: episode: 2575, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.520758, accuracy: 0.906250, mean_q: 99.781784\n",
      " 3066/10000: episode: 2576, duration: 0.016s, episode steps:   1, steps per second:  63, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.957433, accuracy: 0.875000, mean_q: 99.774323\n",
      " 3067/10000: episode: 2577, duration: 0.016s, episode steps:   1, steps per second:  62, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 0.591944, accuracy: 0.937500, mean_q: 99.932213\n",
      " 3068/10000: episode: 2578, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 10.000 [10.000, 10.000],  loss: 0.715786, accuracy: 0.843750, mean_q: 100.342743\n",
      " 3069/10000: episode: 2579, duration: 0.012s, episode steps:   1, steps per second:  83, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 8.000 [8.000, 8.000],  loss: 1.395270, accuracy: 0.875000, mean_q: 100.756516\n",
      " 3071/10000: episode: 2580, duration: 0.020s, episode steps:   2, steps per second: 100, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 8.000 [6.000, 10.000],  loss: 0.550614, accuracy: 0.781250, mean_q: 100.170624\n",
      " 3072/10000: episode: 2581, duration: 0.012s, episode steps:   1, steps per second:  81, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.313540, accuracy: 0.812500, mean_q: 99.929138\n",
      " 3073/10000: episode: 2582, duration: 0.013s, episode steps:   1, steps per second:  75, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.374448, accuracy: 0.812500, mean_q: 100.108047\n",
      " 3074/10000: episode: 2583, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 9.000 [9.000, 9.000],  loss: 152.475388, accuracy: 0.625000, mean_q: 99.932556\n",
      " 3075/10000: episode: 2584, duration: 0.019s, episode steps:   1, steps per second:  53, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 11.000 [11.000, 11.000],  loss: 1.236341, accuracy: 0.906250, mean_q: 98.938217\n",
      " 3077/10000: episode: 2585, duration: 0.025s, episode steps:   2, steps per second:  79, episode reward: 90.000, mean reward: 45.000 [-10.000, 100.000], mean action: 5.000 [3.000, 7.000],  loss: 1.316966, accuracy: 0.828125, mean_q: 99.654419\n",
      " 3078/10000: episode: 2586, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 2.764820, accuracy: 0.843750, mean_q: 100.497620\n",
      " 3079/10000: episode: 2587, duration: 0.012s, episode steps:   1, steps per second:  85, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 1.000 [1.000, 1.000],  loss: 0.589374, accuracy: 0.906250, mean_q: 100.058998\n",
      " 3080/10000: episode: 2588, duration: 0.013s, episode steps:   1, steps per second:  80, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.546463, accuracy: 0.781250, mean_q: 99.483719\n",
      " 3081/10000: episode: 2589, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 2.000 [2.000, 2.000],  loss: 1.840263, accuracy: 0.937500, mean_q: 99.666382\n",
      " 3082/10000: episode: 2590, duration: 0.017s, episode steps:   1, steps per second:  60, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 3.000 [3.000, 3.000],  loss: 0.710018, accuracy: 0.906250, mean_q: 100.006584\n",
      " 3083/10000: episode: 2591, duration: 0.013s, episode steps:   1, steps per second:  77, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 1.122357, accuracy: 0.906250, mean_q: 99.911034\n",
      " 3084/10000: episode: 2592, duration: 0.011s, episode steps:   1, steps per second:  89, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 9.143281, accuracy: 0.718750, mean_q: 100.848862\n",
      " 3085/10000: episode: 2593, duration: 0.012s, episode steps:   1, steps per second:  82, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 5.000 [5.000, 5.000],  loss: 0.636614, accuracy: 0.875000, mean_q: 100.594620\n",
      " 3088/10000: episode: 2594, duration: 0.024s, episode steps:   3, steps per second: 124, episode reward: 80.000, mean reward: 26.667 [-10.000, 100.000], mean action: 7.000 [4.000, 10.000],  loss: 4.640270, accuracy: 0.864583, mean_q: 100.010719\n",
      " 3089/10000: episode: 2595, duration: 0.012s, episode steps:   1, steps per second:  84, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 6.000 [6.000, 6.000],  loss: 0.443090, accuracy: 0.843750, mean_q: 100.240402\n",
      " 3090/10000: episode: 2596, duration: 0.013s, episode steps:   1, steps per second:  78, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 0.000 [0.000, 0.000],  loss: 0.393492, accuracy: 0.937500, mean_q: 100.185913\n",
      " 3091/10000: episode: 2597, duration: 0.013s, episode steps:   1, steps per second:  79, episode reward: 100.000, mean reward: 100.000 [100.000, 100.000], mean action: 4.000 [4.000, 4.000],  loss: 0.419435, accuracy: 0.906250, mean_q: 99.640533\n",
      "done, took 37.168 seconds\n"
     ]
    }
   ],
   "source": [
    "# Train agent\n",
    "depth = 3\n",
    "for i in range(1, depth + 1):\n",
    "    env = CubeLearnEnv(i)\n",
    "    steps = 10000 * (i ** 2)\n",
    "    history = dqn.fit(env, nb_steps=steps, visualize=False, verbose=2, nb_max_episode_steps=i ** 2 * 20)\n",
    "    \n",
    "    with open(\"history{0}.json\".format(i), \"w\") as h_file:\n",
    "        rewards = history.history[\"episode_reward\"]\n",
    "        n = 10\n",
    "        rewards = [(sum(rewards[i:i+n]) / n) for i in range(0, len(rewards), n)]\n",
    "        json.dump(rewards, h_file)\n",
    "    \n",
    "    if dqn.step < steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "dqn.save_weights(\"weights_v2.h5\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqhklEQVR4nO3de3wc9Xnv8c+jy66sXdmSdmVjwLYENoZAwGDZJCRcUkhCSE4gaZqENNSlKU6a0CT0NA29nJD25LxyI+UkbQ8tCSTkRpMSktDmUqiLHXIDhAFjLrYB22Aw9kq+SbKt63P+mFlZyJK9srQ7s9L3/Xrta3dnZ3cej0f77O/3m98z5u6IiIgAVEQdgIiIxIeSgoiIDFFSEBGRIUoKIiIyRElBRESGVEUdwERks1lvbm6OOgwRkbLy8MMPt7t702ivlXVSaG5upq2tLeowRETKipltHes1dR+JiMgQJQURERmipCAiIkOKlhTM7DYz22lm64ctazSze81sU3jfEC43M/uKmT1jZuvM7JxixSUiImMrZkvhG8ClI5ZdD6xy90XAqvA5wFuAReFtJXBzEeMSEZExFC0puPsvgF0jFl8O3B4+vh24Ytjyb3rgt0C9mc0tVmwiIjK6Uo8pzHH37eHjl4E54eMTgBeGrbctXHYYM1tpZm1m1pbL5YoXqYjINBTZPAV3dzMbd91ud78FuAWgtbVVdb9F5DAHegdo7+qhvauHjq5eOrp7aO/qpaE2wbLmBk5uSlNRYVGHGUulTgo7zGyuu28Pu4d2hstfBOYNW+/EcJkcxeCg40ClDnCZwgYHnd37e+no7g2/7HvpCL/wh553H3q+v3fgiJ83a0Y1rQsaWNrcwLLmRl59wixqqitL9K+Jt1InhbuBFcDnwvsfD1t+rZn9K3AusHdYN9Ok+82zHazeuJOZNdWkk1XU1VRRV1Md3lcxM3ycTlZRVVn6s3YP9g2Q6+yhozs48A/9EeR/8eQP/l52dfcA0JhKkEklyaQTZNOH7rPpVy7PppPMSEzfg7+3f5DOg310HuwPbj3DHofL+wedPz6/hZk11VGHO63tPdDH/Zty3Pd0jjUbd9Le1XvYOpUVFh77wfG9YH4tmVGO/2xdkkwqwfa9B2nbsou2Lbtp27qLVU8Hv0sTlRWcccJMljU3snRBA63NjTSmEqX+J8dC0ZKCmd0BXARkzWwbcANBMvi+mX0A2Aq8O1z9p8BlwDPAfuDqYsUF8MRLe/n6r7bQ2z941HVnVFcOJYvhiaMuWc2MRCU2wR/oB/sGaA9/3XSEv366x/iVk0pUDh3c8xprOXt+A9l0cOAO/XLq7uWxbXvo6Oqlq6d/1M+pTVQeSh6p4A9nMhJFOlk1lJAyqSRNdcH9rBnVk95UD/bbsK6Bzl7aw1+K+w70jfKFH3zp9xTwfw7QVJfkqtcsmNSY5cjcnQ07Ornv6Rz3Pb2Th5/fzcCgM2tGNRee0sTZ8+uHfthk0wky6ST14zy2WrIpWrIpfq816JjY1d3Lw1t3B4li626+/qst/MsvngPgpKYUyxY0DrUmmjO12FH+4A/2DbziB0bnwX66evrYN+wYPNA3wJXL5tOcTR37zioiK+fLcba2tvpEah/19Af/gV3D/sP2jfgP7TzYR1dP8Hjf0PLg/kDfkZuohUhWVZId9us++FWTIDviV38mNf5f+Af7BoLmdmfPUJ9qx1AC6gmb4sHzgxP9tzh09/YzOMrhlP81d+iXW/AHfahFE7zWUJugp3+AXOcruwLyCa+9K996OnLCmzXjUEvvUCKvZubQslcuH57008kqzv/Cf3NuS4avXHn2xPaJHFV3Tz+/eqad+zbkWL1hJ9v3HgTg9ONn8obFs3nDqU0smddQsu7Rg30DPP7i3qAlsWUXDz+/mz37+wDIpBIsXdBAXU310HdA8N1w6Puid6CwHx1/ctHJfPLSU4v5TzkiM3vY3VtHe62sC+JNVLKqkmS6kmw6GXUoRVFTXckJ9TM4oX5GSbY3MOjs2d976Et8WELKd3e1d/WwpaObjq7eo/b7AlQYQwklk05wVkP9K5LJ8K6ByeoaW9bcyIObd+HuR/1lKOP3XK5rKAk88NwuegcGSSereP3CLB+/pImLFs9mzsyaSGKrqa5kWXMjy5obgZMZHHSezXXRtnU3bVt2s/b53fT0DQz9mMimE7RkU6P8wAh6E4Yvn1lTTbqmivM+t4qOrp5I/n2FmNZJQSZXZYWF/blJoO6o6+/v7R/WcglaBzMSVWSHWhIJ6msTJR9EP7elkf9Yt50Xdh1gfqa2pNueinr7B/nNcx3c9/ROVm/YyZaO/QAsnJ1mxXkLeMOps2ld0EiiKn5VdyoqjEVz6lg0p44rl8+flM/MpJJ0jDI+EhdKChKZ2kQVtY1VzGuM1xfv8pYMAA9u2aWkMEGdB/tYcduDrH1+D8mqCs47OcMHXt/CRYtnx+7/vVQy6QTtaimIlI9Fs9PU11bz4OYO3rX0xKjDKVvdPf1c/fWHWLdtL1/43TN5+5Ljddon0JRO8lyuO+owxqSkIDJCRYXRuiAYV5Bjs7+3n6u/8RCPvLCHf7jybC57tarW5GXSCTq6e2I7ZhW/TjyRGDi3pZEtHfvZue9g1KGUnQO9A3zgG220bdnF/33PEiWEETLpJAf7Bgs60SIKSgoio1je0ggE4wpSuIN9A1zzzTYe2NzBTe9Zwv846/ioQ4qdTCo/tyie4wpKCiKjOP34mdQmKiPrQhoYdLa0x7ffeTT5hPCrZ9v54rvO4vIlo9a0nPaydcEp8KPN0I4DJQWRUVRVVrB0QUNkSeGutdu46MbV/OiR8igB1tM/wIe+/TD3b2rn8797Jr+rAfoxZVNBUojrXAUlBZExLG9uZMOOTvbsL/0vulVPBTV5PvmDdTz2wp6Sb388evsH+fC317J6Q47PvvPVvLt13tHfNI1lwtI0Hd1qKYiUleUtjbhD25bdJd1u38Agv3qmnUtPP45sOsnKb7XFdsC7b2CQj3x3Laue3slnrjhj0iZ4TWX5QnvtnWopiJSVs+bVk6isKPlg89qtu+ns6eeKs0/gayta6TzYz8pvPTzx+lSTrG9gkI/e8Qj3PrmDv7v8dN6vAoIFqQmLbKqlIFJmaqorOWverJKPK6zZmKOqwnjdwgynzZ3J37/7LB59YQ9/ddfjxKWAZf/AIB//3qP8bP3LfOptr+IPXtscdUhlJZtO6uwjkXK0vKWR9S/upXuMqqzFsHpDbqgaJ8ClZ8zluktO4a5HXuSr9z9XsjjG0j8wyJ99/zF+sm47f/PW0/ij17dEHVLZyaQSsa1/pKQgcgTLmhvpH3QeeX5PSba3c99Bnty+jwsXN71i+UcvXshbXz2Xz/7sae57eucY7y6+gUHnE3eu4+7HXuL6t5zKH59/UmSxlLP8rOY4iiQpmNnHzGy9mT1hZh8Pl33azF40s0fD22VRxCYy3NIFDVQYPLi5oyTbW7MxB8CFp7wyKZgZX/y9MzntuJl89I5HeGZnV0niGW5w0PmLO9fxw0de5BNvXsyHLjy55DFMFZl0UvMU8szsDOAaYDlwFvA2M1sYvnyTuy8Jbz8tdWwiI9XVVHP68bNKNti8ZmOOprokr5o787DXahNVfHVFK4mqCq75Zht7w4u/lMLgoPOXdz3OD9Zu47pLTuEjb1h49DfJmLLpJLv399Jf4EV5SimKlsJpwAPuvt/d+4E1wDsjiEOkIMtbGnnk+T309Bf37J+BQef+Te1ceErTmIXSTqifwT9ftZRtu/dz7R1rS/KlMjjo/PWP1vO9thf46O8s5GOXLCr6Nqe6bDqBO+wuYWIvVBRJYT1wvpllzKyW4NrM+dku15rZOjO7zcwaRnuzma00szYza8vlcqWKWaaxZc2N9PQP8vi2vUXdzqMv7GHvgb7Duo5Gi+czV5zB/Zva+ezPni5qTO7ODXc/wR0PPs+HLzqZ6954SlG3N11k8rOaYziuUPKk4O5PAZ8H7gF+DjwKDAA3AycDS4DtwJfGeP8t7t7q7q1NTUf+4xGZDMuag98nxe5CWrMxR4XB+YuyR133Pcvm84fnNXPrLzfz/bYXihLPs7kurvlmG9/67VY+eOFJfOLNi2NZ6rkc5Wc1t3fGb1whkoFmd7/V3Ze6+wXAbmCju+9w9wF3HwS+SjDmIBK5TDrJotnpos9XWLMxx5J59dTXJgpa/2/eehqvX5jlb364noe3Tl5s7V09/K8fredNN/2C3z63i7+67FSuv/RUJYRJlL8uvFoKITObHd7PJxhP+K6ZDS+6/g6CbiaRWFjW0sjDW3YzMFicyWMdXT2s27aHC0+ZXfB7qior+Mf3nc3x9TV88FtreWnPgQnFcKB3gH/8701c9MXVfPfB53nf8vms/sRFrLzgZCWESZbNtxRieAZSVPMUfmBmTwL/DnzE3fcAXzCzx81sHfAG4LqIYhM5zLktjXT29PPU9n1F+fxfPtOOO1y0eHxdovW1Cb62onWobPWBY7hwy8Cg8/22F3jDjau58Z6NnHdyhnuuu4D/fcUZQ79oZXLNrKmmqsJiWSk1kstxuvv5oyy7KopYRAqxrDm86M7mXZxxwqxJ//w1G3I0phK8+hg+e+HsOr5y5RI+cHsbf37nY/zjlWcX/Mt+zcYcn/3pUzz9cidnzavnK1eePXSBISmeigqjMZWIZakLzWgWKcDx9TOY1zijKOMKg4POmo05LliUpaLi2LppfufUOXzy0lP5ybrt/NN9zxx1/Sdf2sdVtz7AitseZH/vAP/4vrP50YfPU0IooWw6GctSF5G0FETK0bLmRtZsyE36BdefeGkfHd29h5W2GK8PXnASG17u5MZ7NnLKnDredPpxh62zfe8BbvzPjdz1yDZm1lTzv972Kt7/mvkkqyontG0Zv0w6QXsMK6UqKYgU6NyWRu5a+yLP5rpZODs9aZ+7ZmNQy+j8RRNLCmbGZ9/5ap5r7+a67z3KDz58HqceF8yM7jzYx82rn+XWX27GHa45/yQ+ctFCZtVWTzh+OTbZdJLNMbzkqrqPRAq0vCUDMOldSKs35DjzxFmTMqhbU13JLVctJZWs4ppvBhfnuf3XW7jwi6v5f6uf5dIzjmPV/7yQv7rsNCWEiMW1UqqSgkiBmjO1ZNPJSS2Ot3d/H2uf333UWczjMWdmDbf8QSs79vXw+s/fxw13P8Epc9Lcfe3r+PJ7z2ZeY+2kbUuOXSad5EDfQEnLshdC3UciBTIzzm1p5KFJvDznr55tZ9APr4o6UUvm1XPTu5dw+6+3sPKCk7j4tNmaaxAz+bkKHV29pJLx+SqOTyQiZWB5SyM/eXw723bv58SGif/iXr1hJzNrqlgyr37iwY3w1jPn8tYz5x59RYlEvruwvbuH+Zn4tN7UfSQyDvlTNidjXME9OBX1/EVNVFXqT3G6yQxrKcSJjkSRcVg8p46ZNVU8NAnF8Tbs6GTHvp5J7zqS8pDJtxRiNoFNSUFkHCoqjGXNjTwwCS2F1RvCq6xNcH6ClKdMKt9SUFIQKWvLWxp5LtdNrnNif8xrNuQ49bg65sysmaTIpJzUVFdSl6yKXVE8JQWRcVoWjitMpAupq6eftq271EqY5jLpBB0xm9WspCAyTmccP4sZ1ZUTGmz+9TPt9A04F42jVLZMPZl0Ut1HIuUuUVXBOQvqJ5QU1mzMkUpUsnTBqFedlWkiE8NKqUoKIsdgeXOGp17ex94D47/wev5U1PMWZklU6U9wOsvWxa9Sqo5IkWOwrKUBd1i7dfyzm5/NdbNt94FxX1BHpp5sKsGu/b1Fu6LfsYjqcpwfM7P1ZvaEmX08XNZoZvea2abwXu1qia2z5zVQXWnHdGrqmo3BqagXTLAqqpS/TDqJO+zeH5/WQsmTgpmdAVwDLAfOAt5mZguB64FV7r4IWBU+F4mlGYlKzjyx/piK463esJOTm1IqTCdDs5rjNK4QRUvhNOABd9/v7v3AGuCdwOXA7eE6twNXRBCbSMGWNTfy+It7x3Vd5AO9AzyweRcXLdZZR3Ko/lGcxhWiSArrgfPNLGNmtcBlwDxgjrtvD9d5GZgz2pvNbKWZtZlZWy6XK03EIqM4t6WRvgHnkRcKH1f47eYOevsHVdpCgEOVUqd1S8HdnwI+D9wD/Bx4FBgYsY4Do468uPst7t7q7q1NTfrDkugsbW7AbHzF8dZsyFFTXaFrIQsAmZRaCgC4+63uvtTdLwB2AxuBHWY2FyC83xlFbCKFmllTzavmzhxfUtiY47UnZaip1jWRBWbNqKaywqZ3SwHAzGaH9/MJxhO+C9wNrAhXWQH8OIrYRMZjWXMja5/fTW//4FHX3drRzeb2bnUdyZCKCovdZTmjmqfwAzN7Evh34CPuvgf4HPBGM9sEXBI+F4m1c1saOdg3yPqX9h513fypqBdqkFmGyaSTdHTHp6UQyZXX3P38UZZ1ABdHEI7IMVs27KI758w/8tSaNRtyLMjU0pJNlSI0KRPZdCJWlVI1o1lkArLpJCc1pXjoKOMKPf0D/PrZDnUdyWEyqUSsWgpKCiITdG5LIw9u2XXEUgUPbd7Ngb4BJQU5TCadpL1TLQWRKWN5SyOdB/vZ8HLnmOus2biTRGUFrz05U8LIpBxk00kO9A2wv7c/6lAAJQWRCVvWnB9XGLvkxZqNOZa3NFKbiGQYT2IsX+oiLmcgKSmITNCJDbWcUD+Dh7aMPrP5pT0H2LijS11HMqq4zWpWUhCZBMtbGnlg8y6CyfivlD8VVaWyZTT5Wc1xOQNJSUFkEixvaaS9q4fN7d2HvbZmQ47jZ9WwcHY6gsgk7rJ1+VIXaimITBn5cYWHtrzy1NS+gUF+9Uw7Fy6ejZlFEZrEXCYVjil0q6UgMmWc3JQik0ocdtGdtVt309nTr/EEGVNNdSXpZJXGFESmEjNjeUvjYcXx1mzMUVVhvG6hTkWVsWXS8al/pKQgMkmWNTeybfcBXtpzYGjZ6g05li5ooK6mOsLIJO6y6aRaCiJTTf4aCflxhZ37DvLk9n1cqLOO5CjiVClVSUFkkpw2dyZ1yaqhcYVfbGoH0HiCHFWcKqUqKYhMksoKo7W5YWhcYfWGnTTVJXnV3JkRRyZxl00n2NXde8T6WaWipCAyiZa1NPLMzi52dh7k/k3tXHhKk05FlaPKpBIMOuzeH30XUlRXXrvOzJ4ws/VmdoeZ1ZjZN8xss5k9Gt6WRBGbyEScG44r3Hr/ZvYe6FPXkRTk0AS26JNCyatzmdkJwEeBV7n7ATP7PvDe8OVPuPudpY5JZLK8+oR6klUVfOPXW6gwOH9RNuqQpAzkS10Es5rrIo0lqu6jKmCGmVUBtcBLEcUhMqkSVRWcPb+env5Blsyrp742EXVIUgaGiuLFYFZzyZOCu78I3Ag8D2wH9rr7PeHL/8fM1pnZTWaWHO39ZrbSzNrMrC2Xy5UoapHCLW8JJqpdeIquxSyFyaTDonid0Z+BVPKkYGYNwOVAC3A8kDKz9wN/CZwKLAMagU+O9n53v8XdW929talJ/bUSPxefOptEVQWXnnFc1KFImaifUU1lhcXitNQouo8uATa7e87d+4C7gPPcfbsHeoCvA8sjiE1kws6aV88Tf/tmFh8Xbd+wlI+KCqMxJhPYokgKzwOvMbNaC87Vuxh4yszmAoTLrgDWRxCbyKSortTZ3jI+mVQiFtdUKPnZR+7+gJndCawF+oFHgFuAn5lZE2DAo8CHSh2biEhUsjGZ1RzJBWPd/QbghhGLfyeKWERE4iCTTrD1+cMv0lRqauOKiMRANp2ctmMKIiIyQiadYH/vAPt7+yONQ0lBRCQGsql4lLpQUhARiYFMflZzxBfbUVIQEYmBbFotBRERCeVbClGflqqkICISA/lKqVFPYFNSEBGJgRmJSlKJysi7j444ec3M/h0Y8/pw7v72SY9IRGSaytYlIx9oPtqM5hvD+3cCxwHfDp9fCewoVlAiItNRJpWIfEzhiEnB3dcAmNmX3L112Ev/bmZtRY1MRGSayaSTvLBrf6QxFDqmkDKzk/JPzKwFSBUnJBGR6Smbjr5SaqEF8T4OrDaz5wiqmC4AVhYrKBGR6SiTSrKru4eBQaeywiKJ4ahJwcwqgFnAIoIrowE8HV4MR0REJkk2nWDQYc/+3qFLdJbaUbuP3H0Q+At373H3x8KbEoKIyCTLJ4KO7ui6kAodU/gvM/tzM5tnZo35W1EjExGZZuJQ/6jQMYX3hPcfGbbMgZNGWfeozOw64I/Dz3gcuBqYC/wrkAEeBq5y9+iLi4uIlEi+/lGUg80FtRTcvWWU27EmhBOAjwKt7n4GUAm8F/g8cJO7LwR2Ax84ls8XESlXh4rixb+lgJmdAbwKqMkvc/dvTmC7M8ysD6gFthNcjvN94eu3A58Gbj7GzxcRKTv1M6qpsGgrpRaUFMzsBuAigqTwU+AtwC+BcScFd3/RzG4EngcOAPcQdBftcff8JYe2ASeMEctKwtNh58+fP97Ni4jEVkWF0ZhKRjqrudCB5ncBFwMvu/vVwFkEp6mOm5k1AJcDLcDxBJPgLi30/e5+i7u3untrU1PTsYQgIhJbUU9gKzQpHAhPTe03s5nATmDeMW7zEmCzu+fcvQ+4C3gdUG9m+ZbLicCLx/j5IiJlK5NORHr2UaFJoc3M6oGvEnT1rAV+c4zbfB54jZnVmpkRtECeBO4jaJEArAB+fIyfLyJStrLpZPzHFNz9w+HDfzaznwMz3X3dsWzQ3R8wszsJEks/8AhwC/AT4F/N7DPhsluP5fNFRMpZJpWM/9lHZvYt4BfA/e7+9EQ36u43ADeMWPwcsHyiny0iUs4y6QTdvQMc6B1gRqKy5NsvtPvoNoLJZf9gZs+Z2Q/M7GNFjEtEZFrKRjyrudDuo/vM7BfAMuANwIeA04EvFzE2EZFpJzus/tG8xtqSb7/Q7qNVBKeO/ga4H1jm7juLGZiIyHSUiXhWc6HdR+uAXuAM4EzgDDObUbSoRESmqUwq6D6K6gykQruPrgMwszrgD4GvE1yzOZqC3yIiU9RQUbyIZjUX2n10LXA+sBTYQjDwfH/xwhIRmZ5mJCpJJSpp74xxS4GgCN7fAw8Pq08kIiJFkElHV/+o0NLZNwLVwFUAZtZkZi3FDExEZLrKpBORjSkUlBTCKqmfBP4yXFQNfLtYQYmITGeZVDKyeQqFnn30DuDtQDeAu78E1BUrKBGR6SzKSqmFJoVed3eCy2diZqnihSQiMr1l00l2dfcwOOgl3/ZRk0JYyfQ/zOxfCMpbXwP8F0HFVBERmWSZdIJBhz0H+kq+7aOefeTubma/B/wZsA9YDHzK3e8tdnAiItPR8FnNjeFktlIp9JTUtQSXy/xEMYMRERHIhokg19XDojmlHb4tNCmcC/y+mW0lHGwGcPczixKViMg0lq3LtxRKP9hcaFJ482Rt0MwWA98btugk4FNAPXANkAuX/5W7/3SytisiUi4O1T8q/WmphdY+2jpZG3T3DcASADOrJLgW8w+Bq4GbwolyIiLTVn1tggoLymeXWqGnpBbLxcCzk5l0RETKXWWF0ZiKZq5C1EnhvcAdw55fa2brzOw2M2sY7Q1mttLM2sysLZfLjbaKiEjZy6ajmdUcWVIwswTBLOl/CxfdDJxM0LW0HfjSaO9z91vcvdXdW5uamkoRqohIyQX1j6ZRUgDeAqx19x0A7r7D3QfcfZBgYtzyCGMTEYlUJpWcdmMKVzKs68jM5g577R3A+pJHJCISE1FVSi30lNRJFdZOeiPwwWGLv2BmSwjqK20Z8ZqIyLSSTSfp6unnYN8ANdWVJdtuJEnB3buBzIhlV0URi4hIHGXTwVyF9q4eTmyoLdl2oz77SERERpFJRTOrWUlBRCSGMmFLodSX5VRSEBGJoWxYKbW9Uy0FEZFpL99SaFdLQUREahNV1CYqNaYgIiKBKGY1KymIiMRUFLOalRRERGIqm06Q61RLQURECM5AUktBRESAYExhV3cvg4Nesm0qKYiIxFQmlWRg0Nl7oK9k21RSEBGJqcyw+keloqQgIhJTTflZzSWcq6CkICISU5kwKZSy/pGSgohITA0VxVNLQUREGmoTVBglndVc8qRgZovN7NFht31m9nEzazSze81sU3jfUOrYRETipLLCaEwlyE3lloK7b3D3Je6+BFgK7Ad+CFwPrHL3RcCq8LmIyLSWSSWndkthhIuBZ919K3A5cHu4/HbgiqiCEhGJi0w6UdJZzVEnhfcCd4SP57j79vDxy8Cc0d5gZivNrM3M2nK5XCliFBGJTCY9TVoKZpYA3g7828jX3N2BUed1u/st7t7q7q1NTU1FjlJEJFqZVGLazFN4C7DW3XeEz3eY2VyA8H5nZJGJiMREU12Srp5+DvYNlGR7USaFKznUdQRwN7AifLwC+HHJIxIRiZlMKpyrUKJxhUiSgpmlgDcCdw1b/DngjWa2CbgkfC4iMq0NzWou0bhCVUm2MoK7dwOZEcs6CM5GEhGRUKmL4kV99pGIiBxBqYviKSmIiMRYqesfKSmIiMRYbaKKGdWVJRtTUFIQEYm5Us5qVlIQEYm5bDqpgWYREQlk06Wb1aykICISc6WslKqkICISc5l0gl3dvQwOjloSblIpKYiIxFwmnaR/0Nl7oK/o21JSEBGJuWx+rkJ38buQlBRERGIuW8JZzUoKIiIxV8pZzUoKIiIxl0mFlVLVfSQiIo2pBGbQ3qmkICIy7VVWGI21CdpLUOoiqovs1JvZnWb2tJk9ZWavNbNPm9mLZvZoeLssithEROIok06UZAJbJBfZAb4M/Nzd32VmCaAWeDNwk7vfGFFMIiKxFcxqnoItBTObBVwA3Arg7r3uvqfUcYiIlJNMOlGSonhRdB+1ADng62b2iJl9LbxmM8C1ZrbOzG4zs4bR3mxmK82szczacrlcyYIWEYlSNj1FWwoEXVbnADe7+9lAN3A9cDNwMrAE2A58abQ3u/st7t7q7q1NTU2liVhEJGLZdILOnn4O9g0UdTtRJIVtwDZ3fyB8fidwjrvvcPcBdx8EvgosjyA2EZFYyoSzmncV+QykkicFd38ZeMHMFoeLLgaeNLO5w1Z7B7C+1LGJiMRVJhXMai72uEJUZx/9KfCd8Myj54Crga+Y2RLAgS3AByOKTUQkdrJ14azmIo8rRJIU3P1RoHXE4qsiCEVEpCxkU/mieMVtKWhGs4hIGRgqijfVxhRERGT8ahOV1FRXFH1Ws5KCiEgZMDOy6WTRr6mgpCAiUiYy6aTGFEREJJBNJYp+9pGSgohImcikE0W/0I6SgohImciE9Y8GB71o21BSEBEpE9l0kv5BZ9/BvqJtQ0lBRKRMZNP5UhfFG1dQUhARKROZVL7URfHGFZQURETKRClmNSspiIiUiWy6+PWPlBRERMpEQ201ZhpTEBERoKqygobahMYUREQkkCnyrGYlBRGRMpItcv2jSJKCmdWb2Z1m9rSZPWVmrzWzRjO718w2hfcNUcQmIhJnQamLqddS+DLwc3c/FTgLeAq4Hljl7ouAVeFzEREZZsq1FMxsFnABcCuAu/e6+x7gcuD2cLXbgStKHZuISNxlUgk6D/bT0z9QlM+PoqXQAuSAr5vZI2b2NTNLAXPcfXu4zsvAnNHebGYrzazNzNpyuVyJQhYRiYdMOj+ruThdSFEkhSrgHOBmdz8b6GZEV5G7OzBqGUB3v8XdW929tampqejBiojESb7+0VRKCtuAbe7+QPj8ToIkscPM5gKE9zsjiE1EJNbyLYX2Il1XoeRJwd1fBl4ws8XhoouBJ4G7gRXhshXAj0sdm4hI3BW7pVBVlE89uj8FvmNmCeA54GqCBPV9M/sAsBV4d0SxiYjEVjad5M2nz2F2XbIon29B9315am1t9ba2tqjDEBEpK2b2sLu3jvaaZjSLiMgQJQURERmipCAiIkOUFEREZIiSgoiIDFFSEBGRIUoKIiIyRElBRESGlPXkNTPLEcx+PhZZoH0SwymmcolVcU6ucokTyidWxRlY4O6jVhQt66QwEWbWNtaMvrgpl1gV5+QqlzihfGJVnEen7iMRERmipCAiIkOmc1K4JeoAxqFcYlWck6tc4oTyiVVxHsW0HVMQEZHDTeeWgoiIjKCkICIiQ6Z8UjCzS81sg5k9Y2bXj/J60sy+F77+gJk1RxDjPDO7z8yeNLMnzOxjo6xzkZntNbNHw9unSh3nsFi2mNnjYRyHXeXIAl8J9+k6MzsnghgXD9tXj5rZPjP7+Ih1ItmnZnabme00s/XDljWa2b1mtim8bxjjvSvCdTaZ2YrR1ilBrF80s6fD/9sfmln9GO894nFSgjg/bWYvDvv/vWyM9x7xO6IEcX5vWIxbzOzRMd5bmv3p7lP2BlQCzwInAQngMeBVI9b5MPDP4eP3At+LIM65wDnh4zpg4yhxXgT8R9T7NIxlC5A9wuuXAT8DDHgN8EAMjoOXCSbsRL5PgQuAc4D1w5Z9Abg+fHw98PlR3tdIcPnaRqAhfNwQQaxvAqrCx58fLdZCjpMSxPlp4M8LODaO+B1R7DhHvP4l4FNR7s+p3lJYDjzj7s+5ey/wr8DlI9a5HLg9fHwncLGZWQljxN23u/va8HEn8BRwQiljmGSXA9/0wG+BejObG2E8FwPPuvuxzn6fVO7+C2DXiMXDj8PbgStGeeubgXvdfZe77wbuBS4tVpwweqzufo+794dPfwucWMwYCjHGPi1EId8Rk+ZIcYbfO+8G7ijW9gsx1ZPCCcALw55v4/Av26F1wgN9L5ApSXSjCLuvzgYeGOXl15rZY2b2MzM7vbSRvYID95jZw2a2cpTXC9nvpfRexv5Di8s+nePu28PHLwNzRlknbvsV4I8IWoWjOdpxUgrXht1ct43RJRenfXo+sMPdN43xekn251RPCmXFzNLAD4CPu/u+ES+vJej+OAv4B+BHJQ5vuNe7+znAW4CPmNkFEcZyRGaWAN4O/NsoL8dpnw7xoK8g9ueKm9lfA/3Ad8ZYJerj5GbgZGAJsJ2gaybOruTIrYSS7M+pnhReBOYNe35iuGzUdcysCpgFdJQkumHMrJogIXzH3e8a+bq773P3rvDxT4FqM8uWOMx8LC+G9zuBHxI0wYcrZL+XyluAte6+Y+QLcdqnwI58F1t4v3OUdWKzX83sD4G3Ab8fJrHDFHCcFJW773D3AXcfBL46xvZjsU/D7553At8ba51S7c+pnhQeAhaZWUv4i/G9wN0j1rkbyJ/F8S7gv8c6yIsl7Eu8FXjK3f9+jHWOy491mNlygv+7KJJXyszq8o8JBh3Xj1jtbuAPwrOQXgPsHdY1Umpj/vqKyz4NDT8OVwA/HmWd/wTeZGYNYVfIm8JlJWVmlwJ/Abzd3fePsU4hx0lRjRjHescY2y/kO6IULgGedvdto71Y0v1Z7JHsqG8EZ8JsJDjD4K/DZX9HcEAD1BB0LTwDPAicFEGMryfoLlgHPBreLgM+BHwoXOda4AmCsyN+C5wX0f48KYzhsTCe/D4dHqsB/xTu88eB1ohiTRF8yc8atizyfUqQpLYDfQR92B8gGMdaBWwC/gtoDNdtBb427L1/FB6rzwBXRxTrMwT98PljNX/23vHAT490nJQ4zm+Fx986gi/6uSPjDJ8f9h1RyjjD5d/IH5fD1o1kf6rMhYiIDJnq3UciIjIOSgoiIjJESUFERIYoKYiIyBAlBRERGaKkIFPSaNUow+WjViMN51RMemVXM/s7M7tkEj6nazLiETkaJQWZqr7B6MXirgdWufsignkB+VLJbwEWhbeVBCUSJszdP+Xu/zUZnyVSCkoKMiX52NUox6pGWlBlVzN7v5k9GNa0/xczqwyXd5nZTRZcD2OVmTWFy79hZu8KH3/OgmtmrDOzG8NlzWb23+GyVWY2P1zeYma/Cevnf2ZEDJ8ws4fC9/xtuCxlZj8Ji/utN7P3TGgHyrSlpCDTzVjVSI9aLdPMTgPeA7zO3ZcAA8Dvhy+ngDZ3Px1YA9ww4r0ZglILp7v7mUD+i/4fgNvDZd8BvhIu/zJws7u/mmAGbP5z3kTQmllOUOhtaVgY7VLgJXc/y93PAH4+jn0iMkRJQaYtD6bzj2dK/8XAUuAhC66OdTFB+QGAQQ4VM/s2QemS4fYCB4FbzeydQL5m0GuB74aPvzXsfa/jUM2mbw37nDeFt0cIqryeSpAkHgfeaGafN7Pz3X3vOP5dIkOqog5ApMR2mNlcd98+ohppIdUyjeBX/V8WsJ1XJBt37w+L7l1MUHjxWuB3xvMZw2L4rLv/y2EvBIPjlwGfMbNV7v53BcQp8gpqKch0M1Y10kIqu64C3mVms2HoTKYF4WsVBF/2AO8Dfjn8jeG1MmZ5UKL7OuCs8KVfE1TmhKAr6v7w8a9GLM/7T+CPws/DzE4ws9lmdjyw392/DXyR4JKPIuOmloJMSWZ2B8E1mLNmtg24wd1vBT4HfN/MPgBsJbj8IcBPCX5lP0PQtXP1yM909yfN7G8Irn5VQVDp8iPh53QDy8PXdxKMPQxXB/zYzGoIfu3/Wbj8T4Gvm9kngNyw7X4M+K6ZfZJhZbTd/Z5wbOM3YdXvLuD9wELgi2Y2GMb1J+PbYyIBVUkVmQRm1uXu6ajjEJkodR+JiMgQtRRERGSIWgoiIjJESUFERIYoKYiIyBAlBRERGaKkICIiQ/4/xbtbirwpGd8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print reward graph\n",
    "for i in range(1, depth + 1):\n",
    "    with open(\"history{0}.json\".format(i), \"r\") as h_file:\n",
    "        rewards = json.load(h_file)\n",
    "        n = 10\n",
    "        rewards = [(sum(rewards[i:i+n]) / n) for i in range(0, len(rewards), n)]\n",
    "        plt.plot(np.arange(0., float(len(rewards)), 1.), rewards)\n",
    "        plt.xlabel(\"100 episodes\")\n",
    "        plt.ylabel(\"reward\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 turns:\n",
      "mae: 0.00\n",
      "accuracy: 100.00\n",
      "2 turns:\n",
      "mae: 24.23\n",
      "accuracy: 91.20\n",
      "3 turns:\n",
      "mae: 122.08\n",
      "accuracy: 44.90\n",
      "4 turns:\n",
      "mae: 158.13\n",
      "accuracy: 18.30\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "for i in range(1, depth + 2):\n",
    "    test_env = CubeLearnEnv(i)\n",
    "    n = 1000\n",
    "    history = dqn.test(test_env, nb_episodes=n, visualize=False, verbose=0, nb_max_episode_steps=depth * 10)\n",
    "    rewards = history.history[\"episode_reward\"]\n",
    "    accuracy = [(reward > 0) for reward in rewards].count(True) * (100 / n)\n",
    "    mse = np.array(rewards)\n",
    "    print(i, \"turns:\")\n",
    "    print(\"mae: %.2f\" % mean_absolute_error(mse, np.full(mse.size, 100)))\n",
    "    print(\"accuracy: %.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 98.000, steps: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show results\n",
    "class EpisodeLogger(rl.callbacks.Callback):\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "        self._success = False\n",
    "        self._path = []\n",
    "\n",
    "    def on_step_begin(self, episode, logs):\n",
    "        if self._env.cube not in self._path:\n",
    "            self._path.append(self._env.cube.copy())\n",
    "        \n",
    "    def on_step_end(self, step, logs):\n",
    "        self._path.append(self._env.cube.copy())\n",
    "        self._success = logs['reward'] == 100\n",
    "\n",
    "    def view(self):\n",
    "        if self._success:\n",
    "            for cube in self._path[-(self._env.depth + 1):]:\n",
    "                cube.show()\n",
    "        else:\n",
    "            print(\"Failed to solve the cube :(\")\n",
    "\n",
    "\n",
    "cube = Cube()\n",
    "path = input(\"Enter shuffle sequence: \")        # for example: F L R\n",
    "for action in path.split(\" \"):\n",
    "    cube.move(action)\n",
    "\n",
    "test_env = CubeLearnEnv(depth, default=cube.cube)\n",
    "logger = EpisodeLogger(test_env)\n",
    "dqn.test(test_env, nb_episodes=1, visualize=False, callbacks=[logger], nb_max_episode_steps=depth * 10)\n",
    "logger.view()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "534e6d0eea74cc66e78bdecd1dd13b042a46377ee569c2b5f528f15ddf8a3864"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
